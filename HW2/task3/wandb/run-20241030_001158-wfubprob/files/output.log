2024-10-30 00:11:59 - INFO - __main__ - Initializing training script
2024-10-30 00:11:59 - INFO - __main__ - Loading dataset...
2024-10-30 00:12:08 - INFO - __main__ - Loading model and tokenizer...
Some weights of RobertaForSequenceClassificationWithAdapters were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'roberta.encoder.layer.0.output.adapter_after_ffn.down_project.bias', 'roberta.encoder.layer.0.output.adapter_after_ffn.down_project.weight', 'roberta.encoder.layer.0.output.adapter_after_ffn.up_project.bias', 'roberta.encoder.layer.0.output.adapter_after_ffn.up_project.weight', 'roberta.encoder.layer.0.output.adapter_before_ffn.down_project.bias', 'roberta.encoder.layer.0.output.adapter_before_ffn.down_project.weight', 'roberta.encoder.layer.0.output.adapter_before_ffn.up_project.bias', 'roberta.encoder.layer.0.output.adapter_before_ffn.up_project.weight', 'roberta.encoder.layer.1.output.adapter_after_ffn.down_project.bias', 'roberta.encoder.layer.1.output.adapter_after_ffn.down_project.weight', 'roberta.encoder.layer.1.output.adapter_after_ffn.up_project.bias', 'roberta.encoder.layer.1.output.adapter_after_ffn.up_project.weight', 'roberta.encoder.layer.1.output.adapter_before_ffn.down_project.bias', 'roberta.encoder.layer.1.output.adapter_before_ffn.down_project.weight', 'roberta.encoder.layer.1.output.adapter_before_ffn.up_project.bias', 'roberta.encoder.layer.1.output.adapter_before_ffn.up_project.weight', 'roberta.encoder.layer.10.output.adapter_after_ffn.down_project.bias', 'roberta.encoder.layer.10.output.adapter_after_ffn.down_project.weight', 'roberta.encoder.layer.10.output.adapter_after_ffn.up_project.bias', 'roberta.encoder.layer.10.output.adapter_after_ffn.up_project.weight', 'roberta.encoder.layer.10.output.adapter_before_ffn.down_project.bias', 'roberta.encoder.layer.10.output.adapter_before_ffn.down_project.weight', 'roberta.encoder.layer.10.output.adapter_before_ffn.up_project.bias', 'roberta.encoder.layer.10.output.adapter_before_ffn.up_project.weight', 'roberta.encoder.layer.11.output.adapter_after_ffn.down_project.bias', 'roberta.encoder.layer.11.output.adapter_after_ffn.down_project.weight', 'roberta.encoder.layer.11.output.adapter_after_ffn.up_project.bias', 'roberta.encoder.layer.11.output.adapter_after_ffn.up_project.weight', 'roberta.encoder.layer.11.output.adapter_before_ffn.down_project.bias', 'roberta.encoder.layer.11.output.adapter_before_ffn.down_project.weight', 'roberta.encoder.layer.11.output.adapter_before_ffn.up_project.bias', 'roberta.encoder.layer.11.output.adapter_before_ffn.up_project.weight', 'roberta.encoder.layer.2.output.adapter_after_ffn.down_project.bias', 'roberta.encoder.layer.2.output.adapter_after_ffn.down_project.weight', 'roberta.encoder.layer.2.output.adapter_after_ffn.up_project.bias', 'roberta.encoder.layer.2.output.adapter_after_ffn.up_project.weight', 'roberta.encoder.layer.2.output.adapter_before_ffn.down_project.bias', 'roberta.encoder.layer.2.output.adapter_before_ffn.down_project.weight', 'roberta.encoder.layer.2.output.adapter_before_ffn.up_project.bias', 'roberta.encoder.layer.2.output.adapter_before_ffn.up_project.weight', 'roberta.encoder.layer.3.output.adapter_after_ffn.down_project.bias', 'roberta.encoder.layer.3.output.adapter_after_ffn.down_project.weight', 'roberta.encoder.layer.3.output.adapter_after_ffn.up_project.bias', 'roberta.encoder.layer.3.output.adapter_after_ffn.up_project.weight', 'roberta.encoder.layer.3.output.adapter_before_ffn.down_project.bias', 'roberta.encoder.layer.3.output.adapter_before_ffn.down_project.weight', 'roberta.encoder.layer.3.output.adapter_before_ffn.up_project.bias', 'roberta.encoder.layer.3.output.adapter_before_ffn.up_project.weight', 'roberta.encoder.layer.4.output.adapter_after_ffn.down_project.bias', 'roberta.encoder.layer.4.output.adapter_after_ffn.down_project.weight', 'roberta.encoder.layer.4.output.adapter_after_ffn.up_project.bias', 'roberta.encoder.layer.4.output.adapter_after_ffn.up_project.weight', 'roberta.encoder.layer.4.output.adapter_before_ffn.down_project.bias', 'roberta.encoder.layer.4.output.adapter_before_ffn.down_project.weight', 'roberta.e
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/root/HW2/task3/train.py:106: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
2024-10-30 00:12:14 - INFO - __main__ - Starting training...
  0%|                                                                                                    | 0/1284 [00:00<?, ?it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:1160: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.
  warnings.warn(
                                                                                                                                  
{'loss': 1.3858, 'grad_norm': 2.7986769676208496, 'learning_rate': 9.922118380062307e-05, 'epoch': 0.02}
{'loss': 1.3149, 'grad_norm': 3.03244686126709, 'learning_rate': 9.84423676012461e-05, 'epoch': 0.05}
{'loss': 1.0247, 'grad_norm': 4.183598041534424, 'learning_rate': 9.766355140186917e-05, 'epoch': 0.07}
{'loss': 0.6435, 'grad_norm': 2.239351987838745, 'learning_rate': 9.688473520249222e-05, 'epoch': 0.09}
{'loss': 0.4846, 'grad_norm': 3.0183043479919434, 'learning_rate': 9.610591900311527e-05, 'epoch': 0.12}
{'loss': 0.482, 'grad_norm': 7.206270217895508, 'learning_rate': 9.532710280373832e-05, 'epoch': 0.14}
{'loss': 0.3713, 'grad_norm': 6.353057384490967, 'learning_rate': 9.454828660436138e-05, 'epoch': 0.16}
{'loss': 0.3335, 'grad_norm': 4.417660713195801, 'learning_rate': 9.376947040498443e-05, 'epoch': 0.19}
{'loss': 0.4024, 'grad_norm': 5.583807468414307, 'learning_rate': 9.299065420560748e-05, 'epoch': 0.21}
{'loss': 0.4411, 'grad_norm': 6.258852958679199, 'learning_rate': 9.221183800623053e-05, 'epoch': 0.23}
{'loss': 0.5045, 'grad_norm': 5.683914661407471, 'learning_rate': 9.14330218068536e-05, 'epoch': 0.26}
{'loss': 0.3604, 'grad_norm': 11.724687576293945, 'learning_rate': 9.065420560747664e-05, 'epoch': 0.28}
{'loss': 0.436, 'grad_norm': 4.067253589630127, 'learning_rate': 8.98753894080997e-05, 'epoch': 0.3}
{'loss': 0.3239, 'grad_norm': 3.0246241092681885, 'learning_rate': 8.909657320872274e-05, 'epoch': 0.33}
{'loss': 0.3316, 'grad_norm': 5.227484703063965, 'learning_rate': 8.831775700934581e-05, 'epoch': 0.35}
{'loss': 0.2025, 'grad_norm': 1.4527878761291504, 'learning_rate': 8.753894080996884e-05, 'epoch': 0.37}
{'loss': 0.3621, 'grad_norm': 5.77932071685791, 'learning_rate': 8.676012461059191e-05, 'epoch': 0.4}
{'loss': 0.5004, 'grad_norm': 5.062796115875244, 'learning_rate': 8.598130841121496e-05, 'epoch': 0.42}
{'loss': 0.3422, 'grad_norm': 5.939849853515625, 'learning_rate': 8.520249221183801e-05, 'epoch': 0.44}
{'loss': 0.372, 'grad_norm': 5.664063930511475, 'learning_rate': 8.442367601246106e-05, 'epoch': 0.47}
{'loss': 0.4435, 'grad_norm': 3.751826047897339, 'learning_rate': 8.364485981308412e-05, 'epoch': 0.49}
{'loss': 0.3174, 'grad_norm': 6.999156475067139, 'learning_rate': 8.286604361370717e-05, 'epoch': 0.51}
{'loss': 0.3512, 'grad_norm': 4.40463399887085, 'learning_rate': 8.208722741433022e-05, 'epoch': 0.54}
{'loss': 0.2206, 'grad_norm': 1.3844294548034668, 'learning_rate': 8.130841121495327e-05, 'epoch': 0.56}
{'loss': 0.2411, 'grad_norm': 2.412611722946167, 'learning_rate': 8.052959501557633e-05, 'epoch': 0.58}
{'loss': 0.4183, 'grad_norm': 4.11207389831543, 'learning_rate': 7.975077881619938e-05, 'epoch': 0.61}
{'loss': 0.3636, 'grad_norm': 3.958712339401245, 'learning_rate': 7.897196261682243e-05, 'epoch': 0.63}
{'loss': 0.381, 'grad_norm': 1.7442927360534668, 'learning_rate': 7.819314641744548e-05, 'epoch': 0.65}
{'loss': 0.3325, 'grad_norm': 2.970993757247925, 'learning_rate': 7.741433021806855e-05, 'epoch': 0.68}
{'loss': 0.2635, 'grad_norm': 5.8540167808532715, 'learning_rate': 7.663551401869158e-05, 'epoch': 0.7}
{'loss': 0.3013, 'grad_norm': 2.593879461288452, 'learning_rate': 7.585669781931465e-05, 'epoch': 0.72}
{'loss': 0.3404, 'grad_norm': 4.743374824523926, 'learning_rate': 7.50778816199377e-05, 'epoch': 0.75}
{'loss': 0.3214, 'grad_norm': 2.5323638916015625, 'learning_rate': 7.429906542056075e-05, 'epoch': 0.77}
{'loss': 0.3091, 'grad_norm': 3.1854634284973145, 'learning_rate': 7.35202492211838e-05, 'epoch': 0.79}
{'loss': 0.3062, 'grad_norm': 5.5569257736206055, 'learning_rate': 7.274143302180686e-05, 'epoch': 0.82}
{'loss': 0.1966, 'grad_norm': 3.284945249557495, 'learning_rate': 7.196261682242991e-05, 'epoch': 0.84}
{'loss': 0.2798, 'grad_norm': 5.399162769317627, 'learning_rate': 7.118380062305296e-05, 'epoch': 0.86}
{'loss': 0.2329, 'grad_norm': 1.4487661123275757, 'learning_rate': 7.040498442367601e-05, 'epoch': 0.89}
{'loss': 0.307, 'grad_norm': 1.9364219903945923, 'learning_rate': 6.962616822429907e-05, 'epoch': 0.91}
{'loss': 0.2984, 'grad_norm': 2.1181020736694336, 'learning_rate': 6.884735202492212e-05, 'epoch': 0.93}
{'loss': 0.3348, 'grad_norm': 5.71777868270874, 'learning_rate': 6.806853582554517e-05, 'epoch': 0.96}
{'loss': 0.3027, 'grad_norm': 3.5377750396728516, 'learning_rate': 6.728971962616822e-05, 'epoch': 0.98}
  warnings.warn(                                                                                                                  
{'eval_loss': 0.2315724492073059, 'eval_accuracy': 0.925, 'eval_micro_f1': 0.925, 'eval_macro_f1': 0.9234138047973148, 'eval_runtime': 1.6936, 'eval_samples_per_second': 448.759, 'eval_steps_per_second': 28.343, 'epoch': 1.0}
 39%|███████████████████████████████████                                                       | 500/1284 [00:37<00:55, 14.24it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:1160: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.
{'loss': 0.2686, 'grad_norm': 3.7351226806640625, 'learning_rate': 6.651090342679129e-05, 'epoch': 1.0}
{'loss': 0.1906, 'grad_norm': 3.782026529312134, 'learning_rate': 6.573208722741432e-05, 'epoch': 1.03}
{'loss': 0.337, 'grad_norm': 3.808875799179077, 'learning_rate': 6.495327102803739e-05, 'epoch': 1.05}
{'loss': 0.2901, 'grad_norm': 4.938320159912109, 'learning_rate': 6.417445482866044e-05, 'epoch': 1.07}
{'loss': 0.2141, 'grad_norm': 1.6599270105361938, 'learning_rate': 6.339563862928349e-05, 'epoch': 1.1}
{'loss': 0.1902, 'grad_norm': 2.0277628898620605, 'learning_rate': 6.261682242990654e-05, 'epoch': 1.12}
{'loss': 0.3338, 'grad_norm': 0.8342152833938599, 'learning_rate': 6.18380062305296e-05, 'epoch': 1.14}
{'loss': 0.2407, 'grad_norm': 7.631058692932129, 'learning_rate': 6.105919003115265e-05, 'epoch': 1.17}
  warnings.warn(
                                                                                                                                  
{'loss': 0.1957, 'grad_norm': 3.7616615295410156, 'learning_rate': 6.028037383177571e-05, 'epoch': 1.19}
{'loss': 0.2378, 'grad_norm': 9.525419235229492, 'learning_rate': 5.950155763239875e-05, 'epoch': 1.21}
{'loss': 0.2533, 'grad_norm': 1.3216012716293335, 'learning_rate': 5.872274143302181e-05, 'epoch': 1.24}
{'loss': 0.3209, 'grad_norm': 1.4495759010314941, 'learning_rate': 5.794392523364486e-05, 'epoch': 1.26}
{'loss': 0.3395, 'grad_norm': 4.081080436706543, 'learning_rate': 5.7165109034267914e-05, 'epoch': 1.29}
{'loss': 0.2324, 'grad_norm': 5.112151145935059, 'learning_rate': 5.6386292834890964e-05, 'epoch': 1.31}
{'loss': 0.1654, 'grad_norm': 0.9927941560745239, 'learning_rate': 5.560747663551402e-05, 'epoch': 1.33}
{'loss': 0.3015, 'grad_norm': 3.848912239074707, 'learning_rate': 5.482866043613707e-05, 'epoch': 1.36}
{'loss': 0.2428, 'grad_norm': 4.926774978637695, 'learning_rate': 5.404984423676013e-05, 'epoch': 1.38}
{'loss': 0.181, 'grad_norm': 2.9404304027557373, 'learning_rate': 5.327102803738318e-05, 'epoch': 1.4}
{'loss': 0.3284, 'grad_norm': 2.546452760696411, 'learning_rate': 5.2492211838006234e-05, 'epoch': 1.43}
{'loss': 0.3765, 'grad_norm': 6.098384380340576, 'learning_rate': 5.171339563862928e-05, 'epoch': 1.45}
{'loss': 0.2693, 'grad_norm': 5.731873989105225, 'learning_rate': 5.093457943925234e-05, 'epoch': 1.47}
{'loss': 0.2589, 'grad_norm': 4.651399612426758, 'learning_rate': 5.0155763239875384e-05, 'epoch': 1.5}
{'loss': 0.3903, 'grad_norm': 3.8112680912017822, 'learning_rate': 4.937694704049845e-05, 'epoch': 1.52}
{'loss': 0.2257, 'grad_norm': 4.538126468658447, 'learning_rate': 4.85981308411215e-05, 'epoch': 1.54}
{'loss': 0.3815, 'grad_norm': 8.220168113708496, 'learning_rate': 4.781931464174455e-05, 'epoch': 1.57}
{'loss': 0.218, 'grad_norm': 3.0270626544952393, 'learning_rate': 4.7040498442367604e-05, 'epoch': 1.59}
{'loss': 0.1712, 'grad_norm': 3.2707669734954834, 'learning_rate': 4.6261682242990654e-05, 'epoch': 1.61}
{'loss': 0.2553, 'grad_norm': 3.49001145362854, 'learning_rate': 4.548286604361371e-05, 'epoch': 1.64}
{'loss': 0.2611, 'grad_norm': 0.67999267578125, 'learning_rate': 4.470404984423676e-05, 'epoch': 1.66}
{'loss': 0.2256, 'grad_norm': 0.7371584177017212, 'learning_rate': 4.392523364485982e-05, 'epoch': 1.68}
{'loss': 0.2832, 'grad_norm': 5.77107572555542, 'learning_rate': 4.314641744548287e-05, 'epoch': 1.71}
{'loss': 0.2335, 'grad_norm': 0.3464288115501404, 'learning_rate': 4.236760124610592e-05, 'epoch': 1.73}
{'loss': 0.2584, 'grad_norm': 3.8261916637420654, 'learning_rate': 4.1588785046728974e-05, 'epoch': 1.75}
{'loss': 0.2537, 'grad_norm': 3.4641835689544678, 'learning_rate': 4.0809968847352024e-05, 'epoch': 1.78}
{'loss': 0.1573, 'grad_norm': 6.098598957061768, 'learning_rate': 4.003115264797508e-05, 'epoch': 1.8}
{'loss': 0.2816, 'grad_norm': 5.361725330352783, 'learning_rate': 3.925233644859813e-05, 'epoch': 1.82}
{'loss': 0.2436, 'grad_norm': 4.188648223876953, 'learning_rate': 3.847352024922119e-05, 'epoch': 1.85}
{'loss': 0.3505, 'grad_norm': 2.439169406890869, 'learning_rate': 3.769470404984424e-05, 'epoch': 1.87}
{'loss': 0.3607, 'grad_norm': 3.8479959964752197, 'learning_rate': 3.691588785046729e-05, 'epoch': 1.89}
{'loss': 0.2037, 'grad_norm': 5.580795764923096, 'learning_rate': 3.6137071651090344e-05, 'epoch': 1.92}
{'loss': 0.265, 'grad_norm': 4.757655143737793, 'learning_rate': 3.5358255451713394e-05, 'epoch': 1.94}
{'loss': 0.4057, 'grad_norm': 5.2581305503845215, 'learning_rate': 3.457943925233645e-05, 'epoch': 1.96}
{'loss': 0.2563, 'grad_norm': 1.4112316370010376, 'learning_rate': 3.38006230529595e-05, 'epoch': 1.99}
  warnings.warn(                                                                                                                  
{'eval_loss': 0.21493473649024963, 'eval_accuracy': 0.9289473684210526, 'eval_micro_f1': 0.9289473684210526, 'eval_macro_f1': 0.9271476764966591, 'eval_runtime': 1.6929, 'eval_samples_per_second': 448.946, 'eval_steps_per_second': 28.354, 'epoch': 2.0}
 78%|█████████████████████████████████████████████████████████████████████▎                   | 1000/1284 [01:15<00:20, 13.62it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:1160: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.
{'loss': 0.2064, 'grad_norm': 1.0166471004486084, 'learning_rate': 3.302180685358255e-05, 'epoch': 2.01}
{'loss': 0.2925, 'grad_norm': 1.4324880838394165, 'learning_rate': 3.224299065420561e-05, 'epoch': 2.03}
{'loss': 0.1117, 'grad_norm': 2.4884512424468994, 'learning_rate': 3.146417445482866e-05, 'epoch': 2.06}
{'loss': 0.2009, 'grad_norm': 1.7736315727233887, 'learning_rate': 3.0685358255451714e-05, 'epoch': 2.08}
{'loss': 0.2905, 'grad_norm': 2.957489490509033, 'learning_rate': 2.9906542056074764e-05, 'epoch': 2.1}
{'loss': 0.2373, 'grad_norm': 1.614941954612732, 'learning_rate': 2.9127725856697818e-05, 'epoch': 2.13}
{'loss': 0.1503, 'grad_norm': 0.08952970057725906, 'learning_rate': 2.834890965732087e-05, 'epoch': 2.15}
{'loss': 0.1917, 'grad_norm': 6.186840057373047, 'learning_rate': 2.7570093457943924e-05, 'epoch': 2.17}
{'loss': 0.3967, 'grad_norm': 8.585930824279785, 'learning_rate': 2.6791277258566978e-05, 'epoch': 2.2}
{'loss': 0.1545, 'grad_norm': 1.8610496520996094, 'learning_rate': 2.601246105919003e-05, 'epoch': 2.22}
{'loss': 0.1853, 'grad_norm': 4.404162406921387, 'learning_rate': 2.5233644859813084e-05, 'epoch': 2.24}
{'loss': 0.176, 'grad_norm': 4.70141077041626, 'learning_rate': 2.4454828660436138e-05, 'epoch': 2.27}
{'loss': 0.2186, 'grad_norm': 3.6192173957824707, 'learning_rate': 2.367601246105919e-05, 'epoch': 2.29}
{'loss': 0.0786, 'grad_norm': 5.659095764160156, 'learning_rate': 2.2897196261682244e-05, 'epoch': 2.31}
{'loss': 0.1817, 'grad_norm': 6.541678428649902, 'learning_rate': 2.2118380062305298e-05, 'epoch': 2.34}
  warnings.warn(
100%|████████████████████████████████████████████████████████████████████████████████████████▉| 1283/1284 [01:36<00:00, 14.10it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:1160: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.
{'loss': 0.2208, 'grad_norm': 4.483842372894287, 'learning_rate': 2.133956386292835e-05, 'epoch': 2.36}
{'loss': 0.2653, 'grad_norm': 6.605252265930176, 'learning_rate': 2.05607476635514e-05, 'epoch': 2.38}
{'loss': 0.2186, 'grad_norm': 4.951112747192383, 'learning_rate': 1.9781931464174454e-05, 'epoch': 2.41}
{'loss': 0.2588, 'grad_norm': 2.1449458599090576, 'learning_rate': 1.9003115264797507e-05, 'epoch': 2.43}
{'loss': 0.3122, 'grad_norm': 0.8508691191673279, 'learning_rate': 1.822429906542056e-05, 'epoch': 2.45}
{'loss': 0.31, 'grad_norm': 3.2992491722106934, 'learning_rate': 1.7445482866043614e-05, 'epoch': 2.48}
{'loss': 0.2307, 'grad_norm': 1.3775116205215454, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.5}
{'loss': 0.1918, 'grad_norm': 7.492746829986572, 'learning_rate': 1.588785046728972e-05, 'epoch': 2.52}
{'loss': 0.2331, 'grad_norm': 4.178611755371094, 'learning_rate': 1.5109034267912772e-05, 'epoch': 2.55}
{'loss': 0.2929, 'grad_norm': 5.782130718231201, 'learning_rate': 1.4330218068535826e-05, 'epoch': 2.57}
{'loss': 0.127, 'grad_norm': 0.866465151309967, 'learning_rate': 1.3551401869158877e-05, 'epoch': 2.59}
{'loss': 0.2411, 'grad_norm': 2.0782711505889893, 'learning_rate': 1.277258566978193e-05, 'epoch': 2.62}
{'loss': 0.2399, 'grad_norm': 3.6775877475738525, 'learning_rate': 1.1993769470404986e-05, 'epoch': 2.64}
{'loss': 0.1616, 'grad_norm': 6.043001651763916, 'learning_rate': 1.1214953271028037e-05, 'epoch': 2.66}
{'loss': 0.1915, 'grad_norm': 2.903374433517456, 'learning_rate': 1.043613707165109e-05, 'epoch': 2.69}
{'loss': 0.1795, 'grad_norm': 1.8187440633773804, 'learning_rate': 9.657320872274144e-06, 'epoch': 2.71}
{'loss': 0.2072, 'grad_norm': 2.6515510082244873, 'learning_rate': 8.878504672897196e-06, 'epoch': 2.73}
{'loss': 0.3225, 'grad_norm': 2.639535427093506, 'learning_rate': 8.099688473520249e-06, 'epoch': 2.76}
{'loss': 0.0903, 'grad_norm': 0.6952940821647644, 'learning_rate': 7.3208722741433015e-06, 'epoch': 2.78}
{'loss': 0.1366, 'grad_norm': 5.606836318969727, 'learning_rate': 6.542056074766355e-06, 'epoch': 2.8}
{'loss': 0.2209, 'grad_norm': 3.8538291454315186, 'learning_rate': 5.763239875389408e-06, 'epoch': 2.83}
{'loss': 0.2945, 'grad_norm': 6.145456790924072, 'learning_rate': 4.9844236760124615e-06, 'epoch': 2.85}
{'loss': 0.364, 'grad_norm': 0.4849375784397125, 'learning_rate': 4.205607476635514e-06, 'epoch': 2.87}
{'loss': 0.1081, 'grad_norm': 1.6146692037582397, 'learning_rate': 3.426791277258567e-06, 'epoch': 2.9}
{'loss': 0.1891, 'grad_norm': 1.5112744569778442, 'learning_rate': 2.64797507788162e-06, 'epoch': 2.92}
{'loss': 0.1937, 'grad_norm': 9.836252212524414, 'learning_rate': 1.8691588785046728e-06, 'epoch': 2.94}
{'loss': 0.2362, 'grad_norm': 2.336560010910034, 'learning_rate': 1.0903426791277259e-06, 'epoch': 2.97}
{'loss': 0.2654, 'grad_norm': 3.7994112968444824, 'learning_rate': 3.1152647975077885e-07, 'epoch': 2.99}
  warnings.warn(
100%|█████████████████████████████████████████████████████████████████████████████████████████| 1284/1284 [01:39<00:00, 12.94it/s]
2024-10-30 00:13:54 - INFO - __main__ - Starting evaluation...                                                                    
{'eval_loss': 0.217324361205101, 'eval_accuracy': 0.9302631578947368, 'eval_micro_f1': 0.9302631578947368, 'eval_macro_f1': 0.9284171402250008, 'eval_runtime': 1.6869, 'eval_samples_per_second': 450.525, 'eval_steps_per_second': 28.454, 'epoch': 3.0}
{'train_runtime': 99.2213, 'train_samples_per_second': 206.81, 'train_steps_per_second': 12.941, 'train_loss': 0.2989022357627239, 'epoch': 3.0}
/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:1160: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.
  warnings.warn(
100%|█████████████████████████████████████████████████████████████████████████████████████████████| 48/48 [00:01<00:00, 28.89it/s]
2024-10-30 00:13:55 - INFO - __main__ - Evaluation results: {'eval_loss': 0.217324361205101, 'eval_accuracy': 0.9302631578947368, 'eval_micro_f1': 0.9302631578947368, 'eval_macro_f1': 0.9284171402250008, 'eval_runtime': 1.6967, 'eval_samples_per_second': 447.934, 'eval_steps_per_second': 28.291, 'epoch': 3.0}
2024-10-30 00:13:55 - INFO - __main__ - Training complete.
