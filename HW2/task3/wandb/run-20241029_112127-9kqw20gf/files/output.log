2024-10-29 11:21:28 - INFO - __main__ - Initializing training script
2024-10-29 11:21:28 - INFO - __main__ - Loading dataset...
2024-10-29 11:21:28 - INFO - __main__ - Loading model and tokenizer...
Some weights of RobertaForSequenceClassificationWithAdapters were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'roberta.encoder.layer.0.output.adapter_after_ffn.down_project.bias', 'roberta.encoder.layer.0.output.adapter_after_ffn.down_project.weight', 'roberta.encoder.layer.0.output.adapter_after_ffn.up_project.bias', 'roberta.encoder.layer.0.output.adapter_after_ffn.up_project.weight', 'roberta.encoder.layer.0.output.adapter_before_ffn.down_project.bias', 'roberta.encoder.layer.0.output.adapter_before_ffn.down_project.weight', 'roberta.encoder.layer.0.output.adapter_before_ffn.up_project.bias', 'roberta.encoder.layer.0.output.adapter_before_ffn.up_project.weight', 'roberta.encoder.layer.1.output.adapter_after_ffn.down_project.bias', 'roberta.encoder.layer.1.output.adapter_after_ffn.down_project.weight', 'roberta.encoder.layer.1.output.adapter_after_ffn.up_project.bias', 'roberta.encoder.layer.1.output.adapter_after_ffn.up_project.weight', 'roberta.encoder.layer.1.output.adapter_before_ffn.down_project.bias', 'roberta.encoder.layer.1.output.adapter_before_ffn.down_project.weight', 'roberta.encoder.layer.1.output.adapter_before_ffn.up_project.bias', 'roberta.encoder.layer.1.output.adapter_before_ffn.up_project.weight', 'roberta.encoder.layer.10.output.adapter_after_ffn.down_project.bias', 'roberta.encoder.layer.10.output.adapter_after_ffn.down_project.weight', 'roberta.encoder.layer.10.output.adapter_after_ffn.up_project.bias', 'roberta.encoder.layer.10.output.adapter_after_ffn.up_project.weight', 'roberta.encoder.layer.10.output.adapter_before_ffn.down_project.bias', 'roberta.encoder.layer.10.output.adapter_before_ffn.down_project.weight', 'roberta.encoder.layer.10.output.adapter_before_ffn.up_project.bias', 'roberta.encoder.layer.10.output.adapter_before_ffn.up_project.weight', 'roberta.encoder.layer.11.output.adapter_after_ffn.down_project.bias', 'roberta.encoder.layer.11.output.adapter_after_ffn.down_project.weight', 'roberta.encoder.layer.11.output.adapter_after_ffn.up_project.bias', 'roberta.encoder.layer.11.output.adapter_after_ffn.up_project.weight', 'roberta.encoder.layer.11.output.adapter_before_ffn.down_project.bias', 'roberta.encoder.layer.11.output.adapter_before_ffn.down_project.weight', 'roberta.encoder.layer.11.output.adapter_before_ffn.up_project.bias', 'roberta.encoder.layer.11.output.adapter_before_ffn.up_project.weight', 'roberta.encoder.layer.2.output.adapter_after_ffn.down_project.bias', 'roberta.encoder.layer.2.output.adapter_after_ffn.down_project.weight', 'roberta.encoder.layer.2.output.adapter_after_ffn.up_project.bias', 'roberta.encoder.layer.2.output.adapter_after_ffn.up_project.weight', 'roberta.encoder.layer.2.output.adapter_before_ffn.down_project.bias', 'roberta.encoder.layer.2.output.adapter_before_ffn.down_project.weight', 'roberta.encoder.layer.2.output.adapter_before_ffn.up_project.bias', 'roberta.encoder.layer.2.output.adapter_before_ffn.up_project.weight', 'roberta.encoder.layer.3.output.adapter_after_ffn.down_project.bias', 'roberta.encoder.layer.3.output.adapter_after_ffn.down_project.weight', 'roberta.encoder.layer.3.output.adapter_after_ffn.up_project.bias', 'roberta.encoder.layer.3.output.adapter_after_ffn.up_project.weight', 'roberta.encoder.layer.3.output.adapter_before_ffn.down_project.bias', 'roberta.encoder.layer.3.output.adapter_before_ffn.down_project.weight', 'roberta.encoder.layer.3.output.adapter_before_ffn.up_project.bias', 'roberta.encoder.layer.3.output.adapter_before_ffn.up_project.weight', 'roberta.encoder.layer.4.output.adapter_after_ffn.down_project.bias', 'roberta.encoder.layer.4.output.adapter_after_ffn.down_project.weight', 'roberta.encoder.layer.4.output.adapter_after_ffn.up_project.bias', 'roberta.encoder.layer.4.output.adapter_after_ffn.up_project.weight', 'roberta.encoder.layer.4.output.adapter_before_ffn.down_project.bias', 'roberta.encoder.layer.4.output.adapter_before_ffn.down_project.weight', 'roberta.e
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Map: 100%|██████████████████████████████████████████████████████████████████████████| 3452/3452 [00:00<00:00, 17890.95 examples/s]
Map: 100%|██████████████████████████████████████████████████████████████████████████| 1120/1120 [00:00<00:00, 16014.14 examples/s]
/root/HW2/task3/train.py:106: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
2024-10-29 11:21:34 - INFO - __main__ - Starting training...
  0%|                                                                                                     | 0/324 [00:00<?, ?it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
 33%|██████████████████████████████▎                                                            | 108/324 [00:37<00:51,  4.18it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
{'loss': 1.0058, 'grad_norm': 1.4582332372665405, 'learning_rate': 9.691358024691359e-05, 'epoch': 0.09}
{'loss': 0.9547, 'grad_norm': 0.9630460143089294, 'learning_rate': 9.382716049382717e-05, 'epoch': 0.19}
{'loss': 0.9716, 'grad_norm': 1.2939095497131348, 'learning_rate': 9.074074074074075e-05, 'epoch': 0.28}
{'loss': 0.9283, 'grad_norm': 2.6091105937957764, 'learning_rate': 8.765432098765433e-05, 'epoch': 0.37}
{'loss': 0.9685, 'grad_norm': 1.7872226238250732, 'learning_rate': 8.456790123456791e-05, 'epoch': 0.46}
{'loss': 0.8965, 'grad_norm': 1.6496840715408325, 'learning_rate': 8.148148148148148e-05, 'epoch': 0.56}
{'loss': 0.9568, 'grad_norm': 0.6370327472686768, 'learning_rate': 7.839506172839507e-05, 'epoch': 0.65}
{'loss': 0.9535, 'grad_norm': 0.7630677819252014, 'learning_rate': 7.530864197530865e-05, 'epoch': 0.74}
{'loss': 0.9473, 'grad_norm': 1.8946799039840698, 'learning_rate': 7.222222222222222e-05, 'epoch': 0.83}
{'loss': 0.9824, 'grad_norm': 1.097826361656189, 'learning_rate': 6.91358024691358e-05, 'epoch': 0.93}
  warnings.warn(                                                                                                                  
{'eval_loss': 0.8999478220939636, 'eval_accuracy': 0.65, 'eval_micro_f1': 0.65, 'eval_macro_f1': 0.2626262626262626, 'eval_runtime': 6.1893, 'eval_samples_per_second': 180.959, 'eval_steps_per_second': 5.655, 'epoch': 1.0}
 43%|██████████████████████████████████████▊                                                    | 138/324 [00:46<00:56,  3.27it/s]Traceback (most recent call last):
{'loss': 0.9335, 'grad_norm': 1.6287407875061035, 'learning_rate': 6.60493827160494e-05, 'epoch': 1.02}
{'loss': 0.9093, 'grad_norm': 1.0763344764709473, 'learning_rate': 6.296296296296296e-05, 'epoch': 1.11}
{'loss': 0.9363, 'grad_norm': 2.7392895221710205, 'learning_rate': 5.987654320987654e-05, 'epoch': 1.2}
  File "/root/HW2/task3/train.py", line 120, in <module>
    trainer.train()
  File "/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
  File "/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/trainer.py", line 2479, in _inner_training_loop
    and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))
KeyboardInterrupt
