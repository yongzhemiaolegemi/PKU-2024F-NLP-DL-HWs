2024-10-29 23:08:54 - INFO - __main__ - Initializing training script
2024-10-29 23:08:54 - INFO - __main__ - Loading dataset...
2024-10-29 23:08:54 - INFO - __main__ - Loading model and tokenizer...
Some weights of RobertaForSequenceClassificationWithAdapters were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'roberta.encoder.layer.0.output.adapter_after_ffn.down_project.bias', 'roberta.encoder.layer.0.output.adapter_after_ffn.down_project.weight', 'roberta.encoder.layer.0.output.adapter_after_ffn.up_project.bias', 'roberta.encoder.layer.0.output.adapter_after_ffn.up_project.weight', 'roberta.encoder.layer.0.output.adapter_before_ffn.down_project.bias', 'roberta.encoder.layer.0.output.adapter_before_ffn.down_project.weight', 'roberta.encoder.layer.0.output.adapter_before_ffn.up_project.bias', 'roberta.encoder.layer.0.output.adapter_before_ffn.up_project.weight', 'roberta.encoder.layer.1.output.adapter_after_ffn.down_project.bias', 'roberta.encoder.layer.1.output.adapter_after_ffn.down_project.weight', 'roberta.encoder.layer.1.output.adapter_after_ffn.up_project.bias', 'roberta.encoder.layer.1.output.adapter_after_ffn.up_project.weight', 'roberta.encoder.layer.1.output.adapter_before_ffn.down_project.bias', 'roberta.encoder.layer.1.output.adapter_before_ffn.down_project.weight', 'roberta.encoder.layer.1.output.adapter_before_ffn.up_project.bias', 'roberta.encoder.layer.1.output.adapter_before_ffn.up_project.weight', 'roberta.encoder.layer.10.output.adapter_after_ffn.down_project.bias', 'roberta.encoder.layer.10.output.adapter_after_ffn.down_project.weight', 'roberta.encoder.layer.10.output.adapter_after_ffn.up_project.bias', 'roberta.encoder.layer.10.output.adapter_after_ffn.up_project.weight', 'roberta.encoder.layer.10.output.adapter_before_ffn.down_project.bias', 'roberta.encoder.layer.10.output.adapter_before_ffn.down_project.weight', 'roberta.encoder.layer.10.output.adapter_before_ffn.up_project.bias', 'roberta.encoder.layer.10.output.adapter_before_ffn.up_project.weight', 'roberta.encoder.layer.11.output.adapter_after_ffn.down_project.bias', 'roberta.encoder.layer.11.output.adapter_after_ffn.down_project.weight', 'roberta.encoder.layer.11.output.adapter_after_ffn.up_project.bias', 'roberta.encoder.layer.11.output.adapter_after_ffn.up_project.weight', 'roberta.encoder.layer.11.output.adapter_before_ffn.down_project.bias', 'roberta.encoder.layer.11.output.adapter_before_ffn.down_project.weight', 'roberta.encoder.layer.11.output.adapter_before_ffn.up_project.bias', 'roberta.encoder.layer.11.output.adapter_before_ffn.up_project.weight', 'roberta.encoder.layer.2.output.adapter_after_ffn.down_project.bias', 'roberta.encoder.layer.2.output.adapter_after_ffn.down_project.weight', 'roberta.encoder.layer.2.output.adapter_after_ffn.up_project.bias', 'roberta.encoder.layer.2.output.adapter_after_ffn.up_project.weight', 'roberta.encoder.layer.2.output.adapter_before_ffn.down_project.bias', 'roberta.encoder.layer.2.output.adapter_before_ffn.down_project.weight', 'roberta.encoder.layer.2.output.adapter_before_ffn.up_project.bias', 'roberta.encoder.layer.2.output.adapter_before_ffn.up_project.weight', 'roberta.encoder.layer.3.output.adapter_after_ffn.down_project.bias', 'roberta.encoder.layer.3.output.adapter_after_ffn.down_project.weight', 'roberta.encoder.layer.3.output.adapter_after_ffn.up_project.bias', 'roberta.encoder.layer.3.output.adapter_after_ffn.up_project.weight', 'roberta.encoder.layer.3.output.adapter_before_ffn.down_project.bias', 'roberta.encoder.layer.3.output.adapter_before_ffn.down_project.weight', 'roberta.encoder.layer.3.output.adapter_before_ffn.up_project.bias', 'roberta.encoder.layer.3.output.adapter_before_ffn.up_project.weight', 'roberta.encoder.layer.4.output.adapter_after_ffn.down_project.bias', 'roberta.encoder.layer.4.output.adapter_after_ffn.down_project.weight', 'roberta.encoder.layer.4.output.adapter_after_ffn.up_project.bias', 'roberta.encoder.layer.4.output.adapter_after_ffn.up_project.weight', 'roberta.encoder.layer.4.output.adapter_before_ffn.down_project.bias', 'roberta.encoder.layer.4.output.adapter_before_ffn.down_project.weight', 'roberta.e
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Map: 100%|██████████████████████████████████████████████████████████████████████████| 3452/3452 [00:00<00:00, 12821.90 examples/s]
Map: 100%|██████████████████████████████████████████████████████████████████████████| 1120/1120 [00:00<00:00, 12271.96 examples/s]
/root/HW2/task3/train.py:106: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
2024-10-29 23:09:01 - INFO - __main__ - Starting training...
                                                                                                                                  
{'loss': 1.0092, 'grad_norm': 1.5259373188018799, 'learning_rate': 9.84567901234568e-05, 'epoch': 0.05}
{'loss': 0.9667, 'grad_norm': 1.3856385946273804, 'learning_rate': 9.691358024691359e-05, 'epoch': 0.09}
{'loss': 0.9829, 'grad_norm': 2.628713369369507, 'learning_rate': 9.537037037037038e-05, 'epoch': 0.14}
{'loss': 0.9441, 'grad_norm': 2.0784897804260254, 'learning_rate': 9.382716049382717e-05, 'epoch': 0.19}
{'loss': 0.921, 'grad_norm': 1.254211664199829, 'learning_rate': 9.228395061728396e-05, 'epoch': 0.23}
{'loss': 1.0197, 'grad_norm': 2.6658108234405518, 'learning_rate': 9.074074074074075e-05, 'epoch': 0.28}
{'loss': 0.988, 'grad_norm': 1.0109962224960327, 'learning_rate': 8.919753086419754e-05, 'epoch': 0.32}
{'loss': 0.9166, 'grad_norm': 0.9145255088806152, 'learning_rate': 8.765432098765433e-05, 'epoch': 0.37}
{'loss': 0.9805, 'grad_norm': 1.3609365224838257, 'learning_rate': 8.611111111111112e-05, 'epoch': 0.42}
{'loss': 0.8708, 'grad_norm': 0.9661407470703125, 'learning_rate': 8.456790123456791e-05, 'epoch': 0.46}
{'loss': 0.8965, 'grad_norm': 2.845886468887329, 'learning_rate': 8.30246913580247e-05, 'epoch': 0.51}
{'loss': 0.8952, 'grad_norm': 1.0298899412155151, 'learning_rate': 8.148148148148148e-05, 'epoch': 0.56}
{'loss': 0.8993, 'grad_norm': 3.2969982624053955, 'learning_rate': 7.993827160493828e-05, 'epoch': 0.6}
{'loss': 0.8944, 'grad_norm': 0.7001693248748779, 'learning_rate': 7.839506172839507e-05, 'epoch': 0.65}
{'loss': 1.0087, 'grad_norm': 3.7307181358337402, 'learning_rate': 7.685185185185185e-05, 'epoch': 0.69}
{'loss': 0.8863, 'grad_norm': 0.9126002788543701, 'learning_rate': 7.530864197530865e-05, 'epoch': 0.74}
{'loss': 0.9953, 'grad_norm': 2.437238931655884, 'learning_rate': 7.376543209876543e-05, 'epoch': 0.79}
{'loss': 1.0039, 'grad_norm': 1.3483606576919556, 'learning_rate': 7.222222222222222e-05, 'epoch': 0.83}
{'loss': 0.964, 'grad_norm': 1.2047553062438965, 'learning_rate': 7.067901234567902e-05, 'epoch': 0.88}
{'loss': 0.9333, 'grad_norm': 2.2900919914245605, 'learning_rate': 6.91358024691358e-05, 'epoch': 0.93}
{'loss': 0.9191, 'grad_norm': 1.3005926609039307, 'learning_rate': 6.759259259259259e-05, 'epoch': 0.97}
                                                                                                                                  
{'eval_loss': 0.8808834552764893, 'eval_accuracy': 0.65, 'eval_micro_f1': 0.65, 'eval_macro_f1': 0.2626262626262626, 'eval_runtime': 2.3433, 'eval_samples_per_second': 477.952, 'eval_steps_per_second': 29.872, 'epoch': 1.0}
{'loss': 0.9518, 'grad_norm': 1.9472988843917847, 'learning_rate': 6.60493827160494e-05, 'epoch': 1.02}
{'loss': 0.9151, 'grad_norm': 2.980250835418701, 'learning_rate': 6.450617283950617e-05, 'epoch': 1.06}
{'loss': 0.9526, 'grad_norm': 2.08955717086792, 'learning_rate': 6.296296296296296e-05, 'epoch': 1.11}
{'loss': 0.934, 'grad_norm': 3.7541630268096924, 'learning_rate': 6.141975308641975e-05, 'epoch': 1.16}
{'loss': 0.9392, 'grad_norm': 1.1561455726623535, 'learning_rate': 5.987654320987654e-05, 'epoch': 1.2}
{'loss': 0.9098, 'grad_norm': 0.8952253460884094, 'learning_rate': 5.833333333333334e-05, 'epoch': 1.25}
{'loss': 0.8924, 'grad_norm': 2.2322158813476562, 'learning_rate': 5.679012345679012e-05, 'epoch': 1.3}
{'loss': 1.0061, 'grad_norm': 1.9682646989822388, 'learning_rate': 5.524691358024692e-05, 'epoch': 1.34}
{'loss': 0.9366, 'grad_norm': 2.1894779205322266, 'learning_rate': 5.370370370370371e-05, 'epoch': 1.39}
{'loss': 0.9061, 'grad_norm': 0.7771583199501038, 'learning_rate': 5.2160493827160494e-05, 'epoch': 1.44}
{'loss': 0.9853, 'grad_norm': 2.2042126655578613, 'learning_rate': 5.061728395061729e-05, 'epoch': 1.48}
{'loss': 0.8924, 'grad_norm': 2.540062189102173, 'learning_rate': 4.9074074074074075e-05, 'epoch': 1.53}
{'loss': 0.898, 'grad_norm': 1.2307748794555664, 'learning_rate': 4.7530864197530866e-05, 'epoch': 1.57}
{'loss': 0.9428, 'grad_norm': 2.546247959136963, 'learning_rate': 4.5987654320987656e-05, 'epoch': 1.62}
{'loss': 0.8493, 'grad_norm': 3.055565357208252, 'learning_rate': 4.4444444444444447e-05, 'epoch': 1.67}
{'loss': 0.9179, 'grad_norm': 1.3211004734039307, 'learning_rate': 4.290123456790124e-05, 'epoch': 1.71}
{'loss': 0.9504, 'grad_norm': 2.1807422637939453, 'learning_rate': 4.135802469135803e-05, 'epoch': 1.76}
{'loss': 0.9157, 'grad_norm': 2.500626564025879, 'learning_rate': 3.981481481481482e-05, 'epoch': 1.81}
{'loss': 0.9098, 'grad_norm': 1.6339634656906128, 'learning_rate': 3.82716049382716e-05, 'epoch': 1.85}
{'loss': 0.9038, 'grad_norm': 2.2868552207946777, 'learning_rate': 3.67283950617284e-05, 'epoch': 1.9}
{'loss': 0.8161, 'grad_norm': 0.954357922077179, 'learning_rate': 3.518518518518519e-05, 'epoch': 1.94}
{'loss': 0.904, 'grad_norm': 1.3713328838348389, 'learning_rate': 3.364197530864198e-05, 'epoch': 1.99}
{'eval_loss': 0.8640954494476318, 'eval_accuracy': 0.65, 'eval_micro_f1': 0.65, 'eval_macro_f1': 0.2626262626262626, 'eval_runtime': 2.3716, 'eval_samples_per_second': 472.262, 'eval_steps_per_second': 29.516, 'epoch': 2.0}
{'loss': 0.9066, 'grad_norm': 1.0620026588439941, 'learning_rate': 3.209876543209876e-05, 'epoch': 2.04}
{'loss': 0.8714, 'grad_norm': 1.5031123161315918, 'learning_rate': 3.055555555555556e-05, 'epoch': 2.08}
{'loss': 0.8447, 'grad_norm': 2.3338229656219482, 'learning_rate': 2.9012345679012347e-05, 'epoch': 2.13}
{'loss': 0.8999, 'grad_norm': 1.4843446016311646, 'learning_rate': 2.7469135802469138e-05, 'epoch': 2.18}
{'loss': 0.8949, 'grad_norm': 1.445176362991333, 'learning_rate': 2.5925925925925925e-05, 'epoch': 2.22}
{'loss': 0.9702, 'grad_norm': 2.305022716522217, 'learning_rate': 2.438271604938272e-05, 'epoch': 2.27}
{'loss': 0.9011, 'grad_norm': 1.7759315967559814, 'learning_rate': 2.2839506172839506e-05, 'epoch': 2.31}
{'loss': 0.8498, 'grad_norm': 1.78304922580719, 'learning_rate': 2.1296296296296296e-05, 'epoch': 2.36}
{'loss': 0.9613, 'grad_norm': 2.8322105407714844, 'learning_rate': 1.9753086419753087e-05, 'epoch': 2.41}
{'loss': 0.89, 'grad_norm': 1.7655363082885742, 'learning_rate': 1.8209876543209877e-05, 'epoch': 2.45}
{'loss': 0.9318, 'grad_norm': 1.6212435960769653, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.5}
{'loss': 0.8558, 'grad_norm': 4.668675422668457, 'learning_rate': 1.5123456790123458e-05, 'epoch': 2.55}
{'loss': 0.9313, 'grad_norm': 2.090531349182129, 'learning_rate': 1.3580246913580247e-05, 'epoch': 2.59}
{'loss': 0.9218, 'grad_norm': 1.761045217514038, 'learning_rate': 1.2037037037037037e-05, 'epoch': 2.64}
{'loss': 0.9027, 'grad_norm': 2.325505018234253, 'learning_rate': 1.0493827160493827e-05, 'epoch': 2.69}
{'loss': 0.8738, 'grad_norm': 2.4265871047973633, 'learning_rate': 8.950617283950618e-06, 'epoch': 2.73}
{'loss': 0.9372, 'grad_norm': 2.8371775150299072, 'learning_rate': 7.4074074074074075e-06, 'epoch': 2.78}
{'loss': 0.9646, 'grad_norm': 1.4469560384750366, 'learning_rate': 5.864197530864198e-06, 'epoch': 2.82}
{'loss': 0.9398, 'grad_norm': 2.1235640048980713, 'learning_rate': 4.3209876543209875e-06, 'epoch': 2.87}
{'loss': 0.8875, 'grad_norm': 1.6603490114212036, 'learning_rate': 2.777777777777778e-06, 'epoch': 2.92}
{'loss': 0.9541, 'grad_norm': 3.568488359451294, 'learning_rate': 1.234567901234568e-06, 'epoch': 2.96}
{'eval_loss': 0.859452486038208, 'eval_accuracy': 0.65, 'eval_micro_f1': 0.65, 'eval_macro_f1': 0.2626262626262626, 'eval_runtime': 2.3637, 'eval_samples_per_second': 473.829, 'eval_steps_per_second': 29.614, 'epoch': 3.0}
{'train_runtime': 32.1289, 'train_samples_per_second': 322.327, 'train_steps_per_second': 20.169, 'train_loss': 0.9243651724156038, 'epoch': 3.0}
100%|█████████████████████████████████████████████████████████████████████████████████████████████| 70/70 [00:02<00:00, 30.07it/s]
2024-10-29 23:09:35 - INFO - __main__ - Evaluation results: {'eval_loss': 0.859452486038208, 'eval_accuracy': 0.65, 'eval_micro_f1': 0.65, 'eval_macro_f1': 0.2626262626262626, 'eval_runtime': 2.3617, 'eval_samples_per_second': 474.236, 'eval_steps_per_second': 29.64, 'epoch': 3.0}
2024-10-29 23:09:35 - INFO - __main__ - Training complete.
