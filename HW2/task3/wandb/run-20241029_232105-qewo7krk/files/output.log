2024-10-29 23:21:06 - INFO - __main__ - Initializing training script
2024-10-29 23:21:06 - INFO - __main__ - Loading dataset...
2024-10-29 23:21:06 - INFO - __main__ - Loading model and tokenizer...
Some weights of RobertaForSequenceClassificationWithAdapters were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'roberta.encoder.layer.0.output.adapter_after_ffn.down_project.bias', 'roberta.encoder.layer.0.output.adapter_after_ffn.down_project.weight', 'roberta.encoder.layer.0.output.adapter_after_ffn.up_project.bias', 'roberta.encoder.layer.0.output.adapter_after_ffn.up_project.weight', 'roberta.encoder.layer.0.output.adapter_before_ffn.down_project.bias', 'roberta.encoder.layer.0.output.adapter_before_ffn.down_project.weight', 'roberta.encoder.layer.0.output.adapter_before_ffn.up_project.bias', 'roberta.encoder.layer.0.output.adapter_before_ffn.up_project.weight', 'roberta.encoder.layer.1.output.adapter_after_ffn.down_project.bias', 'roberta.encoder.layer.1.output.adapter_after_ffn.down_project.weight', 'roberta.encoder.layer.1.output.adapter_after_ffn.up_project.bias', 'roberta.encoder.layer.1.output.adapter_after_ffn.up_project.weight', 'roberta.encoder.layer.1.output.adapter_before_ffn.down_project.bias', 'roberta.encoder.layer.1.output.adapter_before_ffn.down_project.weight', 'roberta.encoder.layer.1.output.adapter_before_ffn.up_project.bias', 'roberta.encoder.layer.1.output.adapter_before_ffn.up_project.weight', 'roberta.encoder.layer.10.output.adapter_after_ffn.down_project.bias', 'roberta.encoder.layer.10.output.adapter_after_ffn.down_project.weight', 'roberta.encoder.layer.10.output.adapter_after_ffn.up_project.bias', 'roberta.encoder.layer.10.output.adapter_after_ffn.up_project.weight', 'roberta.encoder.layer.10.output.adapter_before_ffn.down_project.bias', 'roberta.encoder.layer.10.output.adapter_before_ffn.down_project.weight', 'roberta.encoder.layer.10.output.adapter_before_ffn.up_project.bias', 'roberta.encoder.layer.10.output.adapter_before_ffn.up_project.weight', 'roberta.encoder.layer.11.output.adapter_after_ffn.down_project.bias', 'roberta.encoder.layer.11.output.adapter_after_ffn.down_project.weight', 'roberta.encoder.layer.11.output.adapter_after_ffn.up_project.bias', 'roberta.encoder.layer.11.output.adapter_after_ffn.up_project.weight', 'roberta.encoder.layer.11.output.adapter_before_ffn.down_project.bias', 'roberta.encoder.layer.11.output.adapter_before_ffn.down_project.weight', 'roberta.encoder.layer.11.output.adapter_before_ffn.up_project.bias', 'roberta.encoder.layer.11.output.adapter_before_ffn.up_project.weight', 'roberta.encoder.layer.2.output.adapter_after_ffn.down_project.bias', 'roberta.encoder.layer.2.output.adapter_after_ffn.down_project.weight', 'roberta.encoder.layer.2.output.adapter_after_ffn.up_project.bias', 'roberta.encoder.layer.2.output.adapter_after_ffn.up_project.weight', 'roberta.encoder.layer.2.output.adapter_before_ffn.down_project.bias', 'roberta.encoder.layer.2.output.adapter_before_ffn.down_project.weight', 'roberta.encoder.layer.2.output.adapter_before_ffn.up_project.bias', 'roberta.encoder.layer.2.output.adapter_before_ffn.up_project.weight', 'roberta.encoder.layer.3.output.adapter_after_ffn.down_project.bias', 'roberta.encoder.layer.3.output.adapter_after_ffn.down_project.weight', 'roberta.encoder.layer.3.output.adapter_after_ffn.up_project.bias', 'roberta.encoder.layer.3.output.adapter_after_ffn.up_project.weight', 'roberta.encoder.layer.3.output.adapter_before_ffn.down_project.bias', 'roberta.encoder.layer.3.output.adapter_before_ffn.down_project.weight', 'roberta.encoder.layer.3.output.adapter_before_ffn.up_project.bias', 'roberta.encoder.layer.3.output.adapter_before_ffn.up_project.weight', 'roberta.encoder.layer.4.output.adapter_after_ffn.down_project.bias', 'roberta.encoder.layer.4.output.adapter_after_ffn.down_project.weight', 'roberta.encoder.layer.4.output.adapter_after_ffn.up_project.bias', 'roberta.encoder.layer.4.output.adapter_after_ffn.up_project.weight', 'roberta.encoder.layer.4.output.adapter_before_ffn.down_project.bias', 'roberta.encoder.layer.4.output.adapter_before_ffn.down_project.weight', 'roberta.e
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Map: 100%|██████████████████████████████████████████████████████████████████████████| 3452/3452 [00:00<00:00, 17997.52 examples/s]
Map: 100%|██████████████████████████████████████████████████████████████████████████| 1120/1120 [00:00<00:00, 16205.34 examples/s]
/root/HW2/task3/train.py:106: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
2024-10-29 23:21:12 - INFO - __main__ - Starting training...
  0%|                                                                                                     | 0/648 [00:00<?, ?it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:1160: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.
  warnings.warn(
 33%|██████████████████████████████▎                                                            | 216/648 [00:18<00:30, 14.09it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:1160: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.
{'loss': 1.0205, 'grad_norm': 2.187779426574707, 'learning_rate': 9.84567901234568e-05, 'epoch': 0.05}
{'loss': 0.8554, 'grad_norm': 0.9535106420516968, 'learning_rate': 9.691358024691359e-05, 'epoch': 0.09}
{'loss': 1.0087, 'grad_norm': 1.7985780239105225, 'learning_rate': 9.537037037037038e-05, 'epoch': 0.14}
{'loss': 0.9367, 'grad_norm': 3.896064519882202, 'learning_rate': 9.382716049382717e-05, 'epoch': 0.19}
{'loss': 0.9482, 'grad_norm': 1.8805996179580688, 'learning_rate': 9.228395061728396e-05, 'epoch': 0.23}
{'loss': 0.8055, 'grad_norm': 3.5324065685272217, 'learning_rate': 9.074074074074075e-05, 'epoch': 0.28}
{'loss': 0.8768, 'grad_norm': 14.049420356750488, 'learning_rate': 8.919753086419754e-05, 'epoch': 0.32}
{'loss': 0.9485, 'grad_norm': 9.864104270935059, 'learning_rate': 8.765432098765433e-05, 'epoch': 0.37}
{'loss': 0.7587, 'grad_norm': 3.57824444770813, 'learning_rate': 8.611111111111112e-05, 'epoch': 0.42}
{'loss': 0.7066, 'grad_norm': 3.2669551372528076, 'learning_rate': 8.456790123456791e-05, 'epoch': 0.46}
{'loss': 0.7309, 'grad_norm': 5.718508243560791, 'learning_rate': 8.30246913580247e-05, 'epoch': 0.51}
{'loss': 0.627, 'grad_norm': 4.7776570320129395, 'learning_rate': 8.148148148148148e-05, 'epoch': 0.56}
{'loss': 0.6887, 'grad_norm': 14.490473747253418, 'learning_rate': 7.993827160493828e-05, 'epoch': 0.6}
{'loss': 0.6758, 'grad_norm': 3.1318914890289307, 'learning_rate': 7.839506172839507e-05, 'epoch': 0.65}
{'loss': 0.659, 'grad_norm': 2.9137649536132812, 'learning_rate': 7.685185185185185e-05, 'epoch': 0.69}
{'loss': 0.6525, 'grad_norm': 4.148121356964111, 'learning_rate': 7.530864197530865e-05, 'epoch': 0.74}
{'loss': 0.7038, 'grad_norm': 9.60193157196045, 'learning_rate': 7.376543209876543e-05, 'epoch': 0.79}
{'loss': 0.6834, 'grad_norm': 17.09181785583496, 'learning_rate': 7.222222222222222e-05, 'epoch': 0.83}
{'loss': 0.6498, 'grad_norm': 3.3177547454833984, 'learning_rate': 7.067901234567902e-05, 'epoch': 0.88}
{'loss': 0.6591, 'grad_norm': 4.473947525024414, 'learning_rate': 6.91358024691358e-05, 'epoch': 0.93}
{'loss': 0.6183, 'grad_norm': 6.074099540710449, 'learning_rate': 6.759259259259259e-05, 'epoch': 0.97}
  warnings.warn(                                                                                                                  
{'eval_loss': 0.5970786809921265, 'eval_accuracy': 0.7642857142857142, 'eval_micro_f1': 0.7642857142857142, 'eval_macro_f1': 0.5300823318341567, 'eval_runtime': 2.5216, 'eval_samples_per_second': 444.156, 'eval_steps_per_second': 27.76, 'epoch': 1.0}
 67%|████████████████████████████████████████████████████████████▋                              | 432/648 [00:36<00:15, 14.07it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:1160: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.
{'loss': 0.6065, 'grad_norm': 5.931227207183838, 'learning_rate': 6.60493827160494e-05, 'epoch': 1.02}
{'loss': 0.5826, 'grad_norm': 3.6394355297088623, 'learning_rate': 6.450617283950617e-05, 'epoch': 1.06}
{'loss': 0.6713, 'grad_norm': 5.750736236572266, 'learning_rate': 6.296296296296296e-05, 'epoch': 1.11}
{'loss': 0.596, 'grad_norm': 5.143309116363525, 'learning_rate': 6.141975308641975e-05, 'epoch': 1.16}
{'loss': 0.4993, 'grad_norm': 6.197649955749512, 'learning_rate': 5.987654320987654e-05, 'epoch': 1.2}
{'loss': 0.5343, 'grad_norm': 3.4113988876342773, 'learning_rate': 5.833333333333334e-05, 'epoch': 1.25}
{'loss': 0.5451, 'grad_norm': 5.867578983306885, 'learning_rate': 5.679012345679012e-05, 'epoch': 1.3}
{'loss': 0.5217, 'grad_norm': 5.866772651672363, 'learning_rate': 5.524691358024692e-05, 'epoch': 1.34}
{'loss': 0.5124, 'grad_norm': 6.535226821899414, 'learning_rate': 5.370370370370371e-05, 'epoch': 1.39}
{'loss': 0.5243, 'grad_norm': 2.482907295227051, 'learning_rate': 5.2160493827160494e-05, 'epoch': 1.44}
{'loss': 0.5148, 'grad_norm': 5.0703043937683105, 'learning_rate': 5.061728395061729e-05, 'epoch': 1.48}
{'loss': 0.5644, 'grad_norm': 8.736950874328613, 'learning_rate': 4.9074074074074075e-05, 'epoch': 1.53}
{'loss': 0.5613, 'grad_norm': 4.77789831161499, 'learning_rate': 4.7530864197530866e-05, 'epoch': 1.57}
{'loss': 0.611, 'grad_norm': 9.41313648223877, 'learning_rate': 4.5987654320987656e-05, 'epoch': 1.62}
{'loss': 0.5356, 'grad_norm': 9.43821907043457, 'learning_rate': 4.4444444444444447e-05, 'epoch': 1.67}
{'loss': 0.483, 'grad_norm': 4.379701614379883, 'learning_rate': 4.290123456790124e-05, 'epoch': 1.71}
{'loss': 0.5461, 'grad_norm': 4.327096462249756, 'learning_rate': 4.135802469135803e-05, 'epoch': 1.76}
{'loss': 0.528, 'grad_norm': 6.921766757965088, 'learning_rate': 3.981481481481482e-05, 'epoch': 1.81}
{'loss': 0.626, 'grad_norm': 7.175541877746582, 'learning_rate': 3.82716049382716e-05, 'epoch': 1.85}
{'loss': 0.5403, 'grad_norm': 3.5294137001037598, 'learning_rate': 3.67283950617284e-05, 'epoch': 1.9}
{'loss': 0.5288, 'grad_norm': 4.334995746612549, 'learning_rate': 3.518518518518519e-05, 'epoch': 1.94}
{'loss': 0.5173, 'grad_norm': 6.925222396850586, 'learning_rate': 3.364197530864198e-05, 'epoch': 1.99}
  warnings.warn(                                                                                                                  
{'eval_loss': 0.4558389484882355, 'eval_accuracy': 0.83125, 'eval_micro_f1': 0.83125, 'eval_macro_f1': 0.72540343456569, 'eval_runtime': 2.4765, 'eval_samples_per_second': 452.252, 'eval_steps_per_second': 28.266, 'epoch': 2.0}
 77%|██████████████████████████████████████████████████████████████████████▏                    | 500/648 [00:41<00:10, 14.02it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:1160: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.
{'loss': 0.5346, 'grad_norm': 4.355916500091553, 'learning_rate': 3.209876543209876e-05, 'epoch': 2.04}
{'loss': 0.3697, 'grad_norm': 4.457491874694824, 'learning_rate': 3.055555555555556e-05, 'epoch': 2.08}
{'loss': 0.4321, 'grad_norm': 14.029434204101562, 'learning_rate': 2.9012345679012347e-05, 'epoch': 2.13}
{'loss': 0.5779, 'grad_norm': 5.073805809020996, 'learning_rate': 2.7469135802469138e-05, 'epoch': 2.18}
{'loss': 0.5126, 'grad_norm': 7.470973968505859, 'learning_rate': 2.5925925925925925e-05, 'epoch': 2.22}
{'loss': 0.4612, 'grad_norm': 11.730860710144043, 'learning_rate': 2.438271604938272e-05, 'epoch': 2.27}
{'loss': 0.4475, 'grad_norm': 7.275988578796387, 'learning_rate': 2.2839506172839506e-05, 'epoch': 2.31}
  warnings.warn(
100%|██████████████████████████████████████████████████████████████████████████████████████████▊| 647/648 [00:52<00:00, 13.83it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:1160: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.
{'loss': 0.4508, 'grad_norm': 3.558110475540161, 'learning_rate': 2.1296296296296296e-05, 'epoch': 2.36}
{'loss': 0.4501, 'grad_norm': 10.44367504119873, 'learning_rate': 1.9753086419753087e-05, 'epoch': 2.41}
{'loss': 0.3795, 'grad_norm': 4.293909072875977, 'learning_rate': 1.8209876543209877e-05, 'epoch': 2.45}
{'loss': 0.3898, 'grad_norm': 6.654184818267822, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.5}
{'loss': 0.3729, 'grad_norm': 4.088515281677246, 'learning_rate': 1.5123456790123458e-05, 'epoch': 2.55}
{'loss': 0.3563, 'grad_norm': 7.259599208831787, 'learning_rate': 1.3580246913580247e-05, 'epoch': 2.59}
{'loss': 0.4864, 'grad_norm': 10.605818748474121, 'learning_rate': 1.2037037037037037e-05, 'epoch': 2.64}
{'loss': 0.4315, 'grad_norm': 7.36015510559082, 'learning_rate': 1.0493827160493827e-05, 'epoch': 2.69}
{'loss': 0.4488, 'grad_norm': 4.151485443115234, 'learning_rate': 8.950617283950618e-06, 'epoch': 2.73}
{'loss': 0.5352, 'grad_norm': 11.869129180908203, 'learning_rate': 7.4074074074074075e-06, 'epoch': 2.78}
{'loss': 0.43, 'grad_norm': 6.647990703582764, 'learning_rate': 5.864197530864198e-06, 'epoch': 2.82}
{'loss': 0.4835, 'grad_norm': 12.660897254943848, 'learning_rate': 4.3209876543209875e-06, 'epoch': 2.87}
{'loss': 0.3232, 'grad_norm': 6.635498523712158, 'learning_rate': 2.777777777777778e-06, 'epoch': 2.92}
{'loss': 0.4122, 'grad_norm': 4.020395278930664, 'learning_rate': 1.234567901234568e-06, 'epoch': 2.96}
  warnings.warn(
100%|███████████████████████████████████████████████████████████████████████████████████████████| 648/648 [00:56<00:00, 11.55it/s]
2024-10-29 23:22:08 - INFO - __main__ - Starting evaluation...                                                                    
{'eval_loss': 0.44240838289260864, 'eval_accuracy': 0.8366071428571429, 'eval_micro_f1': 0.8366071428571429, 'eval_macro_f1': 0.7195321826325286, 'eval_runtime': 2.5244, 'eval_samples_per_second': 443.67, 'eval_steps_per_second': 27.729, 'epoch': 3.0}
{'train_runtime': 56.112, 'train_samples_per_second': 184.559, 'train_steps_per_second': 11.548, 'train_loss': 0.5855851986525972, 'epoch': 3.0}
/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:1160: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.
  warnings.warn(
100%|█████████████████████████████████████████████████████████████████████████████████████████████| 70/70 [00:02<00:00, 28.43it/s]
2024-10-29 23:22:11 - INFO - __main__ - Evaluation results: {'eval_loss': 0.44240838289260864, 'eval_accuracy': 0.8366071428571429, 'eval_micro_f1': 0.8366071428571429, 'eval_macro_f1': 0.7195321826325286, 'eval_runtime': 2.4979, 'eval_samples_per_second': 448.381, 'eval_steps_per_second': 28.024, 'epoch': 3.0}
2024-10-29 23:22:11 - INFO - __main__ - Training complete.
