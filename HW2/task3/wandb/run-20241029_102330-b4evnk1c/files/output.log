2024-10-29 10:23:32 - INFO - __main__ - Initializing training script
2024-10-29 10:23:32 - INFO - __main__ - Loading dataset...
2024-10-29 10:23:40 - INFO - __main__ - Loading model and tokenizer...
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/root/HW2/task2/train.py:101: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
2024-10-29 10:23:46 - INFO - __main__ - Starting training...
  0%|                                                                                                     | 0/642 [00:00<?, ?it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
 33%|██████████████████████████████▎                                                            | 214/642 [00:43<01:16,  5.56it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
{'loss': 1.3354, 'grad_norm': 4.840004920959473, 'learning_rate': 4.922118380062305e-05, 'epoch': 0.05}
{'loss': 0.692, 'grad_norm': 5.8417744636535645, 'learning_rate': 4.844236760124611e-05, 'epoch': 0.09}
{'loss': 0.5104, 'grad_norm': 14.394820213317871, 'learning_rate': 4.766355140186916e-05, 'epoch': 0.14}
{'loss': 0.3471, 'grad_norm': 5.316218852996826, 'learning_rate': 4.6884735202492216e-05, 'epoch': 0.19}
{'loss': 0.3904, 'grad_norm': 16.68250846862793, 'learning_rate': 4.6105919003115266e-05, 'epoch': 0.23}
{'loss': 0.4076, 'grad_norm': 6.080106258392334, 'learning_rate': 4.532710280373832e-05, 'epoch': 0.28}
{'loss': 0.3021, 'grad_norm': 4.161431312561035, 'learning_rate': 4.454828660436137e-05, 'epoch': 0.33}
{'loss': 0.3074, 'grad_norm': 7.106492519378662, 'learning_rate': 4.376947040498442e-05, 'epoch': 0.37}
{'loss': 0.2839, 'grad_norm': 4.762688159942627, 'learning_rate': 4.299065420560748e-05, 'epoch': 0.42}
{'loss': 0.4432, 'grad_norm': 13.299881935119629, 'learning_rate': 4.221183800623053e-05, 'epoch': 0.47}
{'loss': 0.3655, 'grad_norm': 5.864988327026367, 'learning_rate': 4.1433021806853586e-05, 'epoch': 0.51}
{'loss': 0.3896, 'grad_norm': 2.4138810634613037, 'learning_rate': 4.0654205607476636e-05, 'epoch': 0.56}
{'loss': 0.3794, 'grad_norm': 11.81896686553955, 'learning_rate': 3.987538940809969e-05, 'epoch': 0.61}
{'loss': 0.3372, 'grad_norm': 4.631453990936279, 'learning_rate': 3.909657320872274e-05, 'epoch': 0.65}
{'loss': 0.3036, 'grad_norm': 8.394004821777344, 'learning_rate': 3.831775700934579e-05, 'epoch': 0.7}
{'loss': 0.2497, 'grad_norm': 6.924186706542969, 'learning_rate': 3.753894080996885e-05, 'epoch': 0.75}
{'loss': 0.2573, 'grad_norm': 12.751559257507324, 'learning_rate': 3.67601246105919e-05, 'epoch': 0.79}
{'loss': 0.2911, 'grad_norm': 6.995765686035156, 'learning_rate': 3.5981308411214956e-05, 'epoch': 0.84}
{'loss': 0.3324, 'grad_norm': 9.314169883728027, 'learning_rate': 3.5202492211838006e-05, 'epoch': 0.89}
{'loss': 0.3728, 'grad_norm': 5.919923305511475, 'learning_rate': 3.442367601246106e-05, 'epoch': 0.93}
{'loss': 0.354, 'grad_norm': 8.957326889038086, 'learning_rate': 3.364485981308411e-05, 'epoch': 0.98}
  warnings.warn(                                                                                                                  
{'eval_loss': 0.24983635544776917, 'eval_accuracy': 0.9105263157894737, 'eval_micro_f1': 0.9105263157894737, 'eval_macro_f1': 0.9072982419064177, 'eval_runtime': 1.8022, 'eval_samples_per_second': 421.713, 'eval_steps_per_second': 13.317, 'epoch': 1.0}
 67%|████████████████████████████████████████████████████████████▋                              | 428/642 [01:25<00:39,  5.39it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
{'loss': 0.277, 'grad_norm': 4.609667778015137, 'learning_rate': 3.286604361370716e-05, 'epoch': 1.03}
{'loss': 0.2475, 'grad_norm': 8.936817169189453, 'learning_rate': 3.208722741433022e-05, 'epoch': 1.07}
{'loss': 0.2251, 'grad_norm': 4.157405853271484, 'learning_rate': 3.130841121495327e-05, 'epoch': 1.12}
{'loss': 0.2347, 'grad_norm': 12.17159652709961, 'learning_rate': 3.0529595015576326e-05, 'epoch': 1.17}
{'loss': 0.2126, 'grad_norm': 6.825812816619873, 'learning_rate': 2.9750778816199376e-05, 'epoch': 1.21}
{'loss': 0.2341, 'grad_norm': 3.853351593017578, 'learning_rate': 2.897196261682243e-05, 'epoch': 1.26}
{'loss': 0.19, 'grad_norm': 8.309500694274902, 'learning_rate': 2.8193146417445482e-05, 'epoch': 1.31}
{'loss': 0.2267, 'grad_norm': 5.313752174377441, 'learning_rate': 2.7414330218068536e-05, 'epoch': 1.36}
{'loss': 0.2808, 'grad_norm': 4.6641011238098145, 'learning_rate': 2.663551401869159e-05, 'epoch': 1.4}
{'loss': 0.2372, 'grad_norm': 6.7185516357421875, 'learning_rate': 2.585669781931464e-05, 'epoch': 1.45}
{'loss': 0.2211, 'grad_norm': 13.872598648071289, 'learning_rate': 2.5077881619937692e-05, 'epoch': 1.5}
{'loss': 0.1943, 'grad_norm': 5.925553798675537, 'learning_rate': 2.429906542056075e-05, 'epoch': 1.54}
{'loss': 0.249, 'grad_norm': 4.096357345581055, 'learning_rate': 2.3520249221183802e-05, 'epoch': 1.59}
{'loss': 0.1731, 'grad_norm': 4.910252094268799, 'learning_rate': 2.2741433021806856e-05, 'epoch': 1.64}
{'loss': 0.1657, 'grad_norm': 8.949597358703613, 'learning_rate': 2.196261682242991e-05, 'epoch': 1.68}
{'loss': 0.1868, 'grad_norm': 6.705268859863281, 'learning_rate': 2.118380062305296e-05, 'epoch': 1.73}
{'loss': 0.2258, 'grad_norm': 5.290336608886719, 'learning_rate': 2.0404984423676012e-05, 'epoch': 1.78}
{'loss': 0.1906, 'grad_norm': 3.005171537399292, 'learning_rate': 1.9626168224299065e-05, 'epoch': 1.82}
{'loss': 0.1994, 'grad_norm': 1.0090339183807373, 'learning_rate': 1.884735202492212e-05, 'epoch': 1.87}
{'loss': 0.155, 'grad_norm': 7.30902099609375, 'learning_rate': 1.8068535825545172e-05, 'epoch': 1.92}
{'loss': 0.2175, 'grad_norm': 3.292771339416504, 'learning_rate': 1.7289719626168225e-05, 'epoch': 1.96}
  warnings.warn(                                                                                                                  
{'eval_loss': 0.20564541220664978, 'eval_accuracy': 0.9328947368421052, 'eval_micro_f1': 0.9328947368421052, 'eval_macro_f1': 0.9309305063563053, 'eval_runtime': 1.9003, 'eval_samples_per_second': 399.941, 'eval_steps_per_second': 12.63, 'epoch': 2.0}
 78%|██████████████████████████████████████████████████████████████████████▊                    | 500/642 [01:39<00:27,  5.21it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
{'loss': 0.1526, 'grad_norm': 6.369246482849121, 'learning_rate': 1.6510903426791275e-05, 'epoch': 2.01}
{'loss': 0.1481, 'grad_norm': 4.779629230499268, 'learning_rate': 1.573208722741433e-05, 'epoch': 2.06}
{'loss': 0.0626, 'grad_norm': 7.201981544494629, 'learning_rate': 1.4953271028037382e-05, 'epoch': 2.1}
{'loss': 0.2138, 'grad_norm': 1.3848956823349, 'learning_rate': 1.4174454828660435e-05, 'epoch': 2.15}
{'loss': 0.1705, 'grad_norm': 6.59314489364624, 'learning_rate': 1.3395638629283489e-05, 'epoch': 2.2}
{'loss': 0.1632, 'grad_norm': 4.083446979522705, 'learning_rate': 1.2616822429906542e-05, 'epoch': 2.24}
{'loss': 0.0966, 'grad_norm': 4.685632705688477, 'learning_rate': 1.1838006230529595e-05, 'epoch': 2.29}
{'loss': 0.1587, 'grad_norm': 1.260042428970337, 'learning_rate': 1.1059190031152649e-05, 'epoch': 2.34}
  warnings.warn(
100%|███████████████████████████████████████████████████████████████████████████████████████████| 642/642 [02:08<00:00,  5.38it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
{'loss': 0.0984, 'grad_norm': 6.0132551193237305, 'learning_rate': 1.02803738317757e-05, 'epoch': 2.38}
{'loss': 0.0807, 'grad_norm': 2.983900308609009, 'learning_rate': 9.501557632398754e-06, 'epoch': 2.43}
{'loss': 0.0793, 'grad_norm': 3.160750389099121, 'learning_rate': 8.722741433021807e-06, 'epoch': 2.48}
{'loss': 0.1689, 'grad_norm': 16.77124786376953, 'learning_rate': 7.94392523364486e-06, 'epoch': 2.52}
{'loss': 0.2577, 'grad_norm': 14.928730010986328, 'learning_rate': 7.165109034267913e-06, 'epoch': 2.57}
{'loss': 0.1613, 'grad_norm': 5.2016987800598145, 'learning_rate': 6.386292834890965e-06, 'epoch': 2.62}
{'loss': 0.1565, 'grad_norm': 6.931201934814453, 'learning_rate': 5.607476635514019e-06, 'epoch': 2.66}
{'loss': 0.1462, 'grad_norm': 4.793679237365723, 'learning_rate': 4.828660436137072e-06, 'epoch': 2.71}
{'loss': 0.1101, 'grad_norm': 1.367827296257019, 'learning_rate': 4.0498442367601245e-06, 'epoch': 2.76}
{'loss': 0.1008, 'grad_norm': 0.3566505014896393, 'learning_rate': 3.2710280373831774e-06, 'epoch': 2.8}
{'loss': 0.0936, 'grad_norm': 3.8959293365478516, 'learning_rate': 2.4922118380062308e-06, 'epoch': 2.85}
{'loss': 0.1025, 'grad_norm': 2.437002658843994, 'learning_rate': 1.7133956386292835e-06, 'epoch': 2.9}
{'loss': 0.1412, 'grad_norm': 4.635857105255127, 'learning_rate': 9.345794392523364e-07, 'epoch': 2.94}
{'loss': 0.0702, 'grad_norm': 1.5211069583892822, 'learning_rate': 1.5576323987538942e-07, 'epoch': 2.99}
  warnings.warn(
100%|███████████████████████████████████████████████████████████████████████████████████████████| 642/642 [02:12<00:00,  4.83it/s]
2024-10-29 10:25:59 - INFO - __main__ - Starting evaluation...                                                                    
{'eval_loss': 0.1937248259782791, 'eval_accuracy': 0.9394736842105263, 'eval_micro_f1': 0.9394736842105263, 'eval_macro_f1': 0.9378002090829497, 'eval_runtime': 1.9628, 'eval_samples_per_second': 387.201, 'eval_steps_per_second': 12.227, 'epoch': 3.0}
{'train_runtime': 132.844, 'train_samples_per_second': 154.467, 'train_steps_per_second': 4.833, 'train_loss': 0.2513722055403057, 'epoch': 3.0}
/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
100%|█████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:01<00:00, 13.93it/s]
2024-10-29 10:26:01 - INFO - __main__ - Evaluation results: {'eval_loss': 0.1937248259782791, 'eval_accuracy': 0.9394736842105263, 'eval_micro_f1': 0.9394736842105263, 'eval_macro_f1': 0.9378002090829497, 'eval_runtime': 1.8026, 'eval_samples_per_second': 421.605, 'eval_steps_per_second': 13.314, 'epoch': 3.0}
2024-10-29 10:26:01 - INFO - __main__ - Training complete.
