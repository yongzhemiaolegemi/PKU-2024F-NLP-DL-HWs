2024-10-29 23:51:36 - INFO - __main__ - Initializing training script
2024-10-29 23:51:36 - INFO - __main__ - Loading dataset...
2024-10-29 23:51:36 - INFO - __main__ - Loading model and tokenizer...
Some weights of RobertaForSequenceClassificationWithAdapters were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'roberta.encoder.layer.0.output.adapter_after_ffn.down_project.bias', 'roberta.encoder.layer.0.output.adapter_after_ffn.down_project.weight', 'roberta.encoder.layer.0.output.adapter_after_ffn.up_project.bias', 'roberta.encoder.layer.0.output.adapter_after_ffn.up_project.weight', 'roberta.encoder.layer.0.output.adapter_before_ffn.down_project.bias', 'roberta.encoder.layer.0.output.adapter_before_ffn.down_project.weight', 'roberta.encoder.layer.0.output.adapter_before_ffn.up_project.bias', 'roberta.encoder.layer.0.output.adapter_before_ffn.up_project.weight', 'roberta.encoder.layer.1.output.adapter_after_ffn.down_project.bias', 'roberta.encoder.layer.1.output.adapter_after_ffn.down_project.weight', 'roberta.encoder.layer.1.output.adapter_after_ffn.up_project.bias', 'roberta.encoder.layer.1.output.adapter_after_ffn.up_project.weight', 'roberta.encoder.layer.1.output.adapter_before_ffn.down_project.bias', 'roberta.encoder.layer.1.output.adapter_before_ffn.down_project.weight', 'roberta.encoder.layer.1.output.adapter_before_ffn.up_project.bias', 'roberta.encoder.layer.1.output.adapter_before_ffn.up_project.weight', 'roberta.encoder.layer.10.output.adapter_after_ffn.down_project.bias', 'roberta.encoder.layer.10.output.adapter_after_ffn.down_project.weight', 'roberta.encoder.layer.10.output.adapter_after_ffn.up_project.bias', 'roberta.encoder.layer.10.output.adapter_after_ffn.up_project.weight', 'roberta.encoder.layer.10.output.adapter_before_ffn.down_project.bias', 'roberta.encoder.layer.10.output.adapter_before_ffn.down_project.weight', 'roberta.encoder.layer.10.output.adapter_before_ffn.up_project.bias', 'roberta.encoder.layer.10.output.adapter_before_ffn.up_project.weight', 'roberta.encoder.layer.11.output.adapter_after_ffn.down_project.bias', 'roberta.encoder.layer.11.output.adapter_after_ffn.down_project.weight', 'roberta.encoder.layer.11.output.adapter_after_ffn.up_project.bias', 'roberta.encoder.layer.11.output.adapter_after_ffn.up_project.weight', 'roberta.encoder.layer.11.output.adapter_before_ffn.down_project.bias', 'roberta.encoder.layer.11.output.adapter_before_ffn.down_project.weight', 'roberta.encoder.layer.11.output.adapter_before_ffn.up_project.bias', 'roberta.encoder.layer.11.output.adapter_before_ffn.up_project.weight', 'roberta.encoder.layer.2.output.adapter_after_ffn.down_project.bias', 'roberta.encoder.layer.2.output.adapter_after_ffn.down_project.weight', 'roberta.encoder.layer.2.output.adapter_after_ffn.up_project.bias', 'roberta.encoder.layer.2.output.adapter_after_ffn.up_project.weight', 'roberta.encoder.layer.2.output.adapter_before_ffn.down_project.bias', 'roberta.encoder.layer.2.output.adapter_before_ffn.down_project.weight', 'roberta.encoder.layer.2.output.adapter_before_ffn.up_project.bias', 'roberta.encoder.layer.2.output.adapter_before_ffn.up_project.weight', 'roberta.encoder.layer.3.output.adapter_after_ffn.down_project.bias', 'roberta.encoder.layer.3.output.adapter_after_ffn.down_project.weight', 'roberta.encoder.layer.3.output.adapter_after_ffn.up_project.bias', 'roberta.encoder.layer.3.output.adapter_after_ffn.up_project.weight', 'roberta.encoder.layer.3.output.adapter_before_ffn.down_project.bias', 'roberta.encoder.layer.3.output.adapter_before_ffn.down_project.weight', 'roberta.encoder.layer.3.output.adapter_before_ffn.up_project.bias', 'roberta.encoder.layer.3.output.adapter_before_ffn.up_project.weight', 'roberta.encoder.layer.4.output.adapter_after_ffn.down_project.bias', 'roberta.encoder.layer.4.output.adapter_after_ffn.down_project.weight', 'roberta.encoder.layer.4.output.adapter_after_ffn.up_project.bias', 'roberta.encoder.layer.4.output.adapter_after_ffn.up_project.weight', 'roberta.encoder.layer.4.output.adapter_before_ffn.down_project.bias', 'roberta.encoder.layer.4.output.adapter_before_ffn.down_project.weight', 'roberta.e
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Map: 100%|██████████████████████████████████████████████████████████████████████████| 3452/3452 [00:00<00:00, 15608.39 examples/s]
Map: 100%|██████████████████████████████████████████████████████████████████████████| 1120/1120 [00:00<00:00, 11006.71 examples/s]
/root/HW2/task3/train.py:106: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
2024-10-29 23:51:41 - INFO - __main__ - Starting training...
  0%|                                                                                                     | 0/648 [00:00<?, ?it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:1160: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.
  warnings.warn(
 33%|██████████████████████████████▎                                                            | 216/648 [00:18<00:31, 13.77it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:1160: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.
{'loss': 1.0619, 'grad_norm': 1.048200249671936, 'learning_rate': 9.84567901234568e-05, 'epoch': 0.05}
{'loss': 0.9731, 'grad_norm': 1.72986900806427, 'learning_rate': 9.691358024691359e-05, 'epoch': 0.09}
{'loss': 0.9379, 'grad_norm': 1.0095003843307495, 'learning_rate': 9.537037037037038e-05, 'epoch': 0.14}
{'loss': 0.9668, 'grad_norm': 2.924084186553955, 'learning_rate': 9.382716049382717e-05, 'epoch': 0.19}
{'loss': 0.8642, 'grad_norm': 3.689239501953125, 'learning_rate': 9.228395061728396e-05, 'epoch': 0.23}
{'loss': 0.6833, 'grad_norm': 2.0159342288970947, 'learning_rate': 9.074074074074075e-05, 'epoch': 0.28}
{'loss': 0.6844, 'grad_norm': 9.291339874267578, 'learning_rate': 8.919753086419754e-05, 'epoch': 0.32}
{'loss': 0.6831, 'grad_norm': 6.4429240226745605, 'learning_rate': 8.765432098765433e-05, 'epoch': 0.37}
{'loss': 0.7174, 'grad_norm': 5.073584079742432, 'learning_rate': 8.611111111111112e-05, 'epoch': 0.42}
{'loss': 0.601, 'grad_norm': 4.530278205871582, 'learning_rate': 8.456790123456791e-05, 'epoch': 0.46}
{'loss': 0.5955, 'grad_norm': 5.4277729988098145, 'learning_rate': 8.30246913580247e-05, 'epoch': 0.51}
{'loss': 0.7072, 'grad_norm': 3.842491865158081, 'learning_rate': 8.148148148148148e-05, 'epoch': 0.56}
{'loss': 0.631, 'grad_norm': 4.899762153625488, 'learning_rate': 7.993827160493828e-05, 'epoch': 0.6}
{'loss': 0.5946, 'grad_norm': 7.507837295532227, 'learning_rate': 7.839506172839507e-05, 'epoch': 0.65}
{'loss': 0.6362, 'grad_norm': 5.955442428588867, 'learning_rate': 7.685185185185185e-05, 'epoch': 0.69}
{'loss': 0.5428, 'grad_norm': 7.066659450531006, 'learning_rate': 7.530864197530865e-05, 'epoch': 0.74}
{'loss': 0.4911, 'grad_norm': 2.8035426139831543, 'learning_rate': 7.376543209876543e-05, 'epoch': 0.79}
{'loss': 0.6601, 'grad_norm': 2.392301082611084, 'learning_rate': 7.222222222222222e-05, 'epoch': 0.83}
{'loss': 0.5478, 'grad_norm': 6.002772808074951, 'learning_rate': 7.067901234567902e-05, 'epoch': 0.88}
{'loss': 0.5445, 'grad_norm': 6.452701568603516, 'learning_rate': 6.91358024691358e-05, 'epoch': 0.93}
{'loss': 0.4491, 'grad_norm': 4.110864639282227, 'learning_rate': 6.759259259259259e-05, 'epoch': 0.97}
  warnings.warn(                                                                                                                  
{'eval_loss': 0.4412773549556732, 'eval_accuracy': 0.8294642857142858, 'eval_micro_f1': 0.8294642857142858, 'eval_macro_f1': 0.7431671198586821, 'eval_runtime': 2.546, 'eval_samples_per_second': 439.912, 'eval_steps_per_second': 27.495, 'epoch': 1.0}
 67%|████████████████████████████████████████████████████████████▋                              | 432/648 [00:37<00:15, 13.68it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:1160: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.
{'loss': 0.6295, 'grad_norm': 6.120316982269287, 'learning_rate': 6.60493827160494e-05, 'epoch': 1.02}
{'loss': 0.5355, 'grad_norm': 5.395603656768799, 'learning_rate': 6.450617283950617e-05, 'epoch': 1.06}
{'loss': 0.5114, 'grad_norm': 5.465339660644531, 'learning_rate': 6.296296296296296e-05, 'epoch': 1.11}
{'loss': 0.4253, 'grad_norm': 4.935489654541016, 'learning_rate': 6.141975308641975e-05, 'epoch': 1.16}
{'loss': 0.5199, 'grad_norm': 4.411975860595703, 'learning_rate': 5.987654320987654e-05, 'epoch': 1.2}
{'loss': 0.5287, 'grad_norm': 4.432931900024414, 'learning_rate': 5.833333333333334e-05, 'epoch': 1.25}
{'loss': 0.4024, 'grad_norm': 4.675429344177246, 'learning_rate': 5.679012345679012e-05, 'epoch': 1.3}
{'loss': 0.5645, 'grad_norm': 8.744969367980957, 'learning_rate': 5.524691358024692e-05, 'epoch': 1.34}
{'loss': 0.5615, 'grad_norm': 5.692265033721924, 'learning_rate': 5.370370370370371e-05, 'epoch': 1.39}
{'loss': 0.4479, 'grad_norm': 3.4192423820495605, 'learning_rate': 5.2160493827160494e-05, 'epoch': 1.44}
{'loss': 0.52, 'grad_norm': 9.035340309143066, 'learning_rate': 5.061728395061729e-05, 'epoch': 1.48}
{'loss': 0.529, 'grad_norm': 13.21976089477539, 'learning_rate': 4.9074074074074075e-05, 'epoch': 1.53}
{'loss': 0.4407, 'grad_norm': 5.910806179046631, 'learning_rate': 4.7530864197530866e-05, 'epoch': 1.57}
{'loss': 0.4914, 'grad_norm': 4.277125358581543, 'learning_rate': 4.5987654320987656e-05, 'epoch': 1.62}
{'loss': 0.4968, 'grad_norm': 10.64963436126709, 'learning_rate': 4.4444444444444447e-05, 'epoch': 1.67}
{'loss': 0.4057, 'grad_norm': 5.064592361450195, 'learning_rate': 4.290123456790124e-05, 'epoch': 1.71}
{'loss': 0.3798, 'grad_norm': 7.173667907714844, 'learning_rate': 4.135802469135803e-05, 'epoch': 1.76}
{'loss': 0.5413, 'grad_norm': 6.3118462562561035, 'learning_rate': 3.981481481481482e-05, 'epoch': 1.81}
{'loss': 0.3625, 'grad_norm': 3.3781514167785645, 'learning_rate': 3.82716049382716e-05, 'epoch': 1.85}
{'loss': 0.4125, 'grad_norm': 9.097982406616211, 'learning_rate': 3.67283950617284e-05, 'epoch': 1.9}
{'loss': 0.3892, 'grad_norm': 2.538522958755493, 'learning_rate': 3.518518518518519e-05, 'epoch': 1.94}
{'loss': 0.4132, 'grad_norm': 8.413515090942383, 'learning_rate': 3.364197530864198e-05, 'epoch': 1.99}
  warnings.warn(                                                                                                                  
{'eval_loss': 0.3974960446357727, 'eval_accuracy': 0.8508928571428571, 'eval_micro_f1': 0.8508928571428571, 'eval_macro_f1': 0.7617381211743957, 'eval_runtime': 2.5554, 'eval_samples_per_second': 438.291, 'eval_steps_per_second': 27.393, 'epoch': 2.0}
 77%|██████████████████████████████████████████████████████████████████████▏                    | 500/648 [00:42<00:10, 13.80it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:1160: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.
{'loss': 0.3849, 'grad_norm': 14.087535858154297, 'learning_rate': 3.209876543209876e-05, 'epoch': 2.04}
{'loss': 0.3733, 'grad_norm': 13.618431091308594, 'learning_rate': 3.055555555555556e-05, 'epoch': 2.08}
{'loss': 0.3934, 'grad_norm': 3.533188581466675, 'learning_rate': 2.9012345679012347e-05, 'epoch': 2.13}
{'loss': 0.4446, 'grad_norm': 9.334521293640137, 'learning_rate': 2.7469135802469138e-05, 'epoch': 2.18}
{'loss': 0.3171, 'grad_norm': 15.534631729125977, 'learning_rate': 2.5925925925925925e-05, 'epoch': 2.22}
{'loss': 0.3839, 'grad_norm': 6.701179504394531, 'learning_rate': 2.438271604938272e-05, 'epoch': 2.27}
{'loss': 0.5177, 'grad_norm': 6.736020088195801, 'learning_rate': 2.2839506172839506e-05, 'epoch': 2.31}
  warnings.warn(
100%|██████████████████████████████████████████████████████████████████████████████████████████▊| 647/648 [00:53<00:00, 13.67it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:1160: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.
{'loss': 0.3412, 'grad_norm': 9.572068214416504, 'learning_rate': 2.1296296296296296e-05, 'epoch': 2.36}
{'loss': 0.3157, 'grad_norm': 6.260645389556885, 'learning_rate': 1.9753086419753087e-05, 'epoch': 2.41}
{'loss': 0.4751, 'grad_norm': 11.083215713500977, 'learning_rate': 1.8209876543209877e-05, 'epoch': 2.45}
{'loss': 0.3739, 'grad_norm': 7.114074230194092, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.5}
{'loss': 0.4102, 'grad_norm': 5.611170291900635, 'learning_rate': 1.5123456790123458e-05, 'epoch': 2.55}
{'loss': 0.3123, 'grad_norm': 4.529080867767334, 'learning_rate': 1.3580246913580247e-05, 'epoch': 2.59}
{'loss': 0.3821, 'grad_norm': 12.12678337097168, 'learning_rate': 1.2037037037037037e-05, 'epoch': 2.64}
{'loss': 0.4086, 'grad_norm': 4.853616714477539, 'learning_rate': 1.0493827160493827e-05, 'epoch': 2.69}
{'loss': 0.3821, 'grad_norm': 8.94863224029541, 'learning_rate': 8.950617283950618e-06, 'epoch': 2.73}
{'loss': 0.3477, 'grad_norm': 8.497509956359863, 'learning_rate': 7.4074074074074075e-06, 'epoch': 2.78}
{'loss': 0.436, 'grad_norm': 6.317110538482666, 'learning_rate': 5.864197530864198e-06, 'epoch': 2.82}
{'loss': 0.3781, 'grad_norm': 7.150190830230713, 'learning_rate': 4.3209876543209875e-06, 'epoch': 2.87}
{'loss': 0.3939, 'grad_norm': 3.441105842590332, 'learning_rate': 2.777777777777778e-06, 'epoch': 2.92}
{'loss': 0.2675, 'grad_norm': 3.2608935832977295, 'learning_rate': 1.234567901234568e-06, 'epoch': 2.96}
  warnings.warn(
100%|███████████████████████████████████████████████████████████████████████████████████████████| 648/648 [00:57<00:00, 11.30it/s]
2024-10-29 23:52:39 - INFO - __main__ - Starting evaluation...                                                                    
{'eval_loss': 0.4271165728569031, 'eval_accuracy': 0.8491071428571428, 'eval_micro_f1': 0.8491071428571428, 'eval_macro_f1': 0.7496828995946387, 'eval_runtime': 2.5741, 'eval_samples_per_second': 435.107, 'eval_steps_per_second': 27.194, 'epoch': 3.0}
{'train_runtime': 57.3429, 'train_samples_per_second': 180.598, 'train_steps_per_second': 11.3, 'train_loss': 0.5148105845775133, 'epoch': 3.0}
/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:1160: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.
  warnings.warn(
100%|█████████████████████████████████████████████████████████████████████████████████████████████| 70/70 [00:02<00:00, 27.73it/s]
2024-10-29 23:52:42 - INFO - __main__ - Evaluation results: {'eval_loss': 0.4271165728569031, 'eval_accuracy': 0.8491071428571428, 'eval_micro_f1': 0.8491071428571428, 'eval_macro_f1': 0.7496828995946387, 'eval_runtime': 2.561, 'eval_samples_per_second': 437.329, 'eval_steps_per_second': 27.333, 'epoch': 3.0}
2024-10-29 23:52:42 - INFO - __main__ - Training complete.
