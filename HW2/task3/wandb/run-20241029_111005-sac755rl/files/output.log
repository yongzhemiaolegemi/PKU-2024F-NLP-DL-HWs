2024-10-29 11:10:07 - INFO - __main__ - Initializing training script
2024-10-29 11:10:07 - INFO - __main__ - Loading dataset...
2024-10-29 11:10:07 - INFO - __main__ - Loading model and tokenizer...
Some weights of RobertaForSequenceClassificationWithAdapters were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'roberta.encoder.layer.0.output.adapter_after_ffn.down_project.bias', 'roberta.encoder.layer.0.output.adapter_after_ffn.down_project.weight', 'roberta.encoder.layer.0.output.adapter_after_ffn.up_project.bias', 'roberta.encoder.layer.0.output.adapter_after_ffn.up_project.weight', 'roberta.encoder.layer.0.output.adapter_before_ffn.down_project.bias', 'roberta.encoder.layer.0.output.adapter_before_ffn.down_project.weight', 'roberta.encoder.layer.0.output.adapter_before_ffn.up_project.bias', 'roberta.encoder.layer.0.output.adapter_before_ffn.up_project.weight', 'roberta.encoder.layer.1.output.adapter_after_ffn.down_project.bias', 'roberta.encoder.layer.1.output.adapter_after_ffn.down_project.weight', 'roberta.encoder.layer.1.output.adapter_after_ffn.up_project.bias', 'roberta.encoder.layer.1.output.adapter_after_ffn.up_project.weight', 'roberta.encoder.layer.1.output.adapter_before_ffn.down_project.bias', 'roberta.encoder.layer.1.output.adapter_before_ffn.down_project.weight', 'roberta.encoder.layer.1.output.adapter_before_ffn.up_project.bias', 'roberta.encoder.layer.1.output.adapter_before_ffn.up_project.weight', 'roberta.encoder.layer.10.output.adapter_after_ffn.down_project.bias', 'roberta.encoder.layer.10.output.adapter_after_ffn.down_project.weight', 'roberta.encoder.layer.10.output.adapter_after_ffn.up_project.bias', 'roberta.encoder.layer.10.output.adapter_after_ffn.up_project.weight', 'roberta.encoder.layer.10.output.adapter_before_ffn.down_project.bias', 'roberta.encoder.layer.10.output.adapter_before_ffn.down_project.weight', 'roberta.encoder.layer.10.output.adapter_before_ffn.up_project.bias', 'roberta.encoder.layer.10.output.adapter_before_ffn.up_project.weight', 'roberta.encoder.layer.11.output.adapter_after_ffn.down_project.bias', 'roberta.encoder.layer.11.output.adapter_after_ffn.down_project.weight', 'roberta.encoder.layer.11.output.adapter_after_ffn.up_project.bias', 'roberta.encoder.layer.11.output.adapter_after_ffn.up_project.weight', 'roberta.encoder.layer.11.output.adapter_before_ffn.down_project.bias', 'roberta.encoder.layer.11.output.adapter_before_ffn.down_project.weight', 'roberta.encoder.layer.11.output.adapter_before_ffn.up_project.bias', 'roberta.encoder.layer.11.output.adapter_before_ffn.up_project.weight', 'roberta.encoder.layer.2.output.adapter_after_ffn.down_project.bias', 'roberta.encoder.layer.2.output.adapter_after_ffn.down_project.weight', 'roberta.encoder.layer.2.output.adapter_after_ffn.up_project.bias', 'roberta.encoder.layer.2.output.adapter_after_ffn.up_project.weight', 'roberta.encoder.layer.2.output.adapter_before_ffn.down_project.bias', 'roberta.encoder.layer.2.output.adapter_before_ffn.down_project.weight', 'roberta.encoder.layer.2.output.adapter_before_ffn.up_project.bias', 'roberta.encoder.layer.2.output.adapter_before_ffn.up_project.weight', 'roberta.encoder.layer.3.output.adapter_after_ffn.down_project.bias', 'roberta.encoder.layer.3.output.adapter_after_ffn.down_project.weight', 'roberta.encoder.layer.3.output.adapter_after_ffn.up_project.bias', 'roberta.encoder.layer.3.output.adapter_after_ffn.up_project.weight', 'roberta.encoder.layer.3.output.adapter_before_ffn.down_project.bias', 'roberta.encoder.layer.3.output.adapter_before_ffn.down_project.weight', 'roberta.encoder.layer.3.output.adapter_before_ffn.up_project.bias', 'roberta.encoder.layer.3.output.adapter_before_ffn.up_project.weight', 'roberta.encoder.layer.4.output.adapter_after_ffn.down_project.bias', 'roberta.encoder.layer.4.output.adapter_after_ffn.down_project.weight', 'roberta.encoder.layer.4.output.adapter_after_ffn.up_project.bias', 'roberta.encoder.layer.4.output.adapter_after_ffn.up_project.weight', 'roberta.encoder.layer.4.output.adapter_before_ffn.down_project.bias', 'roberta.encoder.layer.4.output.adapter_before_ffn.down_project.weight', 'roberta.e
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Map: 100%|██████████████████████████████████████████████████████████████████████████| 3452/3452 [00:00<00:00, 14954.89 examples/s]
Map: 100%|██████████████████████████████████████████████████████████████████████████| 1120/1120 [00:00<00:00, 13282.87 examples/s]
/root/HW2/task3/train.py:105: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
2024-10-29 11:10:12 - INFO - __main__ - Starting training...
  0%|                                                                                                     | 0/324 [00:00<?, ?it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
 33%|██████████████████████████████▎                                                            | 108/324 [00:16<00:29,  7.34it/s]
{'loss': 1.0809, 'grad_norm': 1.201548457145691, 'learning_rate': 4.845679012345679e-05, 'epoch': 0.09}
{'loss': 1.0143, 'grad_norm': 2.0519461631774902, 'learning_rate': 4.691358024691358e-05, 'epoch': 0.19}
{'loss': 0.9527, 'grad_norm': 1.9481176137924194, 'learning_rate': 4.5370370370370374e-05, 'epoch': 0.28}
{'loss': 0.9917, 'grad_norm': 0.9936801791191101, 'learning_rate': 4.3827160493827164e-05, 'epoch': 0.37}
{'loss': 0.9309, 'grad_norm': 2.615846872329712, 'learning_rate': 4.2283950617283955e-05, 'epoch': 0.46}
{'loss': 0.9137, 'grad_norm': 1.2612181901931763, 'learning_rate': 4.074074074074074e-05, 'epoch': 0.56}
{'loss': 0.9405, 'grad_norm': 0.6425259113311768, 'learning_rate': 3.9197530864197535e-05, 'epoch': 0.65}
{'loss': 0.9653, 'grad_norm': 1.6190041303634644, 'learning_rate': 3.7654320987654326e-05, 'epoch': 0.74}
{'loss': 0.9505, 'grad_norm': 1.4060611724853516, 'learning_rate': 3.611111111111111e-05, 'epoch': 0.83}
{'loss': 0.9308, 'grad_norm': 1.0541833639144897, 'learning_rate': 3.45679012345679e-05, 'epoch': 0.93}
100%|█████████████████████████████████████████████████████████████████████████████████████████████| 35/35 [00:02<00:00, 13.07it/s]
(array([[-0.27367055,  0.7318679 , -0.57217455],
       [-0.24046762,  0.6917136 , -0.5577202 ],
       [-0.25969434,  0.70297426, -0.55047506],
       ...,
       [-0.25979012,  0.7125378 , -0.5644955 ],
       [-0.27932763,  0.7429788 , -0.5720396 ],
       [-0.27359697,  0.7165121 , -0.5643578 ]], dtype=float32), array([[[-0.08834406,  0.11082126, -0.04362461, ..., -0.08950747,
         -0.04796698, -0.07017364],
        [ 0.00333335,  0.21844609, -0.0969618 , ...,  0.12682417,
         -0.04817618,  0.08444475],
        [ 0.00955293,  0.30150947, -0.03414487, ...,  0.39122462,
         -0.1330077 ,  0.20774832],
        ...,
        [ 0.06396934,  0.12177572,  0.02085833, ...,  0.188051  ,
         -0.0246661 , -0.0483366 ],
        [ 0.06396934,  0.12177572,  0.02085833, ...,  0.188051  ,
         -0.0246661 , -0.0483366 ],
        [ 0.06396934,  0.12177572,  0.02085833, ...,  0.188051  ,
         -0.0246661 , -0.0483366 ]],

       [[-0.12776473,  0.12283513, -0.02147752, ..., -0.09260214,
         -0.05063412, -0.0516246 ],
        [-0.23874661,  0.21715176,  0.11276291, ...,  0.01485717,
          0.09160618, -0.03965158],
        [-0.04035881, -0.15433957, -0.05613256, ..., -0.08378554,
         -0.18558727, -0.10675848],
        ...,
        [-0.02547907,  0.1606876 ,  0.06513254, ...,  0.04682769,
         -0.04489827,  0.03539308],
        [-0.02547907,  0.1606876 ,  0.06513254, ...,  0.04682769,
         -0.04489827,  0.03539308],
        [-0.02547907,  0.1606876 ,  0.06513254, ...,  0.04682769,
         -0.04489827,  0.03539308]],

       [[-0.1271483 ,  0.11079026, -0.0165343 , ..., -0.09971319,
         -0.02348262, -0.06299207],
        [ 0.11815226,  0.15227571,  0.19565383, ...,  0.2552077 ,
          0.19152902,  0.22215705],
        [ 0.04521266,  0.09153979,  0.11195642, ..., -0.26557726,
         -0.01643926,  0.2057699 ],
        ...,
        [-0.03000659,  0.09862765,  0.08839838, ..., -0.01882482,
         -0.0243332 ,  0.03552616],
        [-0.03000659,  0.09862765,  0.08839838, ..., -0.01882482,
         -0.0243332 ,  0.03552616],
        [-0.03000659,  0.09862765,  0.08839838, ..., -0.01882482,
         -0.0243332 ,  0.03552616]],

       ...,

       [[-0.14409432,  0.06756784, -0.05001786, ..., -0.0638441 ,
         -0.00922175, -0.0797871 ],
        [-0.06511574,  0.03369045, -0.05609909, ...,  0.17467022,
         -0.05292174, -0.10673776],
        [-0.00919224, -0.01702031,  0.00326672, ...,  0.13456102,
         -0.0409545 , -0.39302248],
        ...,
        [-0.06502884,  0.00875389,  0.03091818, ...,  0.0472491 ,
         -0.0014594 ,  0.04508395],
        [-0.06502884,  0.00875389,  0.03091818, ...,  0.0472491 ,
         -0.0014594 ,  0.04508395],
        [-0.06502884,  0.00875389,  0.03091818, ...,  0.0472491 ,
         -0.0014594 ,  0.04508395]],

       [[-0.09552658,  0.11719444, -0.03409414, ..., -0.07977882,
         -0.01445775, -0.07269991],
        [-0.06637924,  0.11434936,  0.13152836, ..., -0.06850988,
          0.08699191,  0.06207885],
        [-0.04140113, -0.2590969 , -0.0301074 , ...,  0.08459028,
         -0.22766119, -0.1551009 ],
        ...,
        [-0.04381873,  0.12583056,  0.01393582, ...,  0.05545162,
         -0.03310782,  0.05090822],
        [-0.04381873,  0.12583056,  0.01393582, ...,  0.05545162,
         -0.03310782,  0.05090822],
        [-0.04381873,  0.12583056,  0.01393582, ...,  0.05545162,
         -0.03310782,  0.05090822]],

       [[-0.1020661 ,  0.08805439, -0.04522717, ..., -0.04939714,
         -0.02398435, -0.06559324],
        [-0.163058  ,  0.19389686,  0.04559807, ...,  0.29562277,
         -0.01971777,  0.11629584],
        [-0.12287374, -0.1450294 , -0.11211969, ...,  0.19145337,
          0.0151511 ,  0.077979  ],
        ...,
        [-0.0657635 ,  0.09759625,  0.02800001, ...,  0.03735575,
         -0.00188211,  0.00982589],
        [-0.0657635 ,  0.09759625,  0.02800001, ...,  0.03735575,
         -0.00188211,  0.00982589],
        [-0.0657635 ,  0.09759625,  0.02800001, ...,  0.03735575,
         -0.00188211,  0.00982589]]], dtype=float32))
