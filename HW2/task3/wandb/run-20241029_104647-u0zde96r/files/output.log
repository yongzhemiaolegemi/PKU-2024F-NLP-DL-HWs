2024-10-29 10:46:49 - INFO - __main__ - Initializing training script
2024-10-29 10:46:49 - INFO - __main__ - Loading dataset...
2024-10-29 10:46:57 - INFO - __main__ - Loading model and tokenizer...
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Map: 100%|█████████████████████████████████████████████████████████████████████████████| 760/760 [00:00<00:00, 5582.20 examples/s]
/root/HW2/task2/train.py:101: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
2024-10-29 10:47:02 - INFO - __main__ - Starting training...
  0%|                                                                                                     | 0/642 [00:00<?, ?it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
 26%|███████████████████████▋                                                                   | 167/642 [00:32<01:24,  5.60it/s]
{'loss': 1.2331, 'grad_norm': 5.389796257019043, 'learning_rate': 4.922118380062305e-05, 'epoch': 0.05}
{'loss': 0.7961, 'grad_norm': 3.4094932079315186, 'learning_rate': 4.844236760124611e-05, 'epoch': 0.09}
{'loss': 0.4881, 'grad_norm': 2.85577130317688, 'learning_rate': 4.766355140186916e-05, 'epoch': 0.14}
{'loss': 0.429, 'grad_norm': 10.671592712402344, 'learning_rate': 4.6884735202492216e-05, 'epoch': 0.19}
{'loss': 0.4593, 'grad_norm': 6.049612998962402, 'learning_rate': 4.6105919003115266e-05, 'epoch': 0.23}
{'loss': 0.4278, 'grad_norm': 10.518967628479004, 'learning_rate': 4.532710280373832e-05, 'epoch': 0.28}
{'loss': 0.4204, 'grad_norm': 11.992732048034668, 'learning_rate': 4.454828660436137e-05, 'epoch': 0.33}
{'loss': 0.2974, 'grad_norm': 3.117982864379883, 'learning_rate': 4.376947040498442e-05, 'epoch': 0.37}
{'loss': 0.3687, 'grad_norm': 5.252753734588623, 'learning_rate': 4.299065420560748e-05, 'epoch': 0.42}
{'loss': 0.2846, 'grad_norm': 22.739540100097656, 'learning_rate': 4.221183800623053e-05, 'epoch': 0.47}
{'loss': 0.2784, 'grad_norm': 3.208047866821289, 'learning_rate': 4.1433021806853586e-05, 'epoch': 0.51}
{'loss': 0.3402, 'grad_norm': 7.902439117431641, 'learning_rate': 4.0654205607476636e-05, 'epoch': 0.56}
{'loss': 0.327, 'grad_norm': 7.644326686859131, 'learning_rate': 3.987538940809969e-05, 'epoch': 0.61}
{'loss': 0.3028, 'grad_norm': 2.297102689743042, 'learning_rate': 3.909657320872274e-05, 'epoch': 0.65}
{'loss': 0.3025, 'grad_norm': 6.275296688079834, 'learning_rate': 3.831775700934579e-05, 'epoch': 0.7}
{'loss': 0.3204, 'grad_norm': 5.144009113311768, 'learning_rate': 3.753894080996885e-05, 'epoch': 0.75}
