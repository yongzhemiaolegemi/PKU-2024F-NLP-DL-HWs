2024-10-29 11:06:37 - INFO - __main__ - Initializing training script
2024-10-29 11:06:37 - INFO - __main__ - Loading dataset...
2024-10-29 11:06:37 - INFO - __main__ - Loading model and tokenizer...
Some weights of RobertaForSequenceClassificationWithAdapters were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'roberta.encoder.layer.0.output.adapter_after_ffn.down_project.bias', 'roberta.encoder.layer.0.output.adapter_after_ffn.down_project.weight', 'roberta.encoder.layer.0.output.adapter_after_ffn.up_project.bias', 'roberta.encoder.layer.0.output.adapter_after_ffn.up_project.weight', 'roberta.encoder.layer.0.output.adapter_before_ffn.down_project.bias', 'roberta.encoder.layer.0.output.adapter_before_ffn.down_project.weight', 'roberta.encoder.layer.0.output.adapter_before_ffn.up_project.bias', 'roberta.encoder.layer.0.output.adapter_before_ffn.up_project.weight', 'roberta.encoder.layer.1.output.adapter_after_ffn.down_project.bias', 'roberta.encoder.layer.1.output.adapter_after_ffn.down_project.weight', 'roberta.encoder.layer.1.output.adapter_after_ffn.up_project.bias', 'roberta.encoder.layer.1.output.adapter_after_ffn.up_project.weight', 'roberta.encoder.layer.1.output.adapter_before_ffn.down_project.bias', 'roberta.encoder.layer.1.output.adapter_before_ffn.down_project.weight', 'roberta.encoder.layer.1.output.adapter_before_ffn.up_project.bias', 'roberta.encoder.layer.1.output.adapter_before_ffn.up_project.weight', 'roberta.encoder.layer.10.output.adapter_after_ffn.down_project.bias', 'roberta.encoder.layer.10.output.adapter_after_ffn.down_project.weight', 'roberta.encoder.layer.10.output.adapter_after_ffn.up_project.bias', 'roberta.encoder.layer.10.output.adapter_after_ffn.up_project.weight', 'roberta.encoder.layer.10.output.adapter_before_ffn.down_project.bias', 'roberta.encoder.layer.10.output.adapter_before_ffn.down_project.weight', 'roberta.encoder.layer.10.output.adapter_before_ffn.up_project.bias', 'roberta.encoder.layer.10.output.adapter_before_ffn.up_project.weight', 'roberta.encoder.layer.11.output.adapter_after_ffn.down_project.bias', 'roberta.encoder.layer.11.output.adapter_after_ffn.down_project.weight', 'roberta.encoder.layer.11.output.adapter_after_ffn.up_project.bias', 'roberta.encoder.layer.11.output.adapter_after_ffn.up_project.weight', 'roberta.encoder.layer.11.output.adapter_before_ffn.down_project.bias', 'roberta.encoder.layer.11.output.adapter_before_ffn.down_project.weight', 'roberta.encoder.layer.11.output.adapter_before_ffn.up_project.bias', 'roberta.encoder.layer.11.output.adapter_before_ffn.up_project.weight', 'roberta.encoder.layer.2.output.adapter_after_ffn.down_project.bias', 'roberta.encoder.layer.2.output.adapter_after_ffn.down_project.weight', 'roberta.encoder.layer.2.output.adapter_after_ffn.up_project.bias', 'roberta.encoder.layer.2.output.adapter_after_ffn.up_project.weight', 'roberta.encoder.layer.2.output.adapter_before_ffn.down_project.bias', 'roberta.encoder.layer.2.output.adapter_before_ffn.down_project.weight', 'roberta.encoder.layer.2.output.adapter_before_ffn.up_project.bias', 'roberta.encoder.layer.2.output.adapter_before_ffn.up_project.weight', 'roberta.encoder.layer.3.output.adapter_after_ffn.down_project.bias', 'roberta.encoder.layer.3.output.adapter_after_ffn.down_project.weight', 'roberta.encoder.layer.3.output.adapter_after_ffn.up_project.bias', 'roberta.encoder.layer.3.output.adapter_after_ffn.up_project.weight', 'roberta.encoder.layer.3.output.adapter_before_ffn.down_project.bias', 'roberta.encoder.layer.3.output.adapter_before_ffn.down_project.weight', 'roberta.encoder.layer.3.output.adapter_before_ffn.up_project.bias', 'roberta.encoder.layer.3.output.adapter_before_ffn.up_project.weight', 'roberta.encoder.layer.4.output.adapter_after_ffn.down_project.bias', 'roberta.encoder.layer.4.output.adapter_after_ffn.down_project.weight', 'roberta.encoder.layer.4.output.adapter_after_ffn.up_project.bias', 'roberta.encoder.layer.4.output.adapter_after_ffn.up_project.weight', 'roberta.encoder.layer.4.output.adapter_before_ffn.down_project.bias', 'roberta.encoder.layer.4.output.adapter_before_ffn.down_project.weight', 'roberta.e
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Map: 100%|██████████████████████████████████████████████████████████████████████████| 3452/3452 [00:00<00:00, 18875.99 examples/s]
Map: 100%|██████████████████████████████████████████████████████████████████████████| 1120/1120 [00:00<00:00, 15785.55 examples/s]
/root/HW2/task3/train.py:103: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
2024-10-29 11:06:42 - INFO - __main__ - Starting training...
  0%|                                                                                                     | 0/324 [00:00<?, ?it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
 33%|██████████████████████████████▎                                                            | 108/324 [00:15<00:28,  7.53it/s]Traceback (most recent call last):
{'loss': 1.0553, 'grad_norm': 1.597511649131775, 'learning_rate': 4.845679012345679e-05, 'epoch': 0.09}
{'loss': 1.0006, 'grad_norm': 1.879628300666809, 'learning_rate': 4.691358024691358e-05, 'epoch': 0.19}
{'loss': 0.9665, 'grad_norm': 1.0637603998184204, 'learning_rate': 4.5370370370370374e-05, 'epoch': 0.28}
{'loss': 0.9172, 'grad_norm': 0.823241114616394, 'learning_rate': 4.3827160493827164e-05, 'epoch': 0.37}
{'loss': 0.9756, 'grad_norm': 1.3341989517211914, 'learning_rate': 4.2283950617283955e-05, 'epoch': 0.46}
{'loss': 0.9856, 'grad_norm': 1.1401077508926392, 'learning_rate': 4.074074074074074e-05, 'epoch': 0.56}
{'loss': 0.9294, 'grad_norm': 1.19591224193573, 'learning_rate': 3.9197530864197535e-05, 'epoch': 0.65}
{'loss': 0.8989, 'grad_norm': 1.2098388671875, 'learning_rate': 3.7654320987654326e-05, 'epoch': 0.74}
{'loss': 0.9447, 'grad_norm': 0.6903719305992126, 'learning_rate': 3.611111111111111e-05, 'epoch': 0.83}
{'loss': 0.9868, 'grad_norm': 0.6473474502563477, 'learning_rate': 3.45679012345679e-05, 'epoch': 0.93}
  File "/root/HW2/task3/train.py", line 117, in <module>██████████████████████████████████████████| 35/35 [00:02<00:00, 12.46it/s]
    trainer.train()
  File "/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
  File "/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/trainer.py", line 2566, in _inner_training_loop
    self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
  File "/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/trainer.py", line 2997, in _maybe_log_save_evaluate
    metrics = self._evaluate(trial, ignore_keys_for_eval)
  File "/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/trainer.py", line 2951, in _evaluate
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
  File "/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/trainer.py", line 3964, in evaluate
    output = eval_loop(
  File "/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/trainer.py", line 4253, in evaluation_loop
    metrics = self.compute_metrics(
  File "/root/HW2/task3/train.py", line 93, in compute_metrics
    preds = pred.predictions.argmax(-1)
AttributeError: 'tuple' object has no attribute 'argmax'
