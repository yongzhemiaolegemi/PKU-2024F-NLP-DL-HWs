2024-10-29 10:28:54 - INFO - __main__ - Initializing training script
2024-10-29 10:28:54 - INFO - __main__ - Loading dataset...
2024-10-29 10:29:02 - INFO - __main__ - Loading model and tokenizer...
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/root/HW2/task2/train.py:101: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
2024-10-29 10:29:08 - INFO - __main__ - Starting training...
  0%|                                                                                                     | 0/642 [00:00<?, ?it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
 33%|██████████████████████████████▎                                                            | 214/642 [00:43<01:19,  5.37it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
{'loss': 1.3118, 'grad_norm': 4.91536808013916, 'learning_rate': 4.922118380062305e-05, 'epoch': 0.05}
{'loss': 0.6627, 'grad_norm': 6.891605854034424, 'learning_rate': 4.844236760124611e-05, 'epoch': 0.09}
{'loss': 0.4906, 'grad_norm': 4.830028057098389, 'learning_rate': 4.766355140186916e-05, 'epoch': 0.14}
{'loss': 0.4274, 'grad_norm': 6.500511169433594, 'learning_rate': 4.6884735202492216e-05, 'epoch': 0.19}
{'loss': 0.4499, 'grad_norm': 4.410656929016113, 'learning_rate': 4.6105919003115266e-05, 'epoch': 0.23}
{'loss': 0.3267, 'grad_norm': 9.177535057067871, 'learning_rate': 4.532710280373832e-05, 'epoch': 0.28}
{'loss': 0.3211, 'grad_norm': 6.228567600250244, 'learning_rate': 4.454828660436137e-05, 'epoch': 0.33}
{'loss': 0.3889, 'grad_norm': 8.490275382995605, 'learning_rate': 4.376947040498442e-05, 'epoch': 0.37}
{'loss': 0.3436, 'grad_norm': 18.329795837402344, 'learning_rate': 4.299065420560748e-05, 'epoch': 0.42}
{'loss': 0.3406, 'grad_norm': 9.900455474853516, 'learning_rate': 4.221183800623053e-05, 'epoch': 0.47}
{'loss': 0.3818, 'grad_norm': 5.516042232513428, 'learning_rate': 4.1433021806853586e-05, 'epoch': 0.51}
{'loss': 0.2647, 'grad_norm': 12.589475631713867, 'learning_rate': 4.0654205607476636e-05, 'epoch': 0.56}
{'loss': 0.2826, 'grad_norm': 13.469316482543945, 'learning_rate': 3.987538940809969e-05, 'epoch': 0.61}
{'loss': 0.3689, 'grad_norm': 5.611171722412109, 'learning_rate': 3.909657320872274e-05, 'epoch': 0.65}
{'loss': 0.3727, 'grad_norm': 6.879630088806152, 'learning_rate': 3.831775700934579e-05, 'epoch': 0.7}
{'loss': 0.2582, 'grad_norm': 2.897200107574463, 'learning_rate': 3.753894080996885e-05, 'epoch': 0.75}
{'loss': 0.3764, 'grad_norm': 5.428848743438721, 'learning_rate': 3.67601246105919e-05, 'epoch': 0.79}
{'loss': 0.2377, 'grad_norm': 11.290326118469238, 'learning_rate': 3.5981308411214956e-05, 'epoch': 0.84}
{'loss': 0.3698, 'grad_norm': 7.553913116455078, 'learning_rate': 3.5202492211838006e-05, 'epoch': 0.89}
{'loss': 0.363, 'grad_norm': 4.762241840362549, 'learning_rate': 3.442367601246106e-05, 'epoch': 0.93}
{'loss': 0.3236, 'grad_norm': 4.545927047729492, 'learning_rate': 3.364485981308411e-05, 'epoch': 0.98}
  warnings.warn(                                                                                                                  
{'eval_loss': 0.2509792447090149, 'eval_accuracy': 0.9210526315789473, 'eval_micro_f1': 0.9210526315789473, 'eval_macro_f1': 0.9190028728531481, 'eval_runtime': 1.8989, 'eval_samples_per_second': 400.226, 'eval_steps_per_second': 12.639, 'epoch': 1.0}
 67%|████████████████████████████████████████████████████████████▋                              | 428/642 [01:26<00:38,  5.53it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
{'loss': 0.2267, 'grad_norm': 2.202232837677002, 'learning_rate': 3.286604361370716e-05, 'epoch': 1.03}
{'loss': 0.2364, 'grad_norm': 8.365177154541016, 'learning_rate': 3.208722741433022e-05, 'epoch': 1.07}
{'loss': 0.2293, 'grad_norm': 9.084733963012695, 'learning_rate': 3.130841121495327e-05, 'epoch': 1.12}
{'loss': 0.2825, 'grad_norm': 6.405675411224365, 'learning_rate': 3.0529595015576326e-05, 'epoch': 1.17}
{'loss': 0.1979, 'grad_norm': 5.333004474639893, 'learning_rate': 2.9750778816199376e-05, 'epoch': 1.21}
{'loss': 0.252, 'grad_norm': 4.129135608673096, 'learning_rate': 2.897196261682243e-05, 'epoch': 1.26}
{'loss': 0.237, 'grad_norm': 5.386800765991211, 'learning_rate': 2.8193146417445482e-05, 'epoch': 1.31}
{'loss': 0.1985, 'grad_norm': 5.764787197113037, 'learning_rate': 2.7414330218068536e-05, 'epoch': 1.36}
{'loss': 0.2678, 'grad_norm': 9.100522994995117, 'learning_rate': 2.663551401869159e-05, 'epoch': 1.4}
{'loss': 0.197, 'grad_norm': 4.562239170074463, 'learning_rate': 2.585669781931464e-05, 'epoch': 1.45}
{'loss': 0.1668, 'grad_norm': 9.920080184936523, 'learning_rate': 2.5077881619937692e-05, 'epoch': 1.5}
{'loss': 0.2738, 'grad_norm': 6.10648250579834, 'learning_rate': 2.429906542056075e-05, 'epoch': 1.54}
{'loss': 0.2196, 'grad_norm': 3.3407018184661865, 'learning_rate': 2.3520249221183802e-05, 'epoch': 1.59}
{'loss': 0.2017, 'grad_norm': 5.033995151519775, 'learning_rate': 2.2741433021806856e-05, 'epoch': 1.64}
{'loss': 0.3437, 'grad_norm': 4.864480495452881, 'learning_rate': 2.196261682242991e-05, 'epoch': 1.68}
{'loss': 0.2232, 'grad_norm': 4.567028999328613, 'learning_rate': 2.118380062305296e-05, 'epoch': 1.73}
{'loss': 0.1984, 'grad_norm': 5.152956962585449, 'learning_rate': 2.0404984423676012e-05, 'epoch': 1.78}
{'loss': 0.1651, 'grad_norm': 6.684028148651123, 'learning_rate': 1.9626168224299065e-05, 'epoch': 1.82}
{'loss': 0.1862, 'grad_norm': 7.409402370452881, 'learning_rate': 1.884735202492212e-05, 'epoch': 1.87}
{'loss': 0.2124, 'grad_norm': 4.5468926429748535, 'learning_rate': 1.8068535825545172e-05, 'epoch': 1.92}
{'loss': 0.1818, 'grad_norm': 2.4423344135284424, 'learning_rate': 1.7289719626168225e-05, 'epoch': 1.96}
  warnings.warn(                                                                                                                  
{'eval_loss': 0.22005276381969452, 'eval_accuracy': 0.9302631578947368, 'eval_micro_f1': 0.9302631578947368, 'eval_macro_f1': 0.9286676619080607, 'eval_runtime': 1.8086, 'eval_samples_per_second': 420.216, 'eval_steps_per_second': 13.27, 'epoch': 2.0}
 78%|██████████████████████████████████████████████████████████████████████▊                    | 500/642 [01:39<00:26,  5.31it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
{'loss': 0.2136, 'grad_norm': 7.553587436676025, 'learning_rate': 1.6510903426791275e-05, 'epoch': 2.01}
{'loss': 0.1603, 'grad_norm': 4.8043928146362305, 'learning_rate': 1.573208722741433e-05, 'epoch': 2.06}
{'loss': 0.1074, 'grad_norm': 2.805886745452881, 'learning_rate': 1.4953271028037382e-05, 'epoch': 2.1}
{'loss': 0.142, 'grad_norm': 2.9392025470733643, 'learning_rate': 1.4174454828660435e-05, 'epoch': 2.15}
{'loss': 0.1481, 'grad_norm': 3.743161916732788, 'learning_rate': 1.3395638629283489e-05, 'epoch': 2.2}
{'loss': 0.1235, 'grad_norm': 4.9161529541015625, 'learning_rate': 1.2616822429906542e-05, 'epoch': 2.24}
{'loss': 0.1356, 'grad_norm': 0.541100800037384, 'learning_rate': 1.1838006230529595e-05, 'epoch': 2.29}
{'loss': 0.1374, 'grad_norm': 7.074836730957031, 'learning_rate': 1.1059190031152649e-05, 'epoch': 2.34}
  warnings.warn(
100%|███████████████████████████████████████████████████████████████████████████████████████████| 642/642 [02:09<00:00,  5.49it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
{'loss': 0.1696, 'grad_norm': 15.20427131652832, 'learning_rate': 1.02803738317757e-05, 'epoch': 2.38}
{'loss': 0.1121, 'grad_norm': 2.394502878189087, 'learning_rate': 9.501557632398754e-06, 'epoch': 2.43}
{'loss': 0.1663, 'grad_norm': 8.026824951171875, 'learning_rate': 8.722741433021807e-06, 'epoch': 2.48}
{'loss': 0.1274, 'grad_norm': 5.886613845825195, 'learning_rate': 7.94392523364486e-06, 'epoch': 2.52}
{'loss': 0.1627, 'grad_norm': 5.781689643859863, 'learning_rate': 7.165109034267913e-06, 'epoch': 2.57}
{'loss': 0.1638, 'grad_norm': 4.385190486907959, 'learning_rate': 6.386292834890965e-06, 'epoch': 2.62}
{'loss': 0.1208, 'grad_norm': 8.725837707519531, 'learning_rate': 5.607476635514019e-06, 'epoch': 2.66}
{'loss': 0.1397, 'grad_norm': 5.740074634552002, 'learning_rate': 4.828660436137072e-06, 'epoch': 2.71}
{'loss': 0.1428, 'grad_norm': 3.0029680728912354, 'learning_rate': 4.0498442367601245e-06, 'epoch': 2.76}
{'loss': 0.0619, 'grad_norm': 1.5986803770065308, 'learning_rate': 3.2710280373831774e-06, 'epoch': 2.8}
{'loss': 0.077, 'grad_norm': 5.410191535949707, 'learning_rate': 2.4922118380062308e-06, 'epoch': 2.85}
{'loss': 0.1138, 'grad_norm': 1.548574686050415, 'learning_rate': 1.7133956386292835e-06, 'epoch': 2.9}
{'loss': 0.1327, 'grad_norm': 3.5841000080108643, 'learning_rate': 9.345794392523364e-07, 'epoch': 2.94}
{'loss': 0.0832, 'grad_norm': 7.295128345489502, 'learning_rate': 1.5576323987538942e-07, 'epoch': 2.99}
  warnings.warn(
100%|███████████████████████████████████████████████████████████████████████████████████████████| 642/642 [02:13<00:00,  4.81it/s]
2024-10-29 10:31:22 - INFO - __main__ - Starting evaluation...                                                                    
{'eval_loss': 0.2278440147638321, 'eval_accuracy': 0.9302631578947368, 'eval_micro_f1': 0.9302631578947368, 'eval_macro_f1': 0.9285422625726016, 'eval_runtime': 1.9726, 'eval_samples_per_second': 385.274, 'eval_steps_per_second': 12.167, 'epoch': 3.0}
{'train_runtime': 133.5897, 'train_samples_per_second': 153.605, 'train_steps_per_second': 4.806, 'train_loss': 0.2544972099062067, 'epoch': 3.0}
/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
100%|█████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:01<00:00, 13.88it/s]
2024-10-29 10:31:24 - INFO - __main__ - Evaluation results: {'eval_loss': 0.2278440147638321, 'eval_accuracy': 0.9302631578947368, 'eval_micro_f1': 0.9302631578947368, 'eval_macro_f1': 0.9285422625726016, 'eval_runtime': 1.8105, 'eval_samples_per_second': 419.775, 'eval_steps_per_second': 13.256, 'epoch': 3.0}
2024-10-29 10:31:24 - INFO - __main__ - Training complete.
