2024-10-29 11:10:28 - INFO - __main__ - Initializing training script
2024-10-29 11:10:28 - INFO - __main__ - Loading dataset...
2024-10-29 11:10:37 - INFO - __main__ - Loading model and tokenizer...
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Map: 100%|███████████████████████████████████████████████████████████████████████████| 6840/6840 [00:00<00:00, 7090.15 examples/s]
Map: 100%|█████████████████████████████████████████████████████████████████████████████| 760/760 [00:00<00:00, 8449.78 examples/s]
/root/HW2/task2/train.py:101: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
2024-10-29 11:10:44 - INFO - __main__ - Starting training...
  0%|                                                                                                     | 0/642 [00:00<?, ?it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
 33%|██████████████████████████████▎                                                            | 214/642 [00:43<01:17,  5.49it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
{'loss': 1.2187, 'grad_norm': 5.48919677734375, 'learning_rate': 4.922118380062305e-05, 'epoch': 0.05}
{'loss': 0.7873, 'grad_norm': 8.76064395904541, 'learning_rate': 4.844236760124611e-05, 'epoch': 0.09}
{'loss': 0.5681, 'grad_norm': 5.846749305725098, 'learning_rate': 4.766355140186916e-05, 'epoch': 0.14}
{'loss': 0.4064, 'grad_norm': 8.520052909851074, 'learning_rate': 4.6884735202492216e-05, 'epoch': 0.19}
{'loss': 0.5409, 'grad_norm': 6.644667625427246, 'learning_rate': 4.6105919003115266e-05, 'epoch': 0.23}
{'loss': 0.4388, 'grad_norm': 5.458883762359619, 'learning_rate': 4.532710280373832e-05, 'epoch': 0.28}
{'loss': 0.496, 'grad_norm': 8.155068397521973, 'learning_rate': 4.454828660436137e-05, 'epoch': 0.33}
{'loss': 0.3648, 'grad_norm': 9.842151641845703, 'learning_rate': 4.376947040498442e-05, 'epoch': 0.37}
{'loss': 0.4295, 'grad_norm': 5.678156852722168, 'learning_rate': 4.299065420560748e-05, 'epoch': 0.42}
{'loss': 0.3303, 'grad_norm': 10.652731895446777, 'learning_rate': 4.221183800623053e-05, 'epoch': 0.47}
{'loss': 0.4333, 'grad_norm': 6.365392684936523, 'learning_rate': 4.1433021806853586e-05, 'epoch': 0.51}
{'loss': 0.3603, 'grad_norm': 9.643036842346191, 'learning_rate': 4.0654205607476636e-05, 'epoch': 0.56}
{'loss': 0.4008, 'grad_norm': 8.221081733703613, 'learning_rate': 3.987538940809969e-05, 'epoch': 0.61}
{'loss': 0.3754, 'grad_norm': 12.426883697509766, 'learning_rate': 3.909657320872274e-05, 'epoch': 0.65}
{'loss': 0.352, 'grad_norm': 6.59444522857666, 'learning_rate': 3.831775700934579e-05, 'epoch': 0.7}
{'loss': 0.4315, 'grad_norm': 8.592849731445312, 'learning_rate': 3.753894080996885e-05, 'epoch': 0.75}
{'loss': 0.4393, 'grad_norm': 5.271085739135742, 'learning_rate': 3.67601246105919e-05, 'epoch': 0.79}
{'loss': 0.3658, 'grad_norm': 3.888192892074585, 'learning_rate': 3.5981308411214956e-05, 'epoch': 0.84}
{'loss': 0.3442, 'grad_norm': 6.756704807281494, 'learning_rate': 3.5202492211838006e-05, 'epoch': 0.89}
{'loss': 0.3885, 'grad_norm': 3.2883358001708984, 'learning_rate': 3.442367601246106e-05, 'epoch': 0.93}
{'loss': 0.348, 'grad_norm': 6.932294845581055, 'learning_rate': 3.364485981308411e-05, 'epoch': 0.98}
  warnings.warn(                                                                                                                  
{'eval_loss': 0.25351452827453613, 'eval_accuracy': 0.9210526315789473, 'eval_micro_f1': 0.9210526315789473, 'eval_macro_f1': 0.9196306482754789, 'eval_runtime': 1.9484, 'eval_samples_per_second': 390.058, 'eval_steps_per_second': 12.318, 'epoch': 1.0}
 67%|████████████████████████████████████████████████████████████▋                              | 428/642 [01:25<00:39,  5.47it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
{'loss': 0.3042, 'grad_norm': 4.1613006591796875, 'learning_rate': 3.286604361370716e-05, 'epoch': 1.03}
{'loss': 0.2107, 'grad_norm': 10.906052589416504, 'learning_rate': 3.208722741433022e-05, 'epoch': 1.07}
{'loss': 0.2089, 'grad_norm': 7.441583156585693, 'learning_rate': 3.130841121495327e-05, 'epoch': 1.12}
{'loss': 0.2486, 'grad_norm': 2.509690761566162, 'learning_rate': 3.0529595015576326e-05, 'epoch': 1.17}
{'loss': 0.2029, 'grad_norm': 3.332655191421509, 'learning_rate': 2.9750778816199376e-05, 'epoch': 1.21}
{'loss': 0.1667, 'grad_norm': 1.75347900390625, 'learning_rate': 2.897196261682243e-05, 'epoch': 1.26}
{'loss': 0.3011, 'grad_norm': 19.615419387817383, 'learning_rate': 2.8193146417445482e-05, 'epoch': 1.31}
{'loss': 0.1825, 'grad_norm': 5.778144836425781, 'learning_rate': 2.7414330218068536e-05, 'epoch': 1.36}
{'loss': 0.2191, 'grad_norm': 16.625, 'learning_rate': 2.663551401869159e-05, 'epoch': 1.4}
{'loss': 0.2651, 'grad_norm': 7.775880813598633, 'learning_rate': 2.585669781931464e-05, 'epoch': 1.45}
{'loss': 0.2677, 'grad_norm': 5.5964250564575195, 'learning_rate': 2.5077881619937692e-05, 'epoch': 1.5}
{'loss': 0.2048, 'grad_norm': 6.388466835021973, 'learning_rate': 2.429906542056075e-05, 'epoch': 1.54}
{'loss': 0.2273, 'grad_norm': 4.014909744262695, 'learning_rate': 2.3520249221183802e-05, 'epoch': 1.59}
{'loss': 0.2397, 'grad_norm': 3.82137393951416, 'learning_rate': 2.2741433021806856e-05, 'epoch': 1.64}
{'loss': 0.2275, 'grad_norm': 5.063460350036621, 'learning_rate': 2.196261682242991e-05, 'epoch': 1.68}
{'loss': 0.2421, 'grad_norm': 8.634867668151855, 'learning_rate': 2.118380062305296e-05, 'epoch': 1.73}
{'loss': 0.2097, 'grad_norm': 3.6279757022857666, 'learning_rate': 2.0404984423676012e-05, 'epoch': 1.78}
{'loss': 0.2296, 'grad_norm': 6.582247734069824, 'learning_rate': 1.9626168224299065e-05, 'epoch': 1.82}
{'loss': 0.2108, 'grad_norm': 9.039402961730957, 'learning_rate': 1.884735202492212e-05, 'epoch': 1.87}
{'loss': 0.182, 'grad_norm': 3.5837278366088867, 'learning_rate': 1.8068535825545172e-05, 'epoch': 1.92}
{'loss': 0.2486, 'grad_norm': 5.928284168243408, 'learning_rate': 1.7289719626168225e-05, 'epoch': 1.96}
  warnings.warn(                                                                                                                  
{'eval_loss': 0.24957163631916046, 'eval_accuracy': 0.9105263157894737, 'eval_micro_f1': 0.9105263157894737, 'eval_macro_f1': 0.9093181254864947, 'eval_runtime': 1.9454, 'eval_samples_per_second': 390.656, 'eval_steps_per_second': 12.337, 'epoch': 2.0}
 78%|██████████████████████████████████████████████████████████████████████▊                    | 500/642 [01:39<00:27,  5.22it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
{'loss': 0.1853, 'grad_norm': 3.4810590744018555, 'learning_rate': 1.6510903426791275e-05, 'epoch': 2.01}
{'loss': 0.1418, 'grad_norm': 8.398701667785645, 'learning_rate': 1.573208722741433e-05, 'epoch': 2.06}
{'loss': 0.1252, 'grad_norm': 0.7543824911117554, 'learning_rate': 1.4953271028037382e-05, 'epoch': 2.1}
{'loss': 0.0885, 'grad_norm': 4.126198768615723, 'learning_rate': 1.4174454828660435e-05, 'epoch': 2.15}
{'loss': 0.1208, 'grad_norm': 6.213123321533203, 'learning_rate': 1.3395638629283489e-05, 'epoch': 2.2}
{'loss': 0.1387, 'grad_norm': 3.3305842876434326, 'learning_rate': 1.2616822429906542e-05, 'epoch': 2.24}
{'loss': 0.0821, 'grad_norm': 1.549515724182129, 'learning_rate': 1.1838006230529595e-05, 'epoch': 2.29}
{'loss': 0.0608, 'grad_norm': 3.0588624477386475, 'learning_rate': 1.1059190031152649e-05, 'epoch': 2.34}
  warnings.warn(
100%|███████████████████████████████████████████████████████████████████████████████████████████| 642/642 [02:09<00:00,  4.94it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
{'loss': 0.1327, 'grad_norm': 0.7159779667854309, 'learning_rate': 1.02803738317757e-05, 'epoch': 2.38}
{'loss': 0.0576, 'grad_norm': 3.0438311100006104, 'learning_rate': 9.501557632398754e-06, 'epoch': 2.43}
{'loss': 0.0975, 'grad_norm': 0.943157434463501, 'learning_rate': 8.722741433021807e-06, 'epoch': 2.48}
{'loss': 0.1102, 'grad_norm': 9.004060745239258, 'learning_rate': 7.94392523364486e-06, 'epoch': 2.52}
{'loss': 0.1069, 'grad_norm': 0.613002598285675, 'learning_rate': 7.165109034267913e-06, 'epoch': 2.57}
{'loss': 0.1365, 'grad_norm': 2.8827099800109863, 'learning_rate': 6.386292834890965e-06, 'epoch': 2.62}
{'loss': 0.093, 'grad_norm': 8.463772773742676, 'learning_rate': 5.607476635514019e-06, 'epoch': 2.66}
{'loss': 0.0561, 'grad_norm': 0.19968752562999725, 'learning_rate': 4.828660436137072e-06, 'epoch': 2.71}
{'loss': 0.1375, 'grad_norm': 8.349081993103027, 'learning_rate': 4.0498442367601245e-06, 'epoch': 2.76}
{'loss': 0.1225, 'grad_norm': 5.183470249176025, 'learning_rate': 3.2710280373831774e-06, 'epoch': 2.8}
{'loss': 0.0916, 'grad_norm': 0.2732080817222595, 'learning_rate': 2.4922118380062308e-06, 'epoch': 2.85}
{'loss': 0.1498, 'grad_norm': 14.887877464294434, 'learning_rate': 1.7133956386292835e-06, 'epoch': 2.9}
{'loss': 0.0827, 'grad_norm': 1.7499144077301025, 'learning_rate': 9.345794392523364e-07, 'epoch': 2.94}
{'loss': 0.0926, 'grad_norm': 0.8882043361663818, 'learning_rate': 1.5576323987538942e-07, 'epoch': 2.99}
  warnings.warn(
100%|███████████████████████████████████████████████████████████████████████████████████████████| 642/642 [02:13<00:00,  4.82it/s]
2024-10-29 11:12:58 - INFO - __main__ - Starting evaluation...                                                                    
{'eval_loss': 0.28849586844444275, 'eval_accuracy': 0.9105263157894737, 'eval_micro_f1': 0.9105263157894737, 'eval_macro_f1': 0.9092428548358638, 'eval_runtime': 1.8458, 'eval_samples_per_second': 411.74, 'eval_steps_per_second': 13.002, 'epoch': 3.0}
{'train_runtime': 133.1267, 'train_samples_per_second': 154.139, 'train_steps_per_second': 4.822, 'train_loss': 0.2655628261814979, 'epoch': 3.0}
/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
100%|█████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:01<00:00, 13.07it/s]
2024-10-29 11:13:00 - INFO - __main__ - Evaluation results: {'eval_loss': 0.28849586844444275, 'eval_accuracy': 0.9105263157894737, 'eval_micro_f1': 0.9105263157894737, 'eval_macro_f1': 0.9092428548358638, 'eval_runtime': 1.9297, 'eval_samples_per_second': 393.84, 'eval_steps_per_second': 12.437, 'epoch': 3.0}
2024-10-29 11:13:00 - INFO - __main__ - Training complete.
