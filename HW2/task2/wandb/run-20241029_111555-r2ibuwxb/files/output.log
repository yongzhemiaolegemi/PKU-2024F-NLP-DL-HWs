2024-10-29 11:15:56 - INFO - __main__ - Initializing training script
2024-10-29 11:15:56 - INFO - __main__ - Loading dataset...
2024-10-29 11:16:05 - INFO - __main__ - Loading model and tokenizer...
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/root/HW2/task2/train.py:101: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
2024-10-29 11:16:12 - INFO - __main__ - Starting training...
  0%|                                                                                                     | 0/642 [00:00<?, ?it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
 33%|██████████████████████████████▎                                                            | 214/642 [01:07<02:39,  2.68it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
{'loss': 1.2339, 'grad_norm': 8.201592445373535, 'learning_rate': 4.922118380062305e-05, 'epoch': 0.05}
{'loss': 0.785, 'grad_norm': 9.35075569152832, 'learning_rate': 4.844236760124611e-05, 'epoch': 0.09}
{'loss': 0.6388, 'grad_norm': 6.627177715301514, 'learning_rate': 4.766355140186916e-05, 'epoch': 0.14}
{'loss': 0.4574, 'grad_norm': 7.815655708312988, 'learning_rate': 4.6884735202492216e-05, 'epoch': 0.19}
{'loss': 0.4319, 'grad_norm': 6.643640518188477, 'learning_rate': 4.6105919003115266e-05, 'epoch': 0.23}
{'loss': 0.3698, 'grad_norm': 9.619743347167969, 'learning_rate': 4.532710280373832e-05, 'epoch': 0.28}
{'loss': 0.453, 'grad_norm': 6.88667631149292, 'learning_rate': 4.454828660436137e-05, 'epoch': 0.33}
{'loss': 0.4104, 'grad_norm': 12.982878684997559, 'learning_rate': 4.376947040498442e-05, 'epoch': 0.37}
{'loss': 0.4209, 'grad_norm': 6.81357479095459, 'learning_rate': 4.299065420560748e-05, 'epoch': 0.42}
{'loss': 0.3856, 'grad_norm': 5.247631072998047, 'learning_rate': 4.221183800623053e-05, 'epoch': 0.47}
{'loss': 0.4338, 'grad_norm': 13.522616386413574, 'learning_rate': 4.1433021806853586e-05, 'epoch': 0.51}
{'loss': 0.2544, 'grad_norm': 4.3060150146484375, 'learning_rate': 4.0654205607476636e-05, 'epoch': 0.56}
{'loss': 0.4063, 'grad_norm': 26.301084518432617, 'learning_rate': 3.987538940809969e-05, 'epoch': 0.61}
{'loss': 0.3755, 'grad_norm': 5.445137977600098, 'learning_rate': 3.909657320872274e-05, 'epoch': 0.65}
{'loss': 0.4215, 'grad_norm': 5.411105155944824, 'learning_rate': 3.831775700934579e-05, 'epoch': 0.7}
{'loss': 0.312, 'grad_norm': 5.391991138458252, 'learning_rate': 3.753894080996885e-05, 'epoch': 0.75}
{'loss': 0.4416, 'grad_norm': 5.164737701416016, 'learning_rate': 3.67601246105919e-05, 'epoch': 0.79}
{'loss': 0.3279, 'grad_norm': 9.89871883392334, 'learning_rate': 3.5981308411214956e-05, 'epoch': 0.84}
{'loss': 0.3457, 'grad_norm': 4.0268049240112305, 'learning_rate': 3.5202492211838006e-05, 'epoch': 0.89}
{'loss': 0.3959, 'grad_norm': 8.273924827575684, 'learning_rate': 3.442367601246106e-05, 'epoch': 0.93}
{'loss': 0.4377, 'grad_norm': 10.117165565490723, 'learning_rate': 3.364485981308411e-05, 'epoch': 0.98}
  warnings.warn(                                                                                                                  
{'eval_loss': 0.2608867287635803, 'eval_accuracy': 0.906578947368421, 'eval_micro_f1': 0.906578947368421, 'eval_macro_f1': 0.904951279655379, 'eval_runtime': 3.5165, 'eval_samples_per_second': 216.127, 'eval_steps_per_second': 6.825, 'epoch': 1.0}
 67%|████████████████████████████████████████████████████████████▋                              | 428/642 [02:21<00:38,  5.49it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
{'loss': 0.272, 'grad_norm': 4.8586835861206055, 'learning_rate': 3.286604361370716e-05, 'epoch': 1.03}
{'loss': 0.1597, 'grad_norm': 3.2548716068267822, 'learning_rate': 3.208722741433022e-05, 'epoch': 1.07}
{'loss': 0.2063, 'grad_norm': 6.340732574462891, 'learning_rate': 3.130841121495327e-05, 'epoch': 1.12}
{'loss': 0.1849, 'grad_norm': 7.252920150756836, 'learning_rate': 3.0529595015576326e-05, 'epoch': 1.17}
{'loss': 0.142, 'grad_norm': 3.9947147369384766, 'learning_rate': 2.9750778816199376e-05, 'epoch': 1.21}
{'loss': 0.1362, 'grad_norm': 3.430072069168091, 'learning_rate': 2.897196261682243e-05, 'epoch': 1.26}
{'loss': 0.2845, 'grad_norm': 11.816227912902832, 'learning_rate': 2.8193146417445482e-05, 'epoch': 1.31}
{'loss': 0.2176, 'grad_norm': 1.4184530973434448, 'learning_rate': 2.7414330218068536e-05, 'epoch': 1.36}
{'loss': 0.2828, 'grad_norm': 5.150411605834961, 'learning_rate': 2.663551401869159e-05, 'epoch': 1.4}
{'loss': 0.2682, 'grad_norm': 6.028799057006836, 'learning_rate': 2.585669781931464e-05, 'epoch': 1.45}
{'loss': 0.3102, 'grad_norm': 4.156535625457764, 'learning_rate': 2.5077881619937692e-05, 'epoch': 1.5}
{'loss': 0.2151, 'grad_norm': 5.782137870788574, 'learning_rate': 2.429906542056075e-05, 'epoch': 1.54}
{'loss': 0.277, 'grad_norm': 14.636391639709473, 'learning_rate': 2.3520249221183802e-05, 'epoch': 1.59}
{'loss': 0.1458, 'grad_norm': 6.2948503494262695, 'learning_rate': 2.2741433021806856e-05, 'epoch': 1.64}
{'loss': 0.1692, 'grad_norm': 4.282162666320801, 'learning_rate': 2.196261682242991e-05, 'epoch': 1.68}
{'loss': 0.1611, 'grad_norm': 4.02173376083374, 'learning_rate': 2.118380062305296e-05, 'epoch': 1.73}
{'loss': 0.1844, 'grad_norm': 4.842597484588623, 'learning_rate': 2.0404984423676012e-05, 'epoch': 1.78}
{'loss': 0.2876, 'grad_norm': 9.33286190032959, 'learning_rate': 1.9626168224299065e-05, 'epoch': 1.82}
{'loss': 0.2582, 'grad_norm': 7.5950469970703125, 'learning_rate': 1.884735202492212e-05, 'epoch': 1.87}
{'loss': 0.2316, 'grad_norm': 4.017051696777344, 'learning_rate': 1.8068535825545172e-05, 'epoch': 1.92}
{'loss': 0.2494, 'grad_norm': 9.476125717163086, 'learning_rate': 1.7289719626168225e-05, 'epoch': 1.96}
  warnings.warn(                                                                                                                  
{'eval_loss': 0.22599948942661285, 'eval_accuracy': 0.9197368421052632, 'eval_micro_f1': 0.9197368421052632, 'eval_macro_f1': 0.9182578570015427, 'eval_runtime': 1.9409, 'eval_samples_per_second': 391.566, 'eval_steps_per_second': 12.365, 'epoch': 2.0}
 78%|██████████████████████████████████████████████████████████████████████▊                    | 500/642 [02:35<00:25,  5.47it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
{'loss': 0.2015, 'grad_norm': 2.310675621032715, 'learning_rate': 1.6510903426791275e-05, 'epoch': 2.01}
{'loss': 0.1191, 'grad_norm': 4.437050819396973, 'learning_rate': 1.573208722741433e-05, 'epoch': 2.06}
{'loss': 0.1016, 'grad_norm': 1.416053056716919, 'learning_rate': 1.4953271028037382e-05, 'epoch': 2.1}
{'loss': 0.0943, 'grad_norm': 2.510892868041992, 'learning_rate': 1.4174454828660435e-05, 'epoch': 2.15}
{'loss': 0.093, 'grad_norm': 6.154719829559326, 'learning_rate': 1.3395638629283489e-05, 'epoch': 2.2}
{'loss': 0.1169, 'grad_norm': 9.964279174804688, 'learning_rate': 1.2616822429906542e-05, 'epoch': 2.24}
{'loss': 0.1416, 'grad_norm': 0.8747851252555847, 'learning_rate': 1.1838006230529595e-05, 'epoch': 2.29}
{'loss': 0.1309, 'grad_norm': 3.3516061305999756, 'learning_rate': 1.1059190031152649e-05, 'epoch': 2.34}
  warnings.warn(
100%|███████████████████████████████████████████████████████████████████████████████████████████| 642/642 [03:03<00:00,  5.47it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
{'loss': 0.0933, 'grad_norm': 4.173328399658203, 'learning_rate': 1.02803738317757e-05, 'epoch': 2.38}
{'loss': 0.0739, 'grad_norm': 2.9511656761169434, 'learning_rate': 9.501557632398754e-06, 'epoch': 2.43}
{'loss': 0.0883, 'grad_norm': 3.6343886852264404, 'learning_rate': 8.722741433021807e-06, 'epoch': 2.48}
{'loss': 0.1061, 'grad_norm': 2.496943712234497, 'learning_rate': 7.94392523364486e-06, 'epoch': 2.52}
{'loss': 0.0931, 'grad_norm': 4.869121074676514, 'learning_rate': 7.165109034267913e-06, 'epoch': 2.57}
{'loss': 0.0681, 'grad_norm': 4.281382083892822, 'learning_rate': 6.386292834890965e-06, 'epoch': 2.62}
{'loss': 0.085, 'grad_norm': 4.51767635345459, 'learning_rate': 5.607476635514019e-06, 'epoch': 2.66}
{'loss': 0.0608, 'grad_norm': 3.062274217605591, 'learning_rate': 4.828660436137072e-06, 'epoch': 2.71}
{'loss': 0.0723, 'grad_norm': 1.7672919034957886, 'learning_rate': 4.0498442367601245e-06, 'epoch': 2.76}
{'loss': 0.0994, 'grad_norm': 4.60178279876709, 'learning_rate': 3.2710280373831774e-06, 'epoch': 2.8}
{'loss': 0.1693, 'grad_norm': 8.423239707946777, 'learning_rate': 2.4922118380062308e-06, 'epoch': 2.85}
{'loss': 0.1306, 'grad_norm': 7.246847629547119, 'learning_rate': 1.7133956386292835e-06, 'epoch': 2.9}
{'loss': 0.067, 'grad_norm': 0.9485665559768677, 'learning_rate': 9.345794392523364e-07, 'epoch': 2.94}
{'loss': 0.0972, 'grad_norm': 7.907832622528076, 'learning_rate': 1.5576323987538942e-07, 'epoch': 2.99}
  warnings.warn(
100%|███████████████████████████████████████████████████████████████████████████████████████████| 642/642 [03:07<00:00,  3.42it/s]
2024-10-29 11:19:20 - INFO - __main__ - Starting evaluation...                                                                    
{'eval_loss': 0.27603405714035034, 'eval_accuracy': 0.9197368421052632, 'eval_micro_f1': 0.9197368421052632, 'eval_macro_f1': 0.9186293817267915, 'eval_runtime': 1.9435, 'eval_samples_per_second': 391.046, 'eval_steps_per_second': 12.349, 'epoch': 3.0}
{'train_runtime': 187.743, 'train_samples_per_second': 109.298, 'train_steps_per_second': 3.42, 'train_loss': 0.26012449695313833, 'epoch': 3.0}
/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
100%|█████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:01<00:00, 14.15it/s]
2024-10-29 11:19:21 - INFO - __main__ - Evaluation results: {'eval_loss': 0.27603405714035034, 'eval_accuracy': 0.9197368421052632, 'eval_micro_f1': 0.9197368421052632, 'eval_macro_f1': 0.9186293817267915, 'eval_runtime': 1.7796, 'eval_samples_per_second': 427.063, 'eval_steps_per_second': 13.486, 'epoch': 3.0}
2024-10-29 11:19:21 - INFO - __main__ - Training complete.
