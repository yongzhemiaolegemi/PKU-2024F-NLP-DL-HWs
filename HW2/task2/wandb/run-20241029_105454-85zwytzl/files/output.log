2024-10-29 10:54:55 - INFO - __main__ - Initializing training script
2024-10-29 10:54:55 - INFO - __main__ - Loading dataset...
2024-10-29 10:55:04 - INFO - __main__ - Loading model and tokenizer...
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/root/HW2/task2/train.py:101: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
2024-10-29 10:55:09 - INFO - __main__ - Starting training...
  0%|                                                                                                     | 0/642 [00:00<?, ?it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
 33%|██████████████████████████████▎                                                            | 214/642 [00:41<01:12,  5.89it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
{'loss': 1.2043, 'grad_norm': 4.895947456359863, 'learning_rate': 4.922118380062305e-05, 'epoch': 0.05}
{'loss': 0.762, 'grad_norm': 3.8199825286865234, 'learning_rate': 4.844236760124611e-05, 'epoch': 0.09}
{'loss': 0.4682, 'grad_norm': 2.176567554473877, 'learning_rate': 4.766355140186916e-05, 'epoch': 0.14}
{'loss': 0.4958, 'grad_norm': 6.013077259063721, 'learning_rate': 4.6884735202492216e-05, 'epoch': 0.19}
{'loss': 0.3395, 'grad_norm': 2.4611012935638428, 'learning_rate': 4.6105919003115266e-05, 'epoch': 0.23}
{'loss': 0.3463, 'grad_norm': 4.187709331512451, 'learning_rate': 4.532710280373832e-05, 'epoch': 0.28}
{'loss': 0.3907, 'grad_norm': 4.734220504760742, 'learning_rate': 4.454828660436137e-05, 'epoch': 0.33}
{'loss': 0.3972, 'grad_norm': 7.486139297485352, 'learning_rate': 4.376947040498442e-05, 'epoch': 0.37}
{'loss': 0.3765, 'grad_norm': 3.8095290660858154, 'learning_rate': 4.299065420560748e-05, 'epoch': 0.42}
{'loss': 0.3234, 'grad_norm': 4.8362860679626465, 'learning_rate': 4.221183800623053e-05, 'epoch': 0.47}
{'loss': 0.4341, 'grad_norm': 3.279297351837158, 'learning_rate': 4.1433021806853586e-05, 'epoch': 0.51}
{'loss': 0.4274, 'grad_norm': 5.602835178375244, 'learning_rate': 4.0654205607476636e-05, 'epoch': 0.56}
{'loss': 0.3489, 'grad_norm': 4.36738395690918, 'learning_rate': 3.987538940809969e-05, 'epoch': 0.61}
{'loss': 0.2699, 'grad_norm': 2.4643702507019043, 'learning_rate': 3.909657320872274e-05, 'epoch': 0.65}
{'loss': 0.2099, 'grad_norm': 2.1219096183776855, 'learning_rate': 3.831775700934579e-05, 'epoch': 0.7}
{'loss': 0.3212, 'grad_norm': 7.731668472290039, 'learning_rate': 3.753894080996885e-05, 'epoch': 0.75}
{'loss': 0.3003, 'grad_norm': 6.410745143890381, 'learning_rate': 3.67601246105919e-05, 'epoch': 0.79}
{'loss': 0.2638, 'grad_norm': 8.853265762329102, 'learning_rate': 3.5981308411214956e-05, 'epoch': 0.84}
{'loss': 0.3422, 'grad_norm': 5.888042449951172, 'learning_rate': 3.5202492211838006e-05, 'epoch': 0.89}
{'loss': 0.2749, 'grad_norm': 1.7752532958984375, 'learning_rate': 3.442367601246106e-05, 'epoch': 0.93}
{'loss': 0.2467, 'grad_norm': 2.649986505508423, 'learning_rate': 3.364485981308411e-05, 'epoch': 0.98}
  warnings.warn(                                                                                                                  
{'eval_loss': 0.22679118812084198, 'eval_accuracy': 0.925, 'eval_micro_f1': 0.925, 'eval_macro_f1': 0.9234781862192728, 'eval_runtime': 1.7655, 'eval_samples_per_second': 430.476, 'eval_steps_per_second': 13.594, 'epoch': 1.0}
 67%|████████████████████████████████████████████████████████████▋                              | 428/642 [01:23<00:37,  5.67it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
{'loss': 0.1938, 'grad_norm': 1.6083276271820068, 'learning_rate': 3.286604361370716e-05, 'epoch': 1.03}
{'loss': 0.2294, 'grad_norm': 6.263880252838135, 'learning_rate': 3.208722741433022e-05, 'epoch': 1.07}
{'loss': 0.2435, 'grad_norm': 1.446237325668335, 'learning_rate': 3.130841121495327e-05, 'epoch': 1.12}
{'loss': 0.1814, 'grad_norm': 1.678615689277649, 'learning_rate': 3.0529595015576326e-05, 'epoch': 1.17}
{'loss': 0.2141, 'grad_norm': 3.31028151512146, 'learning_rate': 2.9750778816199376e-05, 'epoch': 1.21}
{'loss': 0.2394, 'grad_norm': 4.261094093322754, 'learning_rate': 2.897196261682243e-05, 'epoch': 1.26}
{'loss': 0.1894, 'grad_norm': 4.83815336227417, 'learning_rate': 2.8193146417445482e-05, 'epoch': 1.31}
{'loss': 0.2218, 'grad_norm': 2.141552686691284, 'learning_rate': 2.7414330218068536e-05, 'epoch': 1.36}
{'loss': 0.1688, 'grad_norm': 3.9162166118621826, 'learning_rate': 2.663551401869159e-05, 'epoch': 1.4}
{'loss': 0.1384, 'grad_norm': 2.4731454849243164, 'learning_rate': 2.585669781931464e-05, 'epoch': 1.45}
{'loss': 0.235, 'grad_norm': 7.731235980987549, 'learning_rate': 2.5077881619937692e-05, 'epoch': 1.5}
{'loss': 0.1827, 'grad_norm': 3.774116039276123, 'learning_rate': 2.429906542056075e-05, 'epoch': 1.54}
{'loss': 0.1443, 'grad_norm': 7.7284016609191895, 'learning_rate': 2.3520249221183802e-05, 'epoch': 1.59}
{'loss': 0.1498, 'grad_norm': 3.483647108078003, 'learning_rate': 2.2741433021806856e-05, 'epoch': 1.64}
{'loss': 0.2374, 'grad_norm': 6.999788284301758, 'learning_rate': 2.196261682242991e-05, 'epoch': 1.68}
{'loss': 0.1664, 'grad_norm': 9.047867774963379, 'learning_rate': 2.118380062305296e-05, 'epoch': 1.73}
{'loss': 0.1208, 'grad_norm': 5.836599826812744, 'learning_rate': 2.0404984423676012e-05, 'epoch': 1.78}
{'loss': 0.2327, 'grad_norm': 4.597264289855957, 'learning_rate': 1.9626168224299065e-05, 'epoch': 1.82}
{'loss': 0.1755, 'grad_norm': 2.5510456562042236, 'learning_rate': 1.884735202492212e-05, 'epoch': 1.87}
{'loss': 0.1083, 'grad_norm': 4.369319438934326, 'learning_rate': 1.8068535825545172e-05, 'epoch': 1.92}
{'loss': 0.1705, 'grad_norm': 8.290023803710938, 'learning_rate': 1.7289719626168225e-05, 'epoch': 1.96}
  warnings.warn(                                                                                                                  
{'eval_loss': 0.20886145532131195, 'eval_accuracy': 0.9328947368421052, 'eval_micro_f1': 0.9328947368421052, 'eval_macro_f1': 0.9315357094710113, 'eval_runtime': 1.8832, 'eval_samples_per_second': 403.573, 'eval_steps_per_second': 12.744, 'epoch': 2.0}
 78%|██████████████████████████████████████████████████████████████████████▊                    | 500/642 [01:36<00:26,  5.37it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
{'loss': 0.2285, 'grad_norm': 2.1562929153442383, 'learning_rate': 1.6510903426791275e-05, 'epoch': 2.01}
{'loss': 0.0648, 'grad_norm': 2.0088117122650146, 'learning_rate': 1.573208722741433e-05, 'epoch': 2.06}
{'loss': 0.1088, 'grad_norm': 4.9485578536987305, 'learning_rate': 1.4953271028037382e-05, 'epoch': 2.1}
{'loss': 0.0702, 'grad_norm': 0.8034569621086121, 'learning_rate': 1.4174454828660435e-05, 'epoch': 2.15}
{'loss': 0.0813, 'grad_norm': 2.1247546672821045, 'learning_rate': 1.3395638629283489e-05, 'epoch': 2.2}
{'loss': 0.0606, 'grad_norm': 1.1493045091629028, 'learning_rate': 1.2616822429906542e-05, 'epoch': 2.24}
{'loss': 0.1, 'grad_norm': 1.7994797229766846, 'learning_rate': 1.1838006230529595e-05, 'epoch': 2.29}
{'loss': 0.0867, 'grad_norm': 0.264508455991745, 'learning_rate': 1.1059190031152649e-05, 'epoch': 2.34}
  warnings.warn(
100%|███████████████████████████████████████████████████████████████████████████████████████████| 642/642 [02:05<00:00,  5.65it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
{'loss': 0.0611, 'grad_norm': 3.414306402206421, 'learning_rate': 1.02803738317757e-05, 'epoch': 2.38}
{'loss': 0.0939, 'grad_norm': 0.17143315076828003, 'learning_rate': 9.501557632398754e-06, 'epoch': 2.43}
{'loss': 0.0886, 'grad_norm': 3.3633248805999756, 'learning_rate': 8.722741433021807e-06, 'epoch': 2.48}
{'loss': 0.0869, 'grad_norm': 1.0920641422271729, 'learning_rate': 7.94392523364486e-06, 'epoch': 2.52}
{'loss': 0.1282, 'grad_norm': 2.298428535461426, 'learning_rate': 7.165109034267913e-06, 'epoch': 2.57}
{'loss': 0.1195, 'grad_norm': 1.1665738821029663, 'learning_rate': 6.386292834890965e-06, 'epoch': 2.62}
{'loss': 0.092, 'grad_norm': 14.119370460510254, 'learning_rate': 5.607476635514019e-06, 'epoch': 2.66}
{'loss': 0.0719, 'grad_norm': 3.856947183609009, 'learning_rate': 4.828660436137072e-06, 'epoch': 2.71}
{'loss': 0.0997, 'grad_norm': 4.22804594039917, 'learning_rate': 4.0498442367601245e-06, 'epoch': 2.76}
{'loss': 0.1818, 'grad_norm': 5.335714340209961, 'learning_rate': 3.2710280373831774e-06, 'epoch': 2.8}
{'loss': 0.0683, 'grad_norm': 1.9021267890930176, 'learning_rate': 2.4922118380062308e-06, 'epoch': 2.85}
{'loss': 0.0869, 'grad_norm': 10.339666366577148, 'learning_rate': 1.7133956386292835e-06, 'epoch': 2.9}
{'loss': 0.0878, 'grad_norm': 4.8658294677734375, 'learning_rate': 9.345794392523364e-07, 'epoch': 2.94}
{'loss': 0.0589, 'grad_norm': 1.622358798980713, 'learning_rate': 1.5576323987538942e-07, 'epoch': 2.99}
  warnings.warn(
100%|███████████████████████████████████████████████████████████████████████████████████████████| 642/642 [02:08<00:00,  4.98it/s]
2024-10-29 10:57:18 - INFO - __main__ - Starting evaluation...                                                                    
{'eval_loss': 0.2261222004890442, 'eval_accuracy': 0.9342105263157895, 'eval_micro_f1': 0.9342105263157895, 'eval_macro_f1': 0.9324764464624392, 'eval_runtime': 1.8819, 'eval_samples_per_second': 403.855, 'eval_steps_per_second': 12.753, 'epoch': 3.0}
{'train_runtime': 128.9229, 'train_samples_per_second': 159.165, 'train_steps_per_second': 4.98, 'train_loss': 0.227645236563599, 'epoch': 3.0}
/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
100%|█████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:01<00:00, 13.67it/s]
2024-10-29 10:57:20 - INFO - __main__ - Evaluation results: {'eval_loss': 0.2261222004890442, 'eval_accuracy': 0.9342105263157895, 'eval_micro_f1': 0.9342105263157895, 'eval_macro_f1': 0.9324764464624392, 'eval_runtime': 1.8354, 'eval_samples_per_second': 414.071, 'eval_steps_per_second': 13.076, 'epoch': 3.0}
2024-10-29 10:57:20 - INFO - __main__ - Training complete.
