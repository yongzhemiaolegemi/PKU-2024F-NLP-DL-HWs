2024-10-29 11:13:11 - INFO - __main__ - Initializing training script
2024-10-29 11:13:11 - INFO - __main__ - Loading dataset...
2024-10-29 11:13:20 - INFO - __main__ - Loading model and tokenizer...
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Map: 100%|█████████████████████████████████████████████████████████████████████████████| 760/760 [00:00<00:00, 5726.51 examples/s]
/root/HW2/task2/train.py:101: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
2024-10-29 11:13:26 - INFO - __main__ - Starting training...
  0%|                                                                                                     | 0/642 [00:00<?, ?it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
 33%|██████████████████████████████▎                                                            | 214/642 [00:42<01:14,  5.71it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
{'loss': 1.3016, 'grad_norm': 4.248119831085205, 'learning_rate': 4.922118380062305e-05, 'epoch': 0.05}
{'loss': 0.7718, 'grad_norm': 5.324024677276611, 'learning_rate': 4.844236760124611e-05, 'epoch': 0.09}
{'loss': 0.5588, 'grad_norm': 15.40096378326416, 'learning_rate': 4.766355140186916e-05, 'epoch': 0.14}
{'loss': 0.5028, 'grad_norm': 5.3802361488342285, 'learning_rate': 4.6884735202492216e-05, 'epoch': 0.19}
{'loss': 0.4881, 'grad_norm': 14.379920959472656, 'learning_rate': 4.6105919003115266e-05, 'epoch': 0.23}
{'loss': 0.4624, 'grad_norm': 4.452041149139404, 'learning_rate': 4.532710280373832e-05, 'epoch': 0.28}
{'loss': 0.4661, 'grad_norm': 4.601722717285156, 'learning_rate': 4.454828660436137e-05, 'epoch': 0.33}
{'loss': 0.3977, 'grad_norm': 8.196064949035645, 'learning_rate': 4.376947040498442e-05, 'epoch': 0.37}
{'loss': 0.4611, 'grad_norm': 6.836599826812744, 'learning_rate': 4.299065420560748e-05, 'epoch': 0.42}
{'loss': 0.4097, 'grad_norm': 4.303369998931885, 'learning_rate': 4.221183800623053e-05, 'epoch': 0.47}
{'loss': 0.3265, 'grad_norm': 5.080300331115723, 'learning_rate': 4.1433021806853586e-05, 'epoch': 0.51}
{'loss': 0.3606, 'grad_norm': 4.797750473022461, 'learning_rate': 4.0654205607476636e-05, 'epoch': 0.56}
{'loss': 0.3457, 'grad_norm': 11.1874361038208, 'learning_rate': 3.987538940809969e-05, 'epoch': 0.61}
{'loss': 0.3517, 'grad_norm': 5.030703544616699, 'learning_rate': 3.909657320872274e-05, 'epoch': 0.65}
{'loss': 0.4485, 'grad_norm': 4.123794078826904, 'learning_rate': 3.831775700934579e-05, 'epoch': 0.7}
{'loss': 0.413, 'grad_norm': 2.316699504852295, 'learning_rate': 3.753894080996885e-05, 'epoch': 0.75}
{'loss': 0.3513, 'grad_norm': 5.956240177154541, 'learning_rate': 3.67601246105919e-05, 'epoch': 0.79}
{'loss': 0.349, 'grad_norm': 3.0411221981048584, 'learning_rate': 3.5981308411214956e-05, 'epoch': 0.84}
{'loss': 0.3329, 'grad_norm': 6.721468448638916, 'learning_rate': 3.5202492211838006e-05, 'epoch': 0.89}
{'loss': 0.3228, 'grad_norm': 3.5454261302948, 'learning_rate': 3.442367601246106e-05, 'epoch': 0.93}
{'loss': 0.3374, 'grad_norm': 6.533557415008545, 'learning_rate': 3.364485981308411e-05, 'epoch': 0.98}
  warnings.warn(                                                                                                                  
{'eval_loss': 0.258566290140152, 'eval_accuracy': 0.9184210526315789, 'eval_micro_f1': 0.9184210526315789, 'eval_macro_f1': 0.9173555481907205, 'eval_runtime': 1.8132, 'eval_samples_per_second': 419.138, 'eval_steps_per_second': 13.236, 'epoch': 1.0}
 67%|████████████████████████████████████████████████████████████▋                              | 428/642 [01:24<00:37,  5.71it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
{'loss': 0.2736, 'grad_norm': 3.992990016937256, 'learning_rate': 3.286604361370716e-05, 'epoch': 1.03}
{'loss': 0.1927, 'grad_norm': 3.2267749309539795, 'learning_rate': 3.208722741433022e-05, 'epoch': 1.07}
{'loss': 0.2867, 'grad_norm': 5.379187107086182, 'learning_rate': 3.130841121495327e-05, 'epoch': 1.12}
{'loss': 0.1941, 'grad_norm': 4.826450347900391, 'learning_rate': 3.0529595015576326e-05, 'epoch': 1.17}
{'loss': 0.1983, 'grad_norm': 5.292842388153076, 'learning_rate': 2.9750778816199376e-05, 'epoch': 1.21}
{'loss': 0.2681, 'grad_norm': 4.631949424743652, 'learning_rate': 2.897196261682243e-05, 'epoch': 1.26}
{'loss': 0.227, 'grad_norm': 9.399531364440918, 'learning_rate': 2.8193146417445482e-05, 'epoch': 1.31}
{'loss': 0.2551, 'grad_norm': 10.553475379943848, 'learning_rate': 2.7414330218068536e-05, 'epoch': 1.36}
{'loss': 0.2802, 'grad_norm': 1.647932529449463, 'learning_rate': 2.663551401869159e-05, 'epoch': 1.4}
{'loss': 0.1732, 'grad_norm': 2.171431303024292, 'learning_rate': 2.585669781931464e-05, 'epoch': 1.45}
{'loss': 0.2001, 'grad_norm': 8.384760856628418, 'learning_rate': 2.5077881619937692e-05, 'epoch': 1.5}
{'loss': 0.2192, 'grad_norm': 12.76718807220459, 'learning_rate': 2.429906542056075e-05, 'epoch': 1.54}
{'loss': 0.1832, 'grad_norm': 6.971376895904541, 'learning_rate': 2.3520249221183802e-05, 'epoch': 1.59}
{'loss': 0.2297, 'grad_norm': 4.640621662139893, 'learning_rate': 2.2741433021806856e-05, 'epoch': 1.64}
{'loss': 0.2128, 'grad_norm': 4.0071282386779785, 'learning_rate': 2.196261682242991e-05, 'epoch': 1.68}
{'loss': 0.2188, 'grad_norm': 6.686322212219238, 'learning_rate': 2.118380062305296e-05, 'epoch': 1.73}
{'loss': 0.2195, 'grad_norm': 6.098724365234375, 'learning_rate': 2.0404984423676012e-05, 'epoch': 1.78}
{'loss': 0.3026, 'grad_norm': 6.980874061584473, 'learning_rate': 1.9626168224299065e-05, 'epoch': 1.82}
{'loss': 0.1876, 'grad_norm': 17.088571548461914, 'learning_rate': 1.884735202492212e-05, 'epoch': 1.87}
{'loss': 0.1995, 'grad_norm': 2.590850591659546, 'learning_rate': 1.8068535825545172e-05, 'epoch': 1.92}
{'loss': 0.2328, 'grad_norm': 7.12017297744751, 'learning_rate': 1.7289719626168225e-05, 'epoch': 1.96}
  warnings.warn(                                                                                                                  
{'eval_loss': 0.23903706669807434, 'eval_accuracy': 0.9276315789473685, 'eval_micro_f1': 0.9276315789473685, 'eval_macro_f1': 0.9261464432084309, 'eval_runtime': 1.8048, 'eval_samples_per_second': 421.094, 'eval_steps_per_second': 13.298, 'epoch': 2.0}
 78%|██████████████████████████████████████████████████████████████████████▊                    | 500/642 [01:37<00:26,  5.45it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
{'loss': 0.1792, 'grad_norm': 2.2967569828033447, 'learning_rate': 1.6510903426791275e-05, 'epoch': 2.01}
{'loss': 0.0765, 'grad_norm': 2.767153739929199, 'learning_rate': 1.573208722741433e-05, 'epoch': 2.06}
{'loss': 0.1322, 'grad_norm': 4.099215984344482, 'learning_rate': 1.4953271028037382e-05, 'epoch': 2.1}
{'loss': 0.1058, 'grad_norm': 5.253079891204834, 'learning_rate': 1.4174454828660435e-05, 'epoch': 2.15}
{'loss': 0.0739, 'grad_norm': 5.165468215942383, 'learning_rate': 1.3395638629283489e-05, 'epoch': 2.2}
{'loss': 0.1012, 'grad_norm': 3.9168057441711426, 'learning_rate': 1.2616822429906542e-05, 'epoch': 2.24}
{'loss': 0.0963, 'grad_norm': 8.64197826385498, 'learning_rate': 1.1838006230529595e-05, 'epoch': 2.29}
{'loss': 0.159, 'grad_norm': 9.989625930786133, 'learning_rate': 1.1059190031152649e-05, 'epoch': 2.34}
  warnings.warn(
100%|███████████████████████████████████████████████████████████████████████████████████████████| 642/642 [02:06<00:00,  5.21it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
{'loss': 0.0971, 'grad_norm': 2.3075389862060547, 'learning_rate': 1.02803738317757e-05, 'epoch': 2.38}
{'loss': 0.1241, 'grad_norm': 4.506132125854492, 'learning_rate': 9.501557632398754e-06, 'epoch': 2.43}
{'loss': 0.0942, 'grad_norm': 2.249814987182617, 'learning_rate': 8.722741433021807e-06, 'epoch': 2.48}
{'loss': 0.1272, 'grad_norm': 2.8754241466522217, 'learning_rate': 7.94392523364486e-06, 'epoch': 2.52}
{'loss': 0.1423, 'grad_norm': 11.804232597351074, 'learning_rate': 7.165109034267913e-06, 'epoch': 2.57}
{'loss': 0.1086, 'grad_norm': 1.646412968635559, 'learning_rate': 6.386292834890965e-06, 'epoch': 2.62}
{'loss': 0.1205, 'grad_norm': 4.349637508392334, 'learning_rate': 5.607476635514019e-06, 'epoch': 2.66}
{'loss': 0.0978, 'grad_norm': 7.684413909912109, 'learning_rate': 4.828660436137072e-06, 'epoch': 2.71}
{'loss': 0.1022, 'grad_norm': 1.029217004776001, 'learning_rate': 4.0498442367601245e-06, 'epoch': 2.76}
{'loss': 0.0767, 'grad_norm': 1.309725046157837, 'learning_rate': 3.2710280373831774e-06, 'epoch': 2.8}
{'loss': 0.0703, 'grad_norm': 1.362921118736267, 'learning_rate': 2.4922118380062308e-06, 'epoch': 2.85}
{'loss': 0.1337, 'grad_norm': 4.074063777923584, 'learning_rate': 1.7133956386292835e-06, 'epoch': 2.9}
{'loss': 0.0843, 'grad_norm': 2.4368338584899902, 'learning_rate': 9.345794392523364e-07, 'epoch': 2.94}
{'loss': 0.0744, 'grad_norm': 1.0727038383483887, 'learning_rate': 1.5576323987538942e-07, 'epoch': 2.99}
  warnings.warn(
100%|███████████████████████████████████████████████████████████████████████████████████████████| 642/642 [02:10<00:00,  4.91it/s]
2024-10-29 11:15:37 - INFO - __main__ - Starting evaluation...                                                                    
{'eval_loss': 0.2681531012058258, 'eval_accuracy': 0.9131578947368421, 'eval_micro_f1': 0.9131578947368421, 'eval_macro_f1': 0.9121994598476194, 'eval_runtime': 1.8528, 'eval_samples_per_second': 410.189, 'eval_steps_per_second': 12.953, 'epoch': 3.0}
{'train_runtime': 130.7591, 'train_samples_per_second': 156.93, 'train_steps_per_second': 4.91, 'train_loss': 0.26341831897649437, 'epoch': 3.0}
/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
100%|█████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:01<00:00, 13.92it/s]
2024-10-29 11:15:39 - INFO - __main__ - Evaluation results: {'eval_loss': 0.2681531012058258, 'eval_accuracy': 0.9131578947368421, 'eval_micro_f1': 0.9131578947368421, 'eval_macro_f1': 0.9121994598476194, 'eval_runtime': 1.8055, 'eval_samples_per_second': 420.936, 'eval_steps_per_second': 13.293, 'epoch': 3.0}
2024-10-29 11:15:39 - INFO - __main__ - Training complete.
