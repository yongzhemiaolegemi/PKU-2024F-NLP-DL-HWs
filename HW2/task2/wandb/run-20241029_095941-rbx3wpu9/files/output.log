2024-10-29 09:59:43 - INFO - __main__ - Initializing training script
2024-10-29 09:59:43 - INFO - __main__ - Loading dataset...
2024-10-29 09:59:43 - INFO - __main__ - Loading model and tokenizer...
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Map: 100%|██████████████████████████████████████████████████████████████████████████| 3452/3452 [00:00<00:00, 15783.63 examples/s]
Map: 100%|██████████████████████████████████████████████████████████████████████████| 1120/1120 [00:00<00:00, 16127.95 examples/s]
/root/HW2/task2/train.py:99: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
2024-10-29 09:59:49 - INFO - __main__ - Starting training...
  0%|                                                                                                     | 0/324 [00:00<?, ?it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
 31%|████████████████████████████                                                                | 99/324 [00:19<00:41,  5.36it/s]Traceback (most recent call last):
{'loss': 0.9669, 'grad_norm': 5.14216423034668, 'learning_rate': 4.845679012345679e-05, 'epoch': 0.09}
{'loss': 0.96, 'grad_norm': 5.963944911956787, 'learning_rate': 4.691358024691358e-05, 'epoch': 0.19}
{'loss': 0.8186, 'grad_norm': 12.860967636108398, 'learning_rate': 4.5370370370370374e-05, 'epoch': 0.28}
{'loss': 0.7289, 'grad_norm': 11.799391746520996, 'learning_rate': 4.3827160493827164e-05, 'epoch': 0.37}
{'loss': 0.6113, 'grad_norm': 6.549756050109863, 'learning_rate': 4.2283950617283955e-05, 'epoch': 0.46}
{'loss': 0.6846, 'grad_norm': 10.280709266662598, 'learning_rate': 4.074074074074074e-05, 'epoch': 0.56}
{'loss': 0.5884, 'grad_norm': 8.52060604095459, 'learning_rate': 3.9197530864197535e-05, 'epoch': 0.65}
{'loss': 0.5733, 'grad_norm': 13.240150451660156, 'learning_rate': 3.7654320987654326e-05, 'epoch': 0.74}
{'loss': 0.6323, 'grad_norm': 6.974353790283203, 'learning_rate': 3.611111111111111e-05, 'epoch': 0.83}
  File "/root/HW2/task2/train.py", line 113, in <module>
    trainer.train()
  File "/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
  File "/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/trainer.py", line 2479, in _inner_training_loop
    and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))
KeyboardInterrupt
