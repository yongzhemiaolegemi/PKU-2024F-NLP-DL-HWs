2024-10-29 11:23:13 - INFO - __main__ - Initializing training script
2024-10-29 11:23:13 - INFO - __main__ - Loading dataset...
2024-10-29 11:23:21 - INFO - __main__ - Loading model and tokenizer...
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/root/HW2/task2/train.py:101: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
2024-10-29 11:23:27 - INFO - __main__ - Starting training...
  0%|                                                                                                     | 0/642 [00:00<?, ?it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
 33%|██████████████████████████████▎                                                            | 214/642 [00:43<01:15,  5.63it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
{'loss': 1.3129, 'grad_norm': 4.925628662109375, 'learning_rate': 4.922118380062305e-05, 'epoch': 0.05}
{'loss': 0.7521, 'grad_norm': 9.011994361877441, 'learning_rate': 4.844236760124611e-05, 'epoch': 0.09}
{'loss': 0.6122, 'grad_norm': 5.782170295715332, 'learning_rate': 4.766355140186916e-05, 'epoch': 0.14}
{'loss': 0.4589, 'grad_norm': 8.538688659667969, 'learning_rate': 4.6884735202492216e-05, 'epoch': 0.19}
{'loss': 0.3784, 'grad_norm': 7.32169771194458, 'learning_rate': 4.6105919003115266e-05, 'epoch': 0.23}
{'loss': 0.3769, 'grad_norm': 5.108719348907471, 'learning_rate': 4.532710280373832e-05, 'epoch': 0.28}
{'loss': 0.474, 'grad_norm': 10.174644470214844, 'learning_rate': 4.454828660436137e-05, 'epoch': 0.33}
{'loss': 0.3965, 'grad_norm': 8.883426666259766, 'learning_rate': 4.376947040498442e-05, 'epoch': 0.37}
{'loss': 0.4373, 'grad_norm': 5.944844722747803, 'learning_rate': 4.299065420560748e-05, 'epoch': 0.42}
{'loss': 0.4961, 'grad_norm': 9.220013618469238, 'learning_rate': 4.221183800623053e-05, 'epoch': 0.47}
{'loss': 0.4205, 'grad_norm': 5.730432987213135, 'learning_rate': 4.1433021806853586e-05, 'epoch': 0.51}
{'loss': 0.325, 'grad_norm': 3.884338855743408, 'learning_rate': 4.0654205607476636e-05, 'epoch': 0.56}
{'loss': 0.4403, 'grad_norm': 6.379284381866455, 'learning_rate': 3.987538940809969e-05, 'epoch': 0.61}
{'loss': 0.4038, 'grad_norm': 7.1996259689331055, 'learning_rate': 3.909657320872274e-05, 'epoch': 0.65}
{'loss': 0.3922, 'grad_norm': 7.063690185546875, 'learning_rate': 3.831775700934579e-05, 'epoch': 0.7}
{'loss': 0.2913, 'grad_norm': 15.22734546661377, 'learning_rate': 3.753894080996885e-05, 'epoch': 0.75}
{'loss': 0.3626, 'grad_norm': 8.01725959777832, 'learning_rate': 3.67601246105919e-05, 'epoch': 0.79}
{'loss': 0.3577, 'grad_norm': 9.41108226776123, 'learning_rate': 3.5981308411214956e-05, 'epoch': 0.84}
{'loss': 0.3994, 'grad_norm': 13.14166259765625, 'learning_rate': 3.5202492211838006e-05, 'epoch': 0.89}
{'loss': 0.348, 'grad_norm': 9.895914077758789, 'learning_rate': 3.442367601246106e-05, 'epoch': 0.93}
{'loss': 0.2974, 'grad_norm': 8.119016647338867, 'learning_rate': 3.364485981308411e-05, 'epoch': 0.98}
  warnings.warn(                                                                                                                  
{'eval_loss': 0.25819048285484314, 'eval_accuracy': 0.9197368421052632, 'eval_micro_f1': 0.9197368421052632, 'eval_macro_f1': 0.9185348422022839, 'eval_runtime': 1.8382, 'eval_samples_per_second': 413.441, 'eval_steps_per_second': 13.056, 'epoch': 1.0}
 67%|████████████████████████████████████████████████████████████▋                              | 428/642 [01:25<00:37,  5.64it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
{'loss': 0.2756, 'grad_norm': 8.232311248779297, 'learning_rate': 3.286604361370716e-05, 'epoch': 1.03}
{'loss': 0.239, 'grad_norm': 1.0228981971740723, 'learning_rate': 3.208722741433022e-05, 'epoch': 1.07}
{'loss': 0.2666, 'grad_norm': 6.527553081512451, 'learning_rate': 3.130841121495327e-05, 'epoch': 1.12}
{'loss': 0.2119, 'grad_norm': 5.519537448883057, 'learning_rate': 3.0529595015576326e-05, 'epoch': 1.17}
{'loss': 0.2781, 'grad_norm': 9.346996307373047, 'learning_rate': 2.9750778816199376e-05, 'epoch': 1.21}
{'loss': 0.2324, 'grad_norm': 2.903446674346924, 'learning_rate': 2.897196261682243e-05, 'epoch': 1.26}
{'loss': 0.1707, 'grad_norm': 3.302537441253662, 'learning_rate': 2.8193146417445482e-05, 'epoch': 1.31}
{'loss': 0.2124, 'grad_norm': 8.562777519226074, 'learning_rate': 2.7414330218068536e-05, 'epoch': 1.36}
{'loss': 0.1551, 'grad_norm': 2.4956212043762207, 'learning_rate': 2.663551401869159e-05, 'epoch': 1.4}
{'loss': 0.224, 'grad_norm': 2.136037588119507, 'learning_rate': 2.585669781931464e-05, 'epoch': 1.45}
{'loss': 0.1443, 'grad_norm': 2.127723217010498, 'learning_rate': 2.5077881619937692e-05, 'epoch': 1.5}
{'loss': 0.2489, 'grad_norm': 10.390042304992676, 'learning_rate': 2.429906542056075e-05, 'epoch': 1.54}
{'loss': 0.2309, 'grad_norm': 3.6961264610290527, 'learning_rate': 2.3520249221183802e-05, 'epoch': 1.59}
{'loss': 0.2245, 'grad_norm': 8.020358085632324, 'learning_rate': 2.2741433021806856e-05, 'epoch': 1.64}
{'loss': 0.1797, 'grad_norm': 5.868407726287842, 'learning_rate': 2.196261682242991e-05, 'epoch': 1.68}
{'loss': 0.1631, 'grad_norm': 9.290509223937988, 'learning_rate': 2.118380062305296e-05, 'epoch': 1.73}
{'loss': 0.1739, 'grad_norm': 7.158867835998535, 'learning_rate': 2.0404984423676012e-05, 'epoch': 1.78}
{'loss': 0.1987, 'grad_norm': 3.91614031791687, 'learning_rate': 1.9626168224299065e-05, 'epoch': 1.82}
{'loss': 0.2325, 'grad_norm': 2.4326171875, 'learning_rate': 1.884735202492212e-05, 'epoch': 1.87}
{'loss': 0.1854, 'grad_norm': 8.114156723022461, 'learning_rate': 1.8068535825545172e-05, 'epoch': 1.92}
{'loss': 0.2215, 'grad_norm': 6.919060230255127, 'learning_rate': 1.7289719626168225e-05, 'epoch': 1.96}
  warnings.warn(                                                                                                                  
{'eval_loss': 0.2522076666355133, 'eval_accuracy': 0.9105263157894737, 'eval_micro_f1': 0.9105263157894737, 'eval_macro_f1': 0.9089435211852861, 'eval_runtime': 1.8383, 'eval_samples_per_second': 413.428, 'eval_steps_per_second': 13.056, 'epoch': 2.0}
 78%|██████████████████████████████████████████████████████████████████████▊                    | 500/642 [01:38<00:27,  5.24it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
{'loss': 0.1211, 'grad_norm': 4.326210021972656, 'learning_rate': 1.6510903426791275e-05, 'epoch': 2.01}
{'loss': 0.0937, 'grad_norm': 0.9042876362800598, 'learning_rate': 1.573208722741433e-05, 'epoch': 2.06}
{'loss': 0.1197, 'grad_norm': 2.7080800533294678, 'learning_rate': 1.4953271028037382e-05, 'epoch': 2.1}
{'loss': 0.1232, 'grad_norm': 7.229271411895752, 'learning_rate': 1.4174454828660435e-05, 'epoch': 2.15}
{'loss': 0.1029, 'grad_norm': 4.5834503173828125, 'learning_rate': 1.3395638629283489e-05, 'epoch': 2.2}
{'loss': 0.062, 'grad_norm': 2.5132157802581787, 'learning_rate': 1.2616822429906542e-05, 'epoch': 2.24}
{'loss': 0.0961, 'grad_norm': 3.039008617401123, 'learning_rate': 1.1838006230529595e-05, 'epoch': 2.29}
{'loss': 0.1708, 'grad_norm': 3.8415584564208984, 'learning_rate': 1.1059190031152649e-05, 'epoch': 2.34}
  warnings.warn(
100%|███████████████████████████████████████████████████████████████████████████████████████████| 642/642 [02:07<00:00,  5.61it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
{'loss': 0.0655, 'grad_norm': 2.3367998600006104, 'learning_rate': 1.02803738317757e-05, 'epoch': 2.38}
{'loss': 0.0577, 'grad_norm': 4.428262710571289, 'learning_rate': 9.501557632398754e-06, 'epoch': 2.43}
{'loss': 0.1056, 'grad_norm': 6.479736804962158, 'learning_rate': 8.722741433021807e-06, 'epoch': 2.48}
{'loss': 0.054, 'grad_norm': 2.854652166366577, 'learning_rate': 7.94392523364486e-06, 'epoch': 2.52}
{'loss': 0.056, 'grad_norm': 1.8505045175552368, 'learning_rate': 7.165109034267913e-06, 'epoch': 2.57}
{'loss': 0.1289, 'grad_norm': 1.000858187675476, 'learning_rate': 6.386292834890965e-06, 'epoch': 2.62}
{'loss': 0.1359, 'grad_norm': 5.284504413604736, 'learning_rate': 5.607476635514019e-06, 'epoch': 2.66}
{'loss': 0.0689, 'grad_norm': 0.3810531198978424, 'learning_rate': 4.828660436137072e-06, 'epoch': 2.71}
{'loss': 0.0648, 'grad_norm': 4.242587566375732, 'learning_rate': 4.0498442367601245e-06, 'epoch': 2.76}
{'loss': 0.079, 'grad_norm': 6.592752456665039, 'learning_rate': 3.2710280373831774e-06, 'epoch': 2.8}
{'loss': 0.1192, 'grad_norm': 8.13853931427002, 'learning_rate': 2.4922118380062308e-06, 'epoch': 2.85}
{'loss': 0.1285, 'grad_norm': 15.8535795211792, 'learning_rate': 1.7133956386292835e-06, 'epoch': 2.9}
{'loss': 0.1113, 'grad_norm': 8.706954956054688, 'learning_rate': 9.345794392523364e-07, 'epoch': 2.94}
{'loss': 0.0619, 'grad_norm': 0.25252974033355713, 'learning_rate': 1.5576323987538942e-07, 'epoch': 2.99}
  warnings.warn(
100%|███████████████████████████████████████████████████████████████████████████████████████████| 642/642 [02:11<00:00,  4.89it/s]
2024-10-29 11:25:39 - INFO - __main__ - Starting evaluation...                                                                    
{'eval_loss': 0.2917160391807556, 'eval_accuracy': 0.9184210526315789, 'eval_micro_f1': 0.9184210526315789, 'eval_macro_f1': 0.9165686657926725, 'eval_runtime': 1.9742, 'eval_samples_per_second': 384.963, 'eval_steps_per_second': 12.157, 'epoch': 3.0}
{'train_runtime': 131.2255, 'train_samples_per_second': 156.372, 'train_steps_per_second': 4.892, 'train_loss': 0.2549418249048548, 'epoch': 3.0}
/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
100%|█████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:01<00:00, 13.83it/s]
2024-10-29 11:25:41 - INFO - __main__ - Evaluation results: {'eval_loss': 0.2917160391807556, 'eval_accuracy': 0.9184210526315789, 'eval_micro_f1': 0.9184210526315789, 'eval_macro_f1': 0.9165686657926725, 'eval_runtime': 1.8201, 'eval_samples_per_second': 417.566, 'eval_steps_per_second': 13.186, 'epoch': 3.0}
2024-10-29 11:25:41 - INFO - __main__ - Training complete.
