2024-10-29 10:46:49 - INFO - __main__ - Initializing training script
2024-10-29 10:46:49 - INFO - __main__ - Loading dataset...
2024-10-29 10:46:57 - INFO - __main__ - Loading model and tokenizer...
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Map: 100%|█████████████████████████████████████████████████████████████████████████████| 760/760 [00:00<00:00, 5582.20 examples/s]
/root/HW2/task2/train.py:101: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
2024-10-29 10:47:02 - INFO - __main__ - Starting training...
  0%|                                                                                                     | 0/642 [00:00<?, ?it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
 33%|██████████████████████████████▎                                                            | 214/642 [00:42<01:13,  5.84it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
{'loss': 1.2331, 'grad_norm': 5.389796257019043, 'learning_rate': 4.922118380062305e-05, 'epoch': 0.05}
{'loss': 0.7961, 'grad_norm': 3.4094932079315186, 'learning_rate': 4.844236760124611e-05, 'epoch': 0.09}
{'loss': 0.4881, 'grad_norm': 2.85577130317688, 'learning_rate': 4.766355140186916e-05, 'epoch': 0.14}
{'loss': 0.429, 'grad_norm': 10.671592712402344, 'learning_rate': 4.6884735202492216e-05, 'epoch': 0.19}
{'loss': 0.4593, 'grad_norm': 6.049612998962402, 'learning_rate': 4.6105919003115266e-05, 'epoch': 0.23}
{'loss': 0.4278, 'grad_norm': 10.518967628479004, 'learning_rate': 4.532710280373832e-05, 'epoch': 0.28}
{'loss': 0.4204, 'grad_norm': 11.992732048034668, 'learning_rate': 4.454828660436137e-05, 'epoch': 0.33}
{'loss': 0.2974, 'grad_norm': 3.117982864379883, 'learning_rate': 4.376947040498442e-05, 'epoch': 0.37}
{'loss': 0.3687, 'grad_norm': 5.252753734588623, 'learning_rate': 4.299065420560748e-05, 'epoch': 0.42}
{'loss': 0.2846, 'grad_norm': 22.739540100097656, 'learning_rate': 4.221183800623053e-05, 'epoch': 0.47}
{'loss': 0.2784, 'grad_norm': 3.208047866821289, 'learning_rate': 4.1433021806853586e-05, 'epoch': 0.51}
{'loss': 0.3402, 'grad_norm': 7.902439117431641, 'learning_rate': 4.0654205607476636e-05, 'epoch': 0.56}
{'loss': 0.327, 'grad_norm': 7.644326686859131, 'learning_rate': 3.987538940809969e-05, 'epoch': 0.61}
{'loss': 0.3028, 'grad_norm': 2.297102689743042, 'learning_rate': 3.909657320872274e-05, 'epoch': 0.65}
{'loss': 0.3025, 'grad_norm': 6.275296688079834, 'learning_rate': 3.831775700934579e-05, 'epoch': 0.7}
{'loss': 0.3204, 'grad_norm': 5.144009113311768, 'learning_rate': 3.753894080996885e-05, 'epoch': 0.75}
{'loss': 0.3285, 'grad_norm': 3.926165819168091, 'learning_rate': 3.67601246105919e-05, 'epoch': 0.79}
{'loss': 0.2863, 'grad_norm': 3.0710198879241943, 'learning_rate': 3.5981308411214956e-05, 'epoch': 0.84}
{'loss': 0.2526, 'grad_norm': 7.33050012588501, 'learning_rate': 3.5202492211838006e-05, 'epoch': 0.89}
{'loss': 0.2835, 'grad_norm': 38.64199447631836, 'learning_rate': 3.442367601246106e-05, 'epoch': 0.93}
{'loss': 0.3225, 'grad_norm': 5.144580364227295, 'learning_rate': 3.364485981308411e-05, 'epoch': 0.98}
  warnings.warn(                                                                                                                  
{'eval_loss': 0.26070019602775574, 'eval_accuracy': 0.9171052631578948, 'eval_micro_f1': 0.9171052631578948, 'eval_macro_f1': 0.9144563352046147, 'eval_runtime': 1.7643, 'eval_samples_per_second': 430.764, 'eval_steps_per_second': 13.603, 'epoch': 1.0}
 67%|████████████████████████████████████████████████████████████▋                              | 428/642 [01:22<00:36,  5.88it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
{'loss': 0.2632, 'grad_norm': 5.087735176086426, 'learning_rate': 3.286604361370716e-05, 'epoch': 1.03}
{'loss': 0.2048, 'grad_norm': 2.8003852367401123, 'learning_rate': 3.208722741433022e-05, 'epoch': 1.07}
{'loss': 0.1746, 'grad_norm': 4.472122669219971, 'learning_rate': 3.130841121495327e-05, 'epoch': 1.12}
{'loss': 0.2021, 'grad_norm': 8.247570037841797, 'learning_rate': 3.0529595015576326e-05, 'epoch': 1.17}
{'loss': 0.1149, 'grad_norm': 5.706843376159668, 'learning_rate': 2.9750778816199376e-05, 'epoch': 1.21}
{'loss': 0.1845, 'grad_norm': 6.112496376037598, 'learning_rate': 2.897196261682243e-05, 'epoch': 1.26}
{'loss': 0.1948, 'grad_norm': 0.7353807091712952, 'learning_rate': 2.8193146417445482e-05, 'epoch': 1.31}
{'loss': 0.2138, 'grad_norm': 4.027698040008545, 'learning_rate': 2.7414330218068536e-05, 'epoch': 1.36}
{'loss': 0.1785, 'grad_norm': 0.2583842873573303, 'learning_rate': 2.663551401869159e-05, 'epoch': 1.4}
{'loss': 0.175, 'grad_norm': 7.576070308685303, 'learning_rate': 2.585669781931464e-05, 'epoch': 1.45}
{'loss': 0.2117, 'grad_norm': 8.046939849853516, 'learning_rate': 2.5077881619937692e-05, 'epoch': 1.5}
{'loss': 0.2069, 'grad_norm': 5.475322723388672, 'learning_rate': 2.429906542056075e-05, 'epoch': 1.54}
{'loss': 0.1507, 'grad_norm': 17.2817325592041, 'learning_rate': 2.3520249221183802e-05, 'epoch': 1.59}
{'loss': 0.1508, 'grad_norm': 1.3661963939666748, 'learning_rate': 2.2741433021806856e-05, 'epoch': 1.64}
{'loss': 0.2213, 'grad_norm': 7.823699474334717, 'learning_rate': 2.196261682242991e-05, 'epoch': 1.68}
{'loss': 0.1558, 'grad_norm': 7.738346099853516, 'learning_rate': 2.118380062305296e-05, 'epoch': 1.73}
{'loss': 0.1995, 'grad_norm': 3.429760694503784, 'learning_rate': 2.0404984423676012e-05, 'epoch': 1.78}
{'loss': 0.1891, 'grad_norm': 9.068811416625977, 'learning_rate': 1.9626168224299065e-05, 'epoch': 1.82}
{'loss': 0.2192, 'grad_norm': 1.268548846244812, 'learning_rate': 1.884735202492212e-05, 'epoch': 1.87}
{'loss': 0.1542, 'grad_norm': 2.0239181518554688, 'learning_rate': 1.8068535825545172e-05, 'epoch': 1.92}
{'loss': 0.2072, 'grad_norm': 7.946625709533691, 'learning_rate': 1.7289719626168225e-05, 'epoch': 1.96}
  warnings.warn(                                                                                                                  
{'eval_loss': 0.21089224517345428, 'eval_accuracy': 0.9315789473684211, 'eval_micro_f1': 0.9315789473684211, 'eval_macro_f1': 0.9297610111936147, 'eval_runtime': 1.7567, 'eval_samples_per_second': 432.621, 'eval_steps_per_second': 13.662, 'epoch': 2.0}
 78%|██████████████████████████████████████████████████████████████████████▊                    | 500/642 [01:35<00:25,  5.59it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
{'loss': 0.1697, 'grad_norm': 3.2575016021728516, 'learning_rate': 1.6510903426791275e-05, 'epoch': 2.01}
{'loss': 0.1099, 'grad_norm': 1.5203722715377808, 'learning_rate': 1.573208722741433e-05, 'epoch': 2.06}
{'loss': 0.0872, 'grad_norm': 0.8795442581176758, 'learning_rate': 1.4953271028037382e-05, 'epoch': 2.1}
{'loss': 0.0873, 'grad_norm': 2.174588680267334, 'learning_rate': 1.4174454828660435e-05, 'epoch': 2.15}
{'loss': 0.0914, 'grad_norm': 0.24690183997154236, 'learning_rate': 1.3395638629283489e-05, 'epoch': 2.2}
{'loss': 0.0722, 'grad_norm': 3.3231587409973145, 'learning_rate': 1.2616822429906542e-05, 'epoch': 2.24}
{'loss': 0.0601, 'grad_norm': 6.499504566192627, 'learning_rate': 1.1838006230529595e-05, 'epoch': 2.29}
{'loss': 0.0932, 'grad_norm': 3.1929945945739746, 'learning_rate': 1.1059190031152649e-05, 'epoch': 2.34}
  warnings.warn(
100%|███████████████████████████████████████████████████████████████████████████████████████████| 642/642 [02:02<00:00,  5.31it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
{'loss': 0.1114, 'grad_norm': 1.7569841146469116, 'learning_rate': 1.02803738317757e-05, 'epoch': 2.38}
{'loss': 0.1041, 'grad_norm': 5.105485916137695, 'learning_rate': 9.501557632398754e-06, 'epoch': 2.43}
{'loss': 0.0728, 'grad_norm': 0.27105510234832764, 'learning_rate': 8.722741433021807e-06, 'epoch': 2.48}
{'loss': 0.1263, 'grad_norm': 9.56383991241455, 'learning_rate': 7.94392523364486e-06, 'epoch': 2.52}
{'loss': 0.1381, 'grad_norm': 5.956909656524658, 'learning_rate': 7.165109034267913e-06, 'epoch': 2.57}
{'loss': 0.0765, 'grad_norm': 5.859522819519043, 'learning_rate': 6.386292834890965e-06, 'epoch': 2.62}
{'loss': 0.109, 'grad_norm': 3.2227962017059326, 'learning_rate': 5.607476635514019e-06, 'epoch': 2.66}
{'loss': 0.0469, 'grad_norm': 7.8640265464782715, 'learning_rate': 4.828660436137072e-06, 'epoch': 2.71}
{'loss': 0.0989, 'grad_norm': 4.676579475402832, 'learning_rate': 4.0498442367601245e-06, 'epoch': 2.76}
{'loss': 0.0652, 'grad_norm': 0.2733542025089264, 'learning_rate': 3.2710280373831774e-06, 'epoch': 2.8}
{'loss': 0.0762, 'grad_norm': 0.9413034319877625, 'learning_rate': 2.4922118380062308e-06, 'epoch': 2.85}
{'loss': 0.091, 'grad_norm': 3.112741708755493, 'learning_rate': 1.7133956386292835e-06, 'epoch': 2.9}
{'loss': 0.0982, 'grad_norm': 0.29967251420021057, 'learning_rate': 9.345794392523364e-07, 'epoch': 2.94}
{'loss': 0.0909, 'grad_norm': 7.733522891998291, 'learning_rate': 1.5576323987538942e-07, 'epoch': 2.99}
  warnings.warn(
100%|███████████████████████████████████████████████████████████████████████████████████████████| 642/642 [02:06<00:00,  5.09it/s]
2024-10-29 10:49:08 - INFO - __main__ - Starting evaluation...                                                                    
{'eval_loss': 0.23561088740825653, 'eval_accuracy': 0.9315789473684211, 'eval_micro_f1': 0.9315789473684211, 'eval_macro_f1': 0.9299476118025047, 'eval_runtime': 1.7337, 'eval_samples_per_second': 438.377, 'eval_steps_per_second': 13.843, 'epoch': 3.0}
{'train_runtime': 126.2349, 'train_samples_per_second': 162.554, 'train_steps_per_second': 5.086, 'train_loss': 0.22751329781203256, 'epoch': 3.0}
/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
100%|█████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:01<00:00, 14.47it/s]
2024-10-29 10:49:10 - INFO - __main__ - Evaluation results: {'eval_loss': 0.23561088740825653, 'eval_accuracy': 0.9315789473684211, 'eval_micro_f1': 0.9315789473684211, 'eval_macro_f1': 0.9299476118025047, 'eval_runtime': 1.7374, 'eval_samples_per_second': 437.436, 'eval_steps_per_second': 13.814, 'epoch': 3.0}
2024-10-29 10:49:10 - INFO - __main__ - Training complete.
