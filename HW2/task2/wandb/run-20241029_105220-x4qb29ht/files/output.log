2024-10-29 10:52:21 - INFO - __main__ - Initializing training script
2024-10-29 10:52:21 - INFO - __main__ - Loading dataset...
2024-10-29 10:52:29 - INFO - __main__ - Loading model and tokenizer...
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/root/HW2/task2/train.py:101: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
2024-10-29 10:52:34 - INFO - __main__ - Starting training...
  0%|                                                                                                     | 0/642 [00:00<?, ?it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
 33%|██████████████████████████████▎                                                            | 214/642 [00:41<01:15,  5.67it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
{'loss': 1.2536, 'grad_norm': 6.814509868621826, 'learning_rate': 4.922118380062305e-05, 'epoch': 0.05}
{'loss': 0.7377, 'grad_norm': 4.083258152008057, 'learning_rate': 4.844236760124611e-05, 'epoch': 0.09}
{'loss': 0.4402, 'grad_norm': 8.029101371765137, 'learning_rate': 4.766355140186916e-05, 'epoch': 0.14}
{'loss': 0.4683, 'grad_norm': 10.415628433227539, 'learning_rate': 4.6884735202492216e-05, 'epoch': 0.19}
{'loss': 0.4125, 'grad_norm': 5.2613444328308105, 'learning_rate': 4.6105919003115266e-05, 'epoch': 0.23}
{'loss': 0.406, 'grad_norm': 8.48674488067627, 'learning_rate': 4.532710280373832e-05, 'epoch': 0.28}
{'loss': 0.3422, 'grad_norm': 5.416759014129639, 'learning_rate': 4.454828660436137e-05, 'epoch': 0.33}
{'loss': 0.3145, 'grad_norm': 1.9116590023040771, 'learning_rate': 4.376947040498442e-05, 'epoch': 0.37}
{'loss': 0.3508, 'grad_norm': 5.929488182067871, 'learning_rate': 4.299065420560748e-05, 'epoch': 0.42}
{'loss': 0.3113, 'grad_norm': 21.050857543945312, 'learning_rate': 4.221183800623053e-05, 'epoch': 0.47}
{'loss': 0.315, 'grad_norm': 3.7370879650115967, 'learning_rate': 4.1433021806853586e-05, 'epoch': 0.51}
{'loss': 0.2964, 'grad_norm': 2.404599905014038, 'learning_rate': 4.0654205607476636e-05, 'epoch': 0.56}
{'loss': 0.2184, 'grad_norm': 9.543925285339355, 'learning_rate': 3.987538940809969e-05, 'epoch': 0.61}
{'loss': 0.2509, 'grad_norm': 14.741764068603516, 'learning_rate': 3.909657320872274e-05, 'epoch': 0.65}
{'loss': 0.2567, 'grad_norm': 4.355165004730225, 'learning_rate': 3.831775700934579e-05, 'epoch': 0.7}
{'loss': 0.2996, 'grad_norm': 9.829675674438477, 'learning_rate': 3.753894080996885e-05, 'epoch': 0.75}
{'loss': 0.2594, 'grad_norm': 2.1308178901672363, 'learning_rate': 3.67601246105919e-05, 'epoch': 0.79}
{'loss': 0.3247, 'grad_norm': 4.43950080871582, 'learning_rate': 3.5981308411214956e-05, 'epoch': 0.84}
{'loss': 0.3484, 'grad_norm': 8.743340492248535, 'learning_rate': 3.5202492211838006e-05, 'epoch': 0.89}
{'loss': 0.3223, 'grad_norm': 8.87865924835205, 'learning_rate': 3.442367601246106e-05, 'epoch': 0.93}
{'loss': 0.2585, 'grad_norm': 3.8844316005706787, 'learning_rate': 3.364485981308411e-05, 'epoch': 0.98}
  warnings.warn(                                                                                                                  
{'eval_loss': 0.24634170532226562, 'eval_accuracy': 0.9078947368421053, 'eval_micro_f1': 0.9078947368421053, 'eval_macro_f1': 0.9055055733269338, 'eval_runtime': 1.8144, 'eval_samples_per_second': 418.876, 'eval_steps_per_second': 13.228, 'epoch': 1.0}
 67%|████████████████████████████████████████████████████████████▋                              | 428/642 [01:21<00:36,  5.91it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
{'loss': 0.2998, 'grad_norm': 2.41760516166687, 'learning_rate': 3.286604361370716e-05, 'epoch': 1.03}
{'loss': 0.1526, 'grad_norm': 1.7957055568695068, 'learning_rate': 3.208722741433022e-05, 'epoch': 1.07}
{'loss': 0.2011, 'grad_norm': 2.0449726581573486, 'learning_rate': 3.130841121495327e-05, 'epoch': 1.12}
{'loss': 0.2094, 'grad_norm': 5.381927013397217, 'learning_rate': 3.0529595015576326e-05, 'epoch': 1.17}
{'loss': 0.1722, 'grad_norm': 1.4039783477783203, 'learning_rate': 2.9750778816199376e-05, 'epoch': 1.21}
{'loss': 0.2156, 'grad_norm': 3.756643056869507, 'learning_rate': 2.897196261682243e-05, 'epoch': 1.26}
{'loss': 0.1705, 'grad_norm': 8.793065071105957, 'learning_rate': 2.8193146417445482e-05, 'epoch': 1.31}
{'loss': 0.1435, 'grad_norm': 3.4288341999053955, 'learning_rate': 2.7414330218068536e-05, 'epoch': 1.36}
{'loss': 0.1626, 'grad_norm': 5.91989278793335, 'learning_rate': 2.663551401869159e-05, 'epoch': 1.4}
{'loss': 0.1642, 'grad_norm': 4.875039100646973, 'learning_rate': 2.585669781931464e-05, 'epoch': 1.45}
{'loss': 0.1886, 'grad_norm': 2.1657004356384277, 'learning_rate': 2.5077881619937692e-05, 'epoch': 1.5}
{'loss': 0.2072, 'grad_norm': 5.28883695602417, 'learning_rate': 2.429906542056075e-05, 'epoch': 1.54}
{'loss': 0.1263, 'grad_norm': 1.1077580451965332, 'learning_rate': 2.3520249221183802e-05, 'epoch': 1.59}
{'loss': 0.2133, 'grad_norm': 2.005296230316162, 'learning_rate': 2.2741433021806856e-05, 'epoch': 1.64}
{'loss': 0.1989, 'grad_norm': 5.155270576477051, 'learning_rate': 2.196261682242991e-05, 'epoch': 1.68}
{'loss': 0.241, 'grad_norm': 4.363204002380371, 'learning_rate': 2.118380062305296e-05, 'epoch': 1.73}
{'loss': 0.1432, 'grad_norm': 2.565911293029785, 'learning_rate': 2.0404984423676012e-05, 'epoch': 1.78}
{'loss': 0.1802, 'grad_norm': 2.77413010597229, 'learning_rate': 1.9626168224299065e-05, 'epoch': 1.82}
{'loss': 0.2092, 'grad_norm': 5.301370143890381, 'learning_rate': 1.884735202492212e-05, 'epoch': 1.87}
{'loss': 0.2097, 'grad_norm': 4.267154693603516, 'learning_rate': 1.8068535825545172e-05, 'epoch': 1.92}
{'loss': 0.2329, 'grad_norm': 6.998535633087158, 'learning_rate': 1.7289719626168225e-05, 'epoch': 1.96}
  warnings.warn(                                                                                                                  
{'eval_loss': 0.20712824165821075, 'eval_accuracy': 0.9276315789473685, 'eval_micro_f1': 0.9276315789473685, 'eval_macro_f1': 0.9258982532885445, 'eval_runtime': 1.7365, 'eval_samples_per_second': 437.654, 'eval_steps_per_second': 13.821, 'epoch': 2.0}
 78%|██████████████████████████████████████████████████████████████████████▊                    | 500/642 [01:35<00:26,  5.37it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
{'loss': 0.2215, 'grad_norm': 2.147045850753784, 'learning_rate': 1.6510903426791275e-05, 'epoch': 2.01}
{'loss': 0.1234, 'grad_norm': 4.606268405914307, 'learning_rate': 1.573208722741433e-05, 'epoch': 2.06}
{'loss': 0.0823, 'grad_norm': 4.193357944488525, 'learning_rate': 1.4953271028037382e-05, 'epoch': 2.1}
{'loss': 0.127, 'grad_norm': 1.9370428323745728, 'learning_rate': 1.4174454828660435e-05, 'epoch': 2.15}
{'loss': 0.0882, 'grad_norm': 0.8924447894096375, 'learning_rate': 1.3395638629283489e-05, 'epoch': 2.2}
{'loss': 0.0827, 'grad_norm': 1.2302908897399902, 'learning_rate': 1.2616822429906542e-05, 'epoch': 2.24}
{'loss': 0.1083, 'grad_norm': 6.088108539581299, 'learning_rate': 1.1838006230529595e-05, 'epoch': 2.29}
{'loss': 0.1036, 'grad_norm': 1.2089145183563232, 'learning_rate': 1.1059190031152649e-05, 'epoch': 2.34}
  warnings.warn(
100%|███████████████████████████████████████████████████████████████████████████████████████████| 642/642 [02:02<00:00,  5.88it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
{'loss': 0.0652, 'grad_norm': 4.133250713348389, 'learning_rate': 1.02803738317757e-05, 'epoch': 2.38}
{'loss': 0.1198, 'grad_norm': 1.448095440864563, 'learning_rate': 9.501557632398754e-06, 'epoch': 2.43}
{'loss': 0.0947, 'grad_norm': 5.062743186950684, 'learning_rate': 8.722741433021807e-06, 'epoch': 2.48}
{'loss': 0.0805, 'grad_norm': 1.4509186744689941, 'learning_rate': 7.94392523364486e-06, 'epoch': 2.52}
{'loss': 0.0603, 'grad_norm': 0.11430715769529343, 'learning_rate': 7.165109034267913e-06, 'epoch': 2.57}
{'loss': 0.1051, 'grad_norm': 4.544884204864502, 'learning_rate': 6.386292834890965e-06, 'epoch': 2.62}
{'loss': 0.1161, 'grad_norm': 12.483963966369629, 'learning_rate': 5.607476635514019e-06, 'epoch': 2.66}
{'loss': 0.0696, 'grad_norm': 0.4106256663799286, 'learning_rate': 4.828660436137072e-06, 'epoch': 2.71}
{'loss': 0.0478, 'grad_norm': 1.4285396337509155, 'learning_rate': 4.0498442367601245e-06, 'epoch': 2.76}
{'loss': 0.104, 'grad_norm': 0.9989049434661865, 'learning_rate': 3.2710280373831774e-06, 'epoch': 2.8}
{'loss': 0.1261, 'grad_norm': 0.40770992636680603, 'learning_rate': 2.4922118380062308e-06, 'epoch': 2.85}
{'loss': 0.0739, 'grad_norm': 1.1366089582443237, 'learning_rate': 1.7133956386292835e-06, 'epoch': 2.9}
{'loss': 0.1185, 'grad_norm': 6.691125869750977, 'learning_rate': 9.345794392523364e-07, 'epoch': 2.94}
{'loss': 0.0658, 'grad_norm': 1.6334377527236938, 'learning_rate': 1.5576323987538942e-07, 'epoch': 2.99}
  warnings.warn(
100%|███████████████████████████████████████████████████████████████████████████████████████████| 642/642 [02:06<00:00,  5.08it/s]
2024-10-29 10:54:40 - INFO - __main__ - Starting evaluation...                                                                    
{'eval_loss': 0.25111624598503113, 'eval_accuracy': 0.9289473684210526, 'eval_micro_f1': 0.9289473684210526, 'eval_macro_f1': 0.9270664063434455, 'eval_runtime': 1.9518, 'eval_samples_per_second': 389.39, 'eval_steps_per_second': 12.297, 'epoch': 3.0}
{'train_runtime': 126.4409, 'train_samples_per_second': 162.289, 'train_steps_per_second': 5.077, 'train_loss': 0.22462795749074574, 'epoch': 3.0}
/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
100%|█████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:01<00:00, 13.38it/s]
2024-10-29 10:54:42 - INFO - __main__ - Evaluation results: {'eval_loss': 0.25111624598503113, 'eval_accuracy': 0.9289473684210526, 'eval_micro_f1': 0.9289473684210526, 'eval_macro_f1': 0.9270664063434455, 'eval_runtime': 1.8845, 'eval_samples_per_second': 403.295, 'eval_steps_per_second': 12.736, 'epoch': 3.0}
2024-10-29 10:54:42 - INFO - __main__ - Training complete.
