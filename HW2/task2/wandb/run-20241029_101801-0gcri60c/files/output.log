2024-10-29 10:18:03 - INFO - __main__ - Initializing training script
2024-10-29 10:18:03 - INFO - __main__ - Loading dataset...
README.md: 8.07kB [00:00, 10.8MB/s]
train-00000-of-00001.parquet: 100%|██████████████████████████████████████████████████████████| 18.6M/18.6M [00:02<00:00, 8.03MB/s]
test-00000-of-00001.parquet: 100%|███████████████████████████████████████████████████████████| 1.23M/1.23M [00:00<00:00, 34.1MB/s]
Generating train split: 100%|██████████████████████████████████████████████████| 120000/120000 [00:00<00:00, 364973.07 examples/s]
Generating test split: 100%|███████████████████████████████████████████████████████| 7600/7600 [00:00<00:00, 280805.07 examples/s]
2024-10-29 10:18:18 - INFO - __main__ - Loading model and tokenizer...
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Map: 100%|███████████████████████████████████████████████████████████████████████████| 6840/6840 [00:00<00:00, 9004.41 examples/s]
Map: 100%|█████████████████████████████████████████████████████████████████████████████| 760/760 [00:00<00:00, 9496.64 examples/s]
/root/HW2/task2/train.py:101: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
2024-10-29 10:18:24 - INFO - __main__ - Starting training...
  0%|                                                                                                     | 0/642 [00:00<?, ?it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
 33%|██████████████████████████████▎                                                            | 214/642 [00:43<01:16,  5.56it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
{'loss': 1.3365, 'grad_norm': 4.184363842010498, 'learning_rate': 4.922118380062305e-05, 'epoch': 0.05}
{'loss': 0.6779, 'grad_norm': 9.500732421875, 'learning_rate': 4.844236760124611e-05, 'epoch': 0.09}
{'loss': 0.4617, 'grad_norm': 8.388921737670898, 'learning_rate': 4.766355140186916e-05, 'epoch': 0.14}
{'loss': 0.3366, 'grad_norm': 4.857041358947754, 'learning_rate': 4.6884735202492216e-05, 'epoch': 0.19}
{'loss': 0.2466, 'grad_norm': 9.863204956054688, 'learning_rate': 4.6105919003115266e-05, 'epoch': 0.23}
{'loss': 0.3682, 'grad_norm': 2.879220485687256, 'learning_rate': 4.532710280373832e-05, 'epoch': 0.28}
{'loss': 0.3844, 'grad_norm': 14.847082138061523, 'learning_rate': 4.454828660436137e-05, 'epoch': 0.33}
{'loss': 0.4807, 'grad_norm': 3.8009092807769775, 'learning_rate': 4.376947040498442e-05, 'epoch': 0.37}
{'loss': 0.493, 'grad_norm': 7.391562461853027, 'learning_rate': 4.299065420560748e-05, 'epoch': 0.42}
{'loss': 0.3133, 'grad_norm': 7.772970199584961, 'learning_rate': 4.221183800623053e-05, 'epoch': 0.47}
{'loss': 0.24, 'grad_norm': 6.383650779724121, 'learning_rate': 4.1433021806853586e-05, 'epoch': 0.51}
{'loss': 0.3393, 'grad_norm': 5.527586460113525, 'learning_rate': 4.0654205607476636e-05, 'epoch': 0.56}
{'loss': 0.4111, 'grad_norm': 3.708545446395874, 'learning_rate': 3.987538940809969e-05, 'epoch': 0.61}
{'loss': 0.2925, 'grad_norm': 7.846874237060547, 'learning_rate': 3.909657320872274e-05, 'epoch': 0.65}
{'loss': 0.3533, 'grad_norm': 15.335149765014648, 'learning_rate': 3.831775700934579e-05, 'epoch': 0.7}
{'loss': 0.2446, 'grad_norm': 6.9412384033203125, 'learning_rate': 3.753894080996885e-05, 'epoch': 0.75}
{'loss': 0.29, 'grad_norm': 19.430797576904297, 'learning_rate': 3.67601246105919e-05, 'epoch': 0.79}
{'loss': 0.371, 'grad_norm': 2.42287015914917, 'learning_rate': 3.5981308411214956e-05, 'epoch': 0.84}
{'loss': 0.3479, 'grad_norm': 3.0346155166625977, 'learning_rate': 3.5202492211838006e-05, 'epoch': 0.89}
{'loss': 0.3455, 'grad_norm': 3.1975440979003906, 'learning_rate': 3.442367601246106e-05, 'epoch': 0.93}
{'loss': 0.3827, 'grad_norm': 9.831406593322754, 'learning_rate': 3.364485981308411e-05, 'epoch': 0.98}
  warnings.warn(                                                                                                                  
{'eval_loss': 0.24278570711612701, 'eval_accuracy': 0.9223684210526316, 'eval_micro_f1': 0.9223684210526316, 'eval_macro_f1': 0.9202753171687666, 'eval_runtime': 1.7885, 'eval_samples_per_second': 424.948, 'eval_steps_per_second': 13.419, 'epoch': 1.0}
 67%|████████████████████████████████████████████████████████████▋                              | 428/642 [01:25<00:38,  5.55it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
{'loss': 0.2901, 'grad_norm': 3.068540334701538, 'learning_rate': 3.286604361370716e-05, 'epoch': 1.03}
{'loss': 0.215, 'grad_norm': 6.018436908721924, 'learning_rate': 3.208722741433022e-05, 'epoch': 1.07}
{'loss': 0.274, 'grad_norm': 5.460598945617676, 'learning_rate': 3.130841121495327e-05, 'epoch': 1.12}
{'loss': 0.1906, 'grad_norm': 1.1563477516174316, 'learning_rate': 3.0529595015576326e-05, 'epoch': 1.17}
{'loss': 0.2199, 'grad_norm': 2.7606101036071777, 'learning_rate': 2.9750778816199376e-05, 'epoch': 1.21}
{'loss': 0.2378, 'grad_norm': 8.860888481140137, 'learning_rate': 2.897196261682243e-05, 'epoch': 1.26}
{'loss': 0.2294, 'grad_norm': 13.790220260620117, 'learning_rate': 2.8193146417445482e-05, 'epoch': 1.31}
{'loss': 0.3047, 'grad_norm': 6.928067684173584, 'learning_rate': 2.7414330218068536e-05, 'epoch': 1.36}
{'loss': 0.3052, 'grad_norm': 9.405025482177734, 'learning_rate': 2.663551401869159e-05, 'epoch': 1.4}
{'loss': 0.2404, 'grad_norm': 8.066494941711426, 'learning_rate': 2.585669781931464e-05, 'epoch': 1.45}
{'loss': 0.2714, 'grad_norm': 6.5374755859375, 'learning_rate': 2.5077881619937692e-05, 'epoch': 1.5}
{'loss': 0.1948, 'grad_norm': 1.0234256982803345, 'learning_rate': 2.429906542056075e-05, 'epoch': 1.54}
{'loss': 0.1926, 'grad_norm': 5.985337257385254, 'learning_rate': 2.3520249221183802e-05, 'epoch': 1.59}
{'loss': 0.2228, 'grad_norm': 7.38588285446167, 'learning_rate': 2.2741433021806856e-05, 'epoch': 1.64}
{'loss': 0.1926, 'grad_norm': 5.589636325836182, 'learning_rate': 2.196261682242991e-05, 'epoch': 1.68}
{'loss': 0.2126, 'grad_norm': 3.709773302078247, 'learning_rate': 2.118380062305296e-05, 'epoch': 1.73}
{'loss': 0.2354, 'grad_norm': 3.9242024421691895, 'learning_rate': 2.0404984423676012e-05, 'epoch': 1.78}
{'loss': 0.1669, 'grad_norm': 8.579289436340332, 'learning_rate': 1.9626168224299065e-05, 'epoch': 1.82}
{'loss': 0.1783, 'grad_norm': 9.548317909240723, 'learning_rate': 1.884735202492212e-05, 'epoch': 1.87}
{'loss': 0.2556, 'grad_norm': 3.5918521881103516, 'learning_rate': 1.8068535825545172e-05, 'epoch': 1.92}
{'loss': 0.2525, 'grad_norm': 3.4505019187927246, 'learning_rate': 1.7289719626168225e-05, 'epoch': 1.96}
  warnings.warn(                                                                                                                  
{'eval_loss': 0.23584377765655518, 'eval_accuracy': 0.925, 'eval_micro_f1': 0.925, 'eval_macro_f1': 0.9239015562578315, 'eval_runtime': 1.8164, 'eval_samples_per_second': 418.408, 'eval_steps_per_second': 13.213, 'epoch': 2.0}
 78%|██████████████████████████████████████████████████████████████████████▊                    | 500/642 [01:39<00:26,  5.36it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
{'loss': 0.243, 'grad_norm': 4.567526340484619, 'learning_rate': 1.6510903426791275e-05, 'epoch': 2.01}
{'loss': 0.1566, 'grad_norm': 6.7457380294799805, 'learning_rate': 1.573208722741433e-05, 'epoch': 2.06}
{'loss': 0.126, 'grad_norm': 1.889910101890564, 'learning_rate': 1.4953271028037382e-05, 'epoch': 2.1}
{'loss': 0.1285, 'grad_norm': 14.054193496704102, 'learning_rate': 1.4174454828660435e-05, 'epoch': 2.15}
{'loss': 0.0893, 'grad_norm': 11.12801456451416, 'learning_rate': 1.3395638629283489e-05, 'epoch': 2.2}
{'loss': 0.133, 'grad_norm': 3.3847367763519287, 'learning_rate': 1.2616822429906542e-05, 'epoch': 2.24}
{'loss': 0.1352, 'grad_norm': 5.389906883239746, 'learning_rate': 1.1838006230529595e-05, 'epoch': 2.29}
{'loss': 0.2078, 'grad_norm': 9.703266143798828, 'learning_rate': 1.1059190031152649e-05, 'epoch': 2.34}
  warnings.warn(
100%|███████████████████████████████████████████████████████████████████████████████████████████| 642/642 [02:07<00:00,  5.55it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
{'loss': 0.1293, 'grad_norm': 8.386886596679688, 'learning_rate': 1.02803738317757e-05, 'epoch': 2.38}
{'loss': 0.0932, 'grad_norm': 2.582521677017212, 'learning_rate': 9.501557632398754e-06, 'epoch': 2.43}
{'loss': 0.1132, 'grad_norm': 2.617809772491455, 'learning_rate': 8.722741433021807e-06, 'epoch': 2.48}
{'loss': 0.1421, 'grad_norm': 3.027164936065674, 'learning_rate': 7.94392523364486e-06, 'epoch': 2.52}
{'loss': 0.1609, 'grad_norm': 3.735769271850586, 'learning_rate': 7.165109034267913e-06, 'epoch': 2.57}
{'loss': 0.0893, 'grad_norm': 2.058208465576172, 'learning_rate': 6.386292834890965e-06, 'epoch': 2.62}
{'loss': 0.0882, 'grad_norm': 12.334803581237793, 'learning_rate': 5.607476635514019e-06, 'epoch': 2.66}
{'loss': 0.1749, 'grad_norm': 0.526149332523346, 'learning_rate': 4.828660436137072e-06, 'epoch': 2.71}
{'loss': 0.1493, 'grad_norm': 3.887669563293457, 'learning_rate': 4.0498442367601245e-06, 'epoch': 2.76}
{'loss': 0.1712, 'grad_norm': 4.625781059265137, 'learning_rate': 3.2710280373831774e-06, 'epoch': 2.8}
{'loss': 0.082, 'grad_norm': 5.285001277923584, 'learning_rate': 2.4922118380062308e-06, 'epoch': 2.85}
{'loss': 0.1128, 'grad_norm': 11.590523719787598, 'learning_rate': 1.7133956386292835e-06, 'epoch': 2.9}
{'loss': 0.1663, 'grad_norm': 1.747882604598999, 'learning_rate': 9.345794392523364e-07, 'epoch': 2.94}
{'loss': 0.0733, 'grad_norm': 2.2222697734832764, 'learning_rate': 1.5576323987538942e-07, 'epoch': 2.99}
  warnings.warn(
100%|███████████████████████████████████████████████████████████████████████████████████████████| 642/642 [02:10<00:00,  4.90it/s]
2024-10-29 10:20:35 - INFO - __main__ - Starting evaluation...                                                                    
{'eval_loss': 0.2151927798986435, 'eval_accuracy': 0.9368421052631579, 'eval_micro_f1': 0.9368421052631579, 'eval_macro_f1': 0.9355868351852675, 'eval_runtime': 1.9438, 'eval_samples_per_second': 390.987, 'eval_steps_per_second': 12.347, 'epoch': 3.0}
{'train_runtime': 130.9645, 'train_samples_per_second': 156.684, 'train_steps_per_second': 4.902, 'train_loss': 0.25824284651012064, 'epoch': 3.0}
/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
100%|█████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:01<00:00, 13.87it/s]
2024-10-29 10:20:37 - INFO - __main__ - Evaluation results: {'eval_loss': 0.2151927798986435, 'eval_accuracy': 0.9368421052631579, 'eval_micro_f1': 0.9368421052631579, 'eval_macro_f1': 0.9355868351852675, 'eval_runtime': 1.8105, 'eval_samples_per_second': 419.765, 'eval_steps_per_second': 13.256, 'epoch': 3.0}
2024-10-29 10:20:37 - INFO - __main__ - Training complete.
