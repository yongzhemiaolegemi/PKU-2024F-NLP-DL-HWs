2024-10-29 10:49:24 - INFO - __main__ - Initializing training script
2024-10-29 10:49:24 - INFO - __main__ - Loading dataset...
2024-10-29 10:49:33 - INFO - __main__ - Loading model and tokenizer...
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/root/HW2/task2/train.py:101: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
2024-10-29 10:49:38 - INFO - __main__ - Starting training...
  0%|                                                                                                     | 0/642 [00:00<?, ?it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
 33%|██████████████████████████████▎                                                            | 214/642 [00:42<01:13,  5.85it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
{'loss': 1.2219, 'grad_norm': 6.747310161590576, 'learning_rate': 4.922118380062305e-05, 'epoch': 0.05}
{'loss': 0.7774, 'grad_norm': 7.120999813079834, 'learning_rate': 4.844236760124611e-05, 'epoch': 0.09}
{'loss': 0.4736, 'grad_norm': 7.262653827667236, 'learning_rate': 4.766355140186916e-05, 'epoch': 0.14}
{'loss': 0.3925, 'grad_norm': 6.561558246612549, 'learning_rate': 4.6884735202492216e-05, 'epoch': 0.19}
{'loss': 0.3829, 'grad_norm': 8.675905227661133, 'learning_rate': 4.6105919003115266e-05, 'epoch': 0.23}
{'loss': 0.3397, 'grad_norm': 3.917780876159668, 'learning_rate': 4.532710280373832e-05, 'epoch': 0.28}
{'loss': 0.39, 'grad_norm': 7.393333911895752, 'learning_rate': 4.454828660436137e-05, 'epoch': 0.33}
{'loss': 0.2659, 'grad_norm': 7.264037132263184, 'learning_rate': 4.376947040498442e-05, 'epoch': 0.37}
{'loss': 0.425, 'grad_norm': 3.7955760955810547, 'learning_rate': 4.299065420560748e-05, 'epoch': 0.42}
{'loss': 0.3926, 'grad_norm': 7.481893539428711, 'learning_rate': 4.221183800623053e-05, 'epoch': 0.47}
{'loss': 0.3395, 'grad_norm': 17.921422958374023, 'learning_rate': 4.1433021806853586e-05, 'epoch': 0.51}
{'loss': 0.3882, 'grad_norm': 4.586062908172607, 'learning_rate': 4.0654205607476636e-05, 'epoch': 0.56}
{'loss': 0.4757, 'grad_norm': 6.057643413543701, 'learning_rate': 3.987538940809969e-05, 'epoch': 0.61}
{'loss': 0.2413, 'grad_norm': 3.874677896499634, 'learning_rate': 3.909657320872274e-05, 'epoch': 0.65}
{'loss': 0.3751, 'grad_norm': 5.095726013183594, 'learning_rate': 3.831775700934579e-05, 'epoch': 0.7}
{'loss': 0.3328, 'grad_norm': 5.186026096343994, 'learning_rate': 3.753894080996885e-05, 'epoch': 0.75}
{'loss': 0.3537, 'grad_norm': 6.0244879722595215, 'learning_rate': 3.67601246105919e-05, 'epoch': 0.79}
{'loss': 0.2455, 'grad_norm': 1.6639028787612915, 'learning_rate': 3.5981308411214956e-05, 'epoch': 0.84}
{'loss': 0.2891, 'grad_norm': 5.290059566497803, 'learning_rate': 3.5202492211838006e-05, 'epoch': 0.89}
{'loss': 0.3166, 'grad_norm': 4.86035680770874, 'learning_rate': 3.442367601246106e-05, 'epoch': 0.93}
{'loss': 0.3692, 'grad_norm': 3.0015869140625, 'learning_rate': 3.364485981308411e-05, 'epoch': 0.98}
  warnings.warn(                                                                                                                  
{'eval_loss': 0.23199908435344696, 'eval_accuracy': 0.9289473684210526, 'eval_micro_f1': 0.9289473684210526, 'eval_macro_f1': 0.9272190482224915, 'eval_runtime': 1.7457, 'eval_samples_per_second': 435.36, 'eval_steps_per_second': 13.748, 'epoch': 1.0}
 67%|████████████████████████████████████████████████████████████▋                              | 428/642 [01:45<00:36,  5.82it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
{'loss': 0.2396, 'grad_norm': 5.213459014892578, 'learning_rate': 3.286604361370716e-05, 'epoch': 1.03}
{'loss': 0.2181, 'grad_norm': 3.8727521896362305, 'learning_rate': 3.208722741433022e-05, 'epoch': 1.07}
{'loss': 0.2682, 'grad_norm': 8.091517448425293, 'learning_rate': 3.130841121495327e-05, 'epoch': 1.12}
{'loss': 0.2916, 'grad_norm': 3.7620842456817627, 'learning_rate': 3.0529595015576326e-05, 'epoch': 1.17}
{'loss': 0.172, 'grad_norm': 3.282618761062622, 'learning_rate': 2.9750778816199376e-05, 'epoch': 1.21}
{'loss': 0.1333, 'grad_norm': 4.882789611816406, 'learning_rate': 2.897196261682243e-05, 'epoch': 1.26}
{'loss': 0.1507, 'grad_norm': 1.2090256214141846, 'learning_rate': 2.8193146417445482e-05, 'epoch': 1.31}
{'loss': 0.2175, 'grad_norm': 4.941736221313477, 'learning_rate': 2.7414330218068536e-05, 'epoch': 1.36}
{'loss': 0.2009, 'grad_norm': 4.999418258666992, 'learning_rate': 2.663551401869159e-05, 'epoch': 1.4}
{'loss': 0.2877, 'grad_norm': 3.452160358428955, 'learning_rate': 2.585669781931464e-05, 'epoch': 1.45}
{'loss': 0.2285, 'grad_norm': 6.205728054046631, 'learning_rate': 2.5077881619937692e-05, 'epoch': 1.5}
{'loss': 0.1676, 'grad_norm': 5.19396448135376, 'learning_rate': 2.429906542056075e-05, 'epoch': 1.54}
{'loss': 0.1903, 'grad_norm': 3.261868715286255, 'learning_rate': 2.3520249221183802e-05, 'epoch': 1.59}
{'loss': 0.1758, 'grad_norm': 5.366961479187012, 'learning_rate': 2.2741433021806856e-05, 'epoch': 1.64}
{'loss': 0.1153, 'grad_norm': 3.4917092323303223, 'learning_rate': 2.196261682242991e-05, 'epoch': 1.68}
{'loss': 0.1391, 'grad_norm': 3.3573458194732666, 'learning_rate': 2.118380062305296e-05, 'epoch': 1.73}
{'loss': 0.2155, 'grad_norm': 1.7946151494979858, 'learning_rate': 2.0404984423676012e-05, 'epoch': 1.78}
{'loss': 0.1696, 'grad_norm': 3.764496088027954, 'learning_rate': 1.9626168224299065e-05, 'epoch': 1.82}
{'loss': 0.237, 'grad_norm': 4.562739372253418, 'learning_rate': 1.884735202492212e-05, 'epoch': 1.87}
{'loss': 0.2012, 'grad_norm': 7.8703203201293945, 'learning_rate': 1.8068535825545172e-05, 'epoch': 1.92}
{'loss': 0.1999, 'grad_norm': 1.3507235050201416, 'learning_rate': 1.7289719626168225e-05, 'epoch': 1.96}
  warnings.warn(                                                                                                                  
{'eval_loss': 0.2043275088071823, 'eval_accuracy': 0.9421052631578948, 'eval_micro_f1': 0.9421052631578948, 'eval_macro_f1': 0.9407214121224904, 'eval_runtime': 1.7558, 'eval_samples_per_second': 432.845, 'eval_steps_per_second': 13.669, 'epoch': 2.0}
 78%|██████████████████████████████████████████████████████████████████████▊                    | 500/642 [01:58<00:25,  5.61it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
{'loss': 0.2184, 'grad_norm': 1.2786425352096558, 'learning_rate': 1.6510903426791275e-05, 'epoch': 2.01}
{'loss': 0.1014, 'grad_norm': 0.6580004692077637, 'learning_rate': 1.573208722741433e-05, 'epoch': 2.06}
{'loss': 0.1075, 'grad_norm': 4.393641471862793, 'learning_rate': 1.4953271028037382e-05, 'epoch': 2.1}
{'loss': 0.0797, 'grad_norm': 4.078677177429199, 'learning_rate': 1.4174454828660435e-05, 'epoch': 2.15}
{'loss': 0.1184, 'grad_norm': 3.9655861854553223, 'learning_rate': 1.3395638629283489e-05, 'epoch': 2.2}
{'loss': 0.134, 'grad_norm': 1.999837875366211, 'learning_rate': 1.2616822429906542e-05, 'epoch': 2.24}
{'loss': 0.0726, 'grad_norm': 0.1876511126756668, 'learning_rate': 1.1838006230529595e-05, 'epoch': 2.29}
{'loss': 0.1114, 'grad_norm': 2.3122594356536865, 'learning_rate': 1.1059190031152649e-05, 'epoch': 2.34}
  warnings.warn(
100%|███████████████████████████████████████████████████████████████████████████████████████████| 642/642 [02:25<00:00,  5.89it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
{'loss': 0.1689, 'grad_norm': 4.58545446395874, 'learning_rate': 1.02803738317757e-05, 'epoch': 2.38}
{'loss': 0.0737, 'grad_norm': 0.46294966340065, 'learning_rate': 9.501557632398754e-06, 'epoch': 2.43}
{'loss': 0.0611, 'grad_norm': 1.0457311868667603, 'learning_rate': 8.722741433021807e-06, 'epoch': 2.48}
{'loss': 0.0799, 'grad_norm': 0.3852120637893677, 'learning_rate': 7.94392523364486e-06, 'epoch': 2.52}
{'loss': 0.0785, 'grad_norm': 1.014723539352417, 'learning_rate': 7.165109034267913e-06, 'epoch': 2.57}
{'loss': 0.0831, 'grad_norm': 1.4288475513458252, 'learning_rate': 6.386292834890965e-06, 'epoch': 2.62}
{'loss': 0.1322, 'grad_norm': 10.392971992492676, 'learning_rate': 5.607476635514019e-06, 'epoch': 2.66}
{'loss': 0.0969, 'grad_norm': 6.17767858505249, 'learning_rate': 4.828660436137072e-06, 'epoch': 2.71}
{'loss': 0.0859, 'grad_norm': 2.5005385875701904, 'learning_rate': 4.0498442367601245e-06, 'epoch': 2.76}
{'loss': 0.1051, 'grad_norm': 3.6782310009002686, 'learning_rate': 3.2710280373831774e-06, 'epoch': 2.8}
{'loss': 0.101, 'grad_norm': 1.6117184162139893, 'learning_rate': 2.4922118380062308e-06, 'epoch': 2.85}
{'loss': 0.0884, 'grad_norm': 8.447092056274414, 'learning_rate': 1.7133956386292835e-06, 'epoch': 2.9}
{'loss': 0.03, 'grad_norm': 3.5371298789978027, 'learning_rate': 9.345794392523364e-07, 'epoch': 2.94}
{'loss': 0.1483, 'grad_norm': 3.720519542694092, 'learning_rate': 1.5576323987538942e-07, 'epoch': 2.99}
  warnings.warn(
100%|███████████████████████████████████████████████████████████████████████████████████████████| 642/642 [02:28<00:00,  4.31it/s]
2024-10-29 10:52:07 - INFO - __main__ - Starting evaluation...                                                                    
{'eval_loss': 0.23075295984745026, 'eval_accuracy': 0.9381578947368421, 'eval_micro_f1': 0.9381578947368421, 'eval_macro_f1': 0.9366970588439704, 'eval_runtime': 1.8649, 'eval_samples_per_second': 407.533, 'eval_steps_per_second': 12.869, 'epoch': 3.0}
{'train_runtime': 148.8158, 'train_samples_per_second': 137.889, 'train_steps_per_second': 4.314, 'train_loss': 0.23852945297863626, 'epoch': 3.0}
/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
100%|█████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:01<00:00, 14.11it/s]
2024-10-29 10:52:09 - INFO - __main__ - Evaluation results: {'eval_loss': 0.23075295984745026, 'eval_accuracy': 0.9381578947368421, 'eval_micro_f1': 0.9381578947368421, 'eval_macro_f1': 0.9366970588439704, 'eval_runtime': 1.78, 'eval_samples_per_second': 426.96, 'eval_steps_per_second': 13.483, 'epoch': 3.0}
2024-10-29 10:52:09 - INFO - __main__ - Training complete.
